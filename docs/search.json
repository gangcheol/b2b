[
  {
    "objectID": "posts/CS/01. AI/07. 추천 시스템/01. 내용 기반 추천.html",
    "href": "posts/CS/01. AI/07. 추천 시스템/01. 내용 기반 추천.html",
    "title": "01. 내용 기반 추천",
    "section": "",
    "text": "내용 기반 추천\n1 상품의 속성 데이터, 즉 어떤 삼품인지를 사용해서 추천\n2 데이터 예시 1 : 각 영화마다 어떤 속성(장르)인지 0과 1사이로 표현\n\n\n\n\n액션\n로맨스\n코미디\n감동\n\n\n\n\n러브 액츄얼리\n0.1\n0.9\n0.9\n0.6\n\n\n반지의 제왕\n1.0\n0.5\n0\n0.2\n\n\n국제 시장\n0.5\n0.2\n0\n1.0\n\n\n극한 직업\n0.8\n0.9\n1.0\n0\n\n\n어바웃 타임\n0.3\n0.9\n0.8\n0.5\n\n\n\n3 데이터 예시 2 : 유저와 영화 평점 데이터\n\n\n\n\n러브 액츄얼리\n반지의 제왕\n해리 포터\n극한 직업\n\n\n\n\n현승\n5\n1\n-\n5\n\n\n영훈\n-\n2\n2\n0\n\n\n동욱\n-\n4\n0\n1\n\n\n종훈\n3\n-\n4\n3\n\n\n우재\n5\n4\n-\n2\n\n\n…\n…\n…\n…\n…\n\n\n\n4 만약 현승에게 영화 추천을 한다면? 두 개의 데이터를 합해서 다음과 같은 구조로 만듬\n\n입력 변수 : 액션 ~ 코미디\n목표 변수 : 현승의 영화 평점\n\n\n\n\n\n액션\n로맨스\n코미디\n감동\n현승\n\n\n\n\n러브 액츄얼리\n0.1\n0.9\n0.9\n0.6\n5\n\n\n반지의 제왕\n1.0\n0.5\n0\n0.2\n1.0\n\n\n국제 시장\n0.5\n0.2\n0\n1.0\n1.0\n\n\n극한 직업\n0.8\n0.9\n1.0\n0\n1.0\n\n\n어바웃 타임\n0.3\n0.9\n0.8\n0.5\n5\n\n\n\n5 해당 데이터를 기반으로 학습 시킨 모델로 현승이 어떤 영화의 어떤 평점을 줄 지 예상할 수 있음\n\n분류 알고리즘 : 좋아요/싫어요\n회귀 알고리즘 : 1~5점, 선형 회귀, 다항 회귀….\n\n\n\n\nex1. 선형회귀\n1 패키지 로드\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nimport numpy as np\n\n2 데이터 로드\n\nMOVIE_DATA_PATH = './data/movie_rating.csv'\nmovie_rating_df = pd.read_csv(MOVIE_DATA_PATH)\n\nmovie_rating_df.head()\n\n\n\n\n\n\n\n\n\nromance\naction\ncomedy\nheart-warming\nrating\n\n\n\n\n0\n0.740458\n0.733800\n0.526879\n0.332906\n3\n\n\n1\n0.658391\n0.825211\n0.608177\n0.906809\n5\n\n\n2\n0.680250\n0.401992\n0.400964\n0.535223\n2\n\n\n3\n0.572216\n0.312618\n0.496313\n0.319996\n1\n\n\n4\n0.543545\n0.623021\n0.713110\n0.696774\n4\n\n\n\n\n\n\n\n\n3 X, y 분리\n\ntarget = \"rating\"\n\nX = movie_rating_df.drop(target, axis = 1)\nY = movie_rating_df[target]\n\n4 훈련, 검증 데이터 분리\n\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=5)\n\n5 모델 학슴\n\nmodel = LinearRegression()\n\nmodel.fit(X_train, y_train)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LinearRegression?Documentation for LinearRegressioniFittedLinearRegression() \n\n\n6 예측\n\ny_pred = model.predict(X_test)\ny_pred\n\narray([3.83034441, 2.59418977, 2.63653567, 3.48333221, 2.75217812,\n       2.43303141, 3.03247628, 4.41312853, 4.28145539, 3.61139607,\n       3.82260281, 3.01099122, 3.06324646, 4.41401949, 4.08837667,\n       3.30347874, 4.69514621, 4.3397661 , 3.42084949, 3.94638571])\n\n\n\n\n장단점\n1 장점\n\n상품을 추천할 때 다른 유저 데이터가 필요하지 않다.\n\n즉, 혼자서 부지런히 평점을 주면 자신의 활동을 기반으로 다른 상품들을 추천받을 수 있다.\n\n새롭게 출시한 상품이나, 인기가 없는 상품을 추천할 수 있다.\n\n다른 유저들의 데이터를 사용하지 않기 때문!\n\n\n2 단점\n\n상품에 대한 적합한 속성을 고르는 것이 어렵다.\n\n어떤 영화는 배우, 어떤 영화는 감독이 중요한 영화일 수 있기 때문\n\n고른 속성 값들이 주관적으로 선정될 수 있다.\n유저가 준 데이터를 벗어나는 추천을 할 수 없다.\n\n유저가 코디디, 액션 영화만 평점을 주었다면, 로맨스, 스릴러와 같은 영화를 추천할 수 없다.\n\n인기가 많은 상품들을 더 추천해 줄 수 없다.\n\n다른 사람들이 좋게 평가한 영화들을 추천받기 힘들기 때문!",
    "crumbs": [
      "Posts",
      "CS",
      "01. AI",
      "07. 추천 시스템",
      "01. 내용 기반 추천"
    ]
  },
  {
    "objectID": "posts/CS/01. AI/01. 인공 신경망 이론/00. Intro.html",
    "href": "posts/CS/01. AI/01. 인공 신경망 이론/00. Intro.html",
    "title": "00. Intro",
    "section": "",
    "text": "1. MNIST\n- 흑백 손글씨 숫자 데이터\n\nMNIST DATABASE : Modified National Institute of Standards and Technology database\n총 6만 개의 학습 데이터와 만 개의 테스트 데이터로 이뤄져 있다.\n28 x 28 픽셀 이미지 : 픽셀은 하나의 색을 가진다. 즉, 하나의 이미지는 28 x 28 = 784개의 색으로 이루어진다.\n또한, 해당 데이터튼 검정, 흰색, 회색으로 이루어진다. 밝음의 정도는 0 ~ 1 사이의 숫자로 표현한다.\n픽셀정보는 파이썬 리스트처럼 표현한다.(784개의 한 줄로 저장되어 있음)\n각 이미지마다 어떤 숫자를 나타내는 지도 정해져 있음\n입력 변수 : 픽셀 데이터, 목표 변수 : 이미지가 나타내는 숫자\nMNIST와 같은 데이터 셋을 이용하면 인공 신경망을 조금 더 직관적으로 이해할 수 있음!\n데이터 다운로드\n\n\n\n\n2. 로지스틱 회귀\n\n숫자 5인지를 예측하는 경우 가설함수 설정(픽셀이 1개 일경우)\n\n\\[h_{\\theta}(x) = \\frac{1}{e^{-(\\theta_0 + \\theta_1 x)}}\\]\n\n그러나 mnist 데이터는 픽셀 데이터가 하나가 아니라 784개 이다.\n즉, 입력 변수의 개수는 : 784+1, 추정해야할 파라미터 \\(\\theta_i\\)는 785개이다.\n따라서 가설함수를 다시 쓰면, 아래와 같이 표현할 수 있다.\n0에 가까울 수록 해당 숫자가 5가 아닐 확률이 높고, 맞을 경우 1에 가깝게 나온다.\n\n\\[x = \\begin{bmatrix} x_0 \\\\  \\\\ x_1 \\\\ \\\\ \\dots \\\\ \\\\ x_{785}\\end{bmatrix},\\,\\, \\theta = \\begin{bmatrix} \\theta_0 \\\\  \\\\ \\theta_1 \\\\ \\\\ \\dots \\\\ \\\\ \\theta_{785}\\end{bmatrix}, \\quad h_{\\theta}(x) = \\frac{1}{e^{-(\\theta_0 + \\theta_1 x_1 \\dots \\theta_{785}x_{785})}} = \\sigma(\\theta^{\\top}x)\\]\n\n\\(\\theta_0 = b\\) : 편향(bias), \\(\\theta_{i} = w_i(i \\neq 0)\\) : 가중치라고 한다.\n입력 변수를 종합하는 가설 함수를 꼭, 시그모이드 함수를 사용해야 하는 것은 아님(relu, tanh 등등)\n그리고 이 시그모이드 함수와 같은 것들을 활성 함수(activation function)이라고 한다.\n\n\n\n\n3. 인공신경망\n- 인공 신경망은 수많은 인공 뉴런(로지스틱 등등)을 엮어서 예측을 하는 머신 러닝 알고리즘\n- 보통 여러 뉴런들은 층이라는 단위로 묶어서 표현\n\n입력층에서 인풋을 받아, 다음 층 모든 뉴런들의 출력을 구함\n구한 뉴런들의 출력을 사용해 다음 층 모든 뉴런들의 출력을 계산\n반복 후, 마지막 층 모든 뉴런들의 출력을 계산한다.\n\n- 뒤에 있는 층으로 갈수록 그림안에서 아래와 같이 좀 더 고차원적인 패턴을 찾는데 최적화되어 있음\n\n이해를 위해 우리에게 익숙한 패턴을 찾는 것이라고 생각하자.\n\n\n출력층 전 단계층\n\n\n\n\n\n출력층 전 단계층의 전 단계층\n\n\n\n\n- 즉, 인공신경망은?\n\n신경계의 뉴런들는 작은 패턴을 찾아내는 데 특화\n작은 단위의 패턴을 다음 층의 인풋으로 사용해서 더 섬세하게 예측한다.\n따라서, 인공 신경망은 로지스틱 회귀나 다르 알고리즘보다 성능이 좋게 나옴을 기대할 수 있다!",
    "crumbs": [
      "Posts",
      "CS",
      "01. AI",
      "01. 인공 신경망 이론",
      "00. Intro"
    ]
  },
  {
    "objectID": "posts/CS/00. DA/03. 자연어 처리/03. 감성 분석.html",
    "href": "posts/CS/00. DA/03. 자연어 처리/03. 감성 분석.html",
    "title": "03. 감성 분석",
    "section": "",
    "text": "자연어의 담긴 어조가 긍정, 부정, 중립인지 확인하는 작업\n감성을 판단하는 기준을 만드는 방법에 따라 두 가지 접근법이 존재함\n\n\n- 감성 어휘 사전을 기준으로 특정 단어가 어떠한 감정인지 분류하는 장법\n\n직관적으로 이해하기 쉽고, 연산 속도가 빠름\n그러나 감성 어휘 사전에 없는 단어들로 이루어진 코퍼스는 분석이 제한된다는 단점이 있다..\n\n\n\n\n- 다수의 코퍼스들을 통해 긍정 단어와 부정 단어를 구분하는 모델을 학습시켜 학습한 모델을 기반으로 감성 지수를 확인하는 방법\n\n이점 : 감성 어휘 사전에 없는 단어들로 이루어졌거나 오타가 많은 코퍼스를 분석할 때 효과적이다.\n단점 : 학습을 위한 대량의 훈련 데이터가 필요함, 분석 결과가 규칙 기반 감성 분석보다 안정적이지 않다는 단점도 있다.",
    "crumbs": [
      "Posts",
      "CS",
      "00. DA",
      "03. 자연어 처리",
      "03. 감성 분석"
    ]
  },
  {
    "objectID": "posts/CS/00. DA/03. 자연어 처리/03. 감성 분석.html#규칙-기반-감성-분석",
    "href": "posts/CS/00. DA/03. 자연어 처리/03. 감성 분석.html#규칙-기반-감성-분석",
    "title": "03. 감성 분석",
    "section": "",
    "text": "- 감성 어휘 사전을 기준으로 특정 단어가 어떠한 감정인지 분류하는 장법\n\n직관적으로 이해하기 쉽고, 연산 속도가 빠름\n그러나 감성 어휘 사전에 없는 단어들로 이루어진 코퍼스는 분석이 제한된다는 단점이 있다..",
    "crumbs": [
      "Posts",
      "CS",
      "00. DA",
      "03. 자연어 처리",
      "03. 감성 분석"
    ]
  },
  {
    "objectID": "posts/CS/00. DA/03. 자연어 처리/03. 감성 분석.html#머신러닝-기반-감성-분석",
    "href": "posts/CS/00. DA/03. 자연어 처리/03. 감성 분석.html#머신러닝-기반-감성-분석",
    "title": "03. 감성 분석",
    "section": "",
    "text": "- 다수의 코퍼스들을 통해 긍정 단어와 부정 단어를 구분하는 모델을 학습시켜 학습한 모델을 기반으로 감성 지수를 확인하는 방법\n\n이점 : 감성 어휘 사전에 없는 단어들로 이루어졌거나 오타가 많은 코퍼스를 분석할 때 효과적이다.\n단점 : 학습을 위한 대량의 훈련 데이터가 필요함, 분석 결과가 규칙 기반 감성 분석보다 안정적이지 않다는 단점도 있다.",
    "crumbs": [
      "Posts",
      "CS",
      "00. DA",
      "03. 자연어 처리",
      "03. 감성 분석"
    ]
  },
  {
    "objectID": "posts/CS/00. DA/03. 자연어 처리/03. 감성 분석.html#실습",
    "href": "posts/CS/00. DA/03. 자연어 처리/03. 감성 분석.html#실습",
    "title": "03. 감성 분석",
    "section": "실습",
    "text": "실습\n1 함수 로드\n\nfrom nltk.corpus import wordnet as wn\n\n2 Synset 적용\n\nsynsets = wn.synsets('lead')\nprint(synsets)\n\n[Synset('lead.n.01'), Synset('lead.n.02'), Synset('lead.n.03'), Synset('lead.n.04'), Synset('lead.n.05'), Synset('lead.n.06'), Synset('lead.n.07'), Synset('star.n.04'), Synset('lead.n.09'), Synset('tip.n.03'), Synset('lead.n.11'), Synset('spark_advance.n.01'), Synset('leash.n.01'), Synset('lead.n.14'), Synset('lead.n.15'), Synset('jumper_cable.n.01'), Synset('lead.n.17'), Synset('lead.v.01'), Synset('leave.v.07'), Synset('lead.v.03'), Synset('lead.v.04'), Synset('lead.v.05'), Synset('run.v.03'), Synset('head.v.02'), Synset('lead.v.08'), Synset('contribute.v.03'), Synset('conduct.v.02'), Synset('go.v.25'), Synset('precede.v.04'), Synset('run.v.23'), Synset('moderate.v.01')]\n\n\n3 결과 해석\n\nSynset은 Synset('단어, 품사, 순번')의 형태를 가진다.\nWordnet 품사 복기\n\n\n\n\n품사 태그\n품사\n\n\n\n\nn (wn.NOUN)\n명사\n\n\na (wn.ADJ)\n형용사\n\n\nr (wn.ADV)\n부사\n\n\nr (wn.VERB)\n동사\n\n\n\n\nSynset('lead.n.01')과 Synset('lead.n.02')처럼 단어의 형태와 품사가 같더라도 의미가 다른 경우를 순번으로 구분\n각 순번에 해당하는 정확한 단어의 의미는 definition() 함수로 확인\n\n\nprint(wn.synset('lead.n.01').definition())\nprint(wn.synset('lead.n.02').definition())\n\nan advantage held by a competitor in a race\na soft heavy toxic malleable metallic element; bluish white when freshly cut but tarnishes readily to dull grey\n\n\n\n또한, lead와 비슷한 의미의 단어들도 Synset 목록에 포함된다.\n\nSynset('star.n.04'), Synset('tip.n.03)\n\n마지막으로, 같은 단어가 품사에 따라 의미가 다를 수 있음\n\nSynset('lead.n.01'), Synset('lead.v.01')\n이럴 경우에는 특별히 원하는 품사의 Synset만 따로 추출하여 사용할 수 있다.\n\n\n\nsynsets = wn.synsets('lead', 'n')\n\nprint(synsets)\n\n[Synset('lead.n.01'), Synset('lead.n.02'), Synset('lead.n.03'), Synset('lead.n.04'), Synset('lead.n.05'), Synset('lead.n.06'), Synset('lead.n.07'), Synset('star.n.04'), Synset('lead.n.09'), Synset('tip.n.03'), Synset('lead.n.11'), Synset('spark_advance.n.01'), Synset('leash.n.01'), Synset('lead.n.14'), Synset('lead.n.15'), Synset('jumper_cable.n.01'), Synset('lead.n.17')]\n\n\n\nprint(wn.synsets('lead', wn.NOUN))\n\n[Synset('lead.n.01'), Synset('lead.n.02'), Synset('lead.n.03'), Synset('lead.n.04'), Synset('lead.n.05'), Synset('lead.n.06'), Synset('lead.n.07'), Synset('star.n.04'), Synset('lead.n.09'), Synset('tip.n.03'), Synset('lead.n.11'), Synset('spark_advance.n.01'), Synset('leash.n.01'), Synset('lead.n.14'), Synset('lead.n.15'), Synset('jumper_cable.n.01'), Synset('lead.n.17')]\n\n\n- 단어의 품사에 따라 감성 지수가 달라진다.\n\n즉, 감성 지수를 정확하게 계산하려면 원하는 품사의 Synset을 정확하게 지정하여 사용해야 한다.",
    "crumbs": [
      "Posts",
      "CS",
      "00. DA",
      "03. 자연어 처리",
      "03. 감성 분석"
    ]
  },
  {
    "objectID": "posts/CS/00. DA/03. 자연어 처리/03. 감성 분석.html#실습-1",
    "href": "posts/CS/00. DA/03. 자연어 처리/03. 감성 분석.html#실습-1",
    "title": "03. 감성 분석",
    "section": "실습 1",
    "text": "실습 1\n1 패키지, 함수 로드\n\nfrom nltk.corpus import sentiwordnet as swn\nimport nltk\nnltk.download('sentiwordnet')\n\n[nltk_data] Downloading package sentiwordnet to\n[nltk_data]     C:\\Users\\rkdcj\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package sentiwordnet is already up-to-date!\n\n\nTrue\n\n\n2 wordnet과 결과 비교\n\nprint(\"wordnet-happy: \", wn.synsets('happy'))\nprint(\"\\nsentiwordnet-happy: \", list(swn.senti_synsets('happy')))\n\nwordnet-happy:  [Synset('happy.a.01'), Synset('felicitous.s.02'), Synset('glad.s.02'), Synset('happy.s.04')]\n\nsentiwordnet-happy:  [SentiSynset('happy.a.01'), SentiSynset('felicitous.s.02'), SentiSynset('glad.s.02'), SentiSynset('happy.s.04')]\n\n\n\n비슷하다. 그래서 감성 지수는?\n\n\nhappy_sentisynsets = list(swn.senti_synsets('happy'))\n\npos_score = happy_sentisynsets[0].pos_score()\nneg_score = happy_sentisynsets[0].neg_score()\nobj_score = happy_sentisynsets[0].obj_score()\n\nprint(pos_score, neg_score, obj_score)\n\n0.875 0.0 0.125\n\n\n\n최종 감성 지수는 긍정 시수에서 부정 지수를 뺀 값이 사용된다.\n\n\nprint(pos_score - neg_score)\n\n0.875\n\n\n\n긍정 지수, 부정 지수, 객관성 지수는 0과 1 사이의 값을 갖는다.\n긍정 지수에서 부정 지수를 뺀 감정 지수는 -1과 1 사이의 값을 갖는다.\n-1에 가까우면 부정적인 의미를, 0에 가까우면 중립적인 의미를, 1에 가까우면 긍정적인 의미를 가진 단어로 해석할 수 있다.\n\n3 특정 품사의 SentiSynset 찾기\n\n단어는 품사에 따라 문맥상 의미가 달라진다.\n따라서 단어가 어떤 품사로 사용됐는지에 따라 감성 지수의 결과도 달라진다.\n이렇기 때문에 분석에 사용할 품사인 단어의 SentiSynset을 특정해서 찾는게 필요하다.\n\n\nex1. hard라는 단어의 형용사, 부사를 기준으로 감성 지수 구하기 * 구한 synset의 가장 보편적인 의미로 사용되는 첫번째 synset을 가져와서 활용\n\n\nadj_synset = wn.synsets('hard', wn.ADJ)[0]\nadv_synset = wn.synsets('hard', wn.ADV)[0]\n\n\n해당 synset의 단어, 품사, 순번정보를 swn.senti_synset()파라미터로 넣어 주자.( name() 함수 활용)\n\n\nadj_senti_synset = swn.senti_synset(adj_synset.name())\nadv_senti_synset = swn.senti_synset(adv_synset.name())\n\nprint(adj_senti_synset)\nprint(adv_senti_synset)\n\n&lt;difficult.a.01: PosScore=0.0 NegScore=0.75&gt;\n&lt;hard.r.01: PosScore=0.125 NegScore=0.125&gt;\n\n\n\n형용사 hard : 부정적인 의미로 확인\n부사 hard : 중립적인 의미로 확인된다.\n\n\ndef get_sentiment_score(word, pos):\n    # 단어와 품사 태그를 기반으로 Synsets 구하기\n    synsets = wn.synsets(word, pos)\n        \n    # SentiSynset의 긍정 지수, 부정 지수 구하기\n    pos_score = swn.senti_synset(synsets[0].name()).pos_score()\n    neg_score = swn.senti_synset(synsets[0].name()).neg_score()\n\n    # 긍정 지수 - 부정 지수로 감성 지수 값 계산해 반환하기\n    sentiment_score = pos_score - neg_score\n\n    return sentiment_score\n\nget_sentiment_score('love', wn.VERB)\n\n0.5",
    "crumbs": [
      "Posts",
      "CS",
      "00. DA",
      "03. 자연어 처리",
      "03. 감성 분석"
    ]
  },
  {
    "objectID": "posts/CS/00. DA/03. 자연어 처리/03. 감성 분석.html#실습-2",
    "href": "posts/CS/00. DA/03. 자연어 처리/03. 감성 분석.html#실습-2",
    "title": "03. 감성 분석",
    "section": "실습 2",
    "text": "실습 2\n\nimport warnings\nwarnings.filterwarnings(action = \"ignore\")\n\n\n\nCode\nimport pandas as pd\nfrom nltk.corpus import stopwords\nfrom preprocess import *\n\ndf = pd.read_csv('imdb.tsv', delimiter = \"\\\\t\")\n\ndf['review'] = df['review'].str.lower()\ndf['sent_tokens'] = df['review'].apply(sent_tokenize)\ndf['pos_tagged_tokens'] = df['sent_tokens'].apply(pos_tagger)\ndf['lemmatized_tokens'] = df['pos_tagged_tokens'].apply(words_lemmatizer)\nstopwords_set = set(stopwords.words('english'))\n\ndf['cleaned_tokens'] = df['lemmatized_tokens'].apply(lambda x: clean_by_freq(x, 1))\ndf['cleaned_tokens'] = df['cleaned_tokens'].apply(lambda x: clean_by_len(x, 2))\ndf['cleaned_tokens'] = df['cleaned_tokens'].apply(lambda x: clean_by_stopwords(x, stopwords_set))\n\n\n[nltk_data] Downloading package wordnet to\n[nltk_data]     C:\\Users\\rkdcj\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package omw-1.4 to\n[nltk_data]     C:\\Users\\rkdcj\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package omw-1.4 is already up-to-date!\n\n\n1 첫 번째 로우에 있는 코퍼스를 받아온 다음 감성 전수 초기값을 0으로 설정\n\npos_tagged_words = df['pos_tagged_tokens'][0]\nsenti_score = 0\n\n2 PennTreebank Tag로 태깅된 품사를 WordNet Tag 기준으로 변경\n\nPennTreebank Tag에는 있지만 WordNet Tag에는 없는 품사가 있기 때문에 이 경우는 분석에서 제외\n\n\nfor word, tag in pos_tagged_words:\n    wn_tag = penn_to_wn(tag)\n    \n    # WordNet Tag에 포함되지 않는 경우는 제외\n    if wn_tag not in (wn.NOUN, wn.ADJ, wn.ADV, wn.VERB):\n        continue\n\n3 Synset, SentiSynset 구하기\n\nfor word, tag in pos_tagged_words:\n    # ...\n    \n    # Synset 확인, 어휘 사전에 없을 경우에는 제외\n    if not wn.synsets(word, wn_tag):\n        continue\n    else:\n        synsets = wn.synsets(word, wn_tag)\n    \n    # SentiSynset 확인\n    synset = synsets[0]\n    swn_synset = swn.senti_synset(synset.name())\n\n4 감성지수 계산\n\nfor word, tag in pos_tagged_words:\n    # ...\n    \n    # 감성 지수 계산\n    word_senti_score = (swn_synset.pos_score() - swn_synset.neg_score())\n    senti_score += word_senti_score\n\n\nprint(senti_score)\n\n14.375\n\n\n5 함수로 작성\n\ndef swn_polarity(pos_tagged_words):\n    senti_score = 0\n\n    for word, tag in pos_tagged_words:\n        # PennTreeBank 기준 품사를 WordNet 기준 품사로 변경\n        wn_tag = penn_to_wn(tag)\n        if wn_tag not in (wn.NOUN, wn.ADJ, wn.ADV, wn.VERB):\n            continue\n    \n        # Synset 확인, 어휘 사전에 없을 경우에는 스킵\n        if not wn.synsets(word, wn_tag):\n            continue\n        else:\n            synsets = wn.synsets(word, wn_tag)\n    \n        # SentiSynset 확인\n        synset = synsets[0]\n        swn_synset = swn.senti_synset(synset.name())\n\n        # 감성 지수 계산\n        word_senti_score = (swn_synset.pos_score() - swn_synset.neg_score())\n        senti_score += word_senti_score\n\n    return senti_score\n\n7 감성분석 결과 확인\n\nfrom preprocess import swn_polarity\n\n# dataframe에 swn_polarity() 함수 적용\ndf['swn_sentiment'] = df['pos_tagged_tokens'].apply(swn_polarity)\n\ndf[['review', 'swn_sentiment']]\n\n\n\n\n\n\n\n\n\nreview\nswn_sentiment\n\n\n\n\n0\n\"watching time chasers, it obvious that it was...\n-0.375\n\n\n1\ni saw this film about 20 years ago and remembe...\n-1.500\n\n\n2\nminor spoilers in new york, joan barnard (elvi...\n-2.250\n\n\n3\ni went to see this film with a great deal of e...\n-0.500\n\n\n4\n\"yes, i agree with everyone on this site this ...\n3.000\n\n\n5\n\"jennifer ehle was sparkling in \\\"\"pride and p...\n6.750\n\n\n6\namy poehler is a terrific comedian on saturday...\n0.750\n\n\n7\n\"a plane carrying employees of a large biotech...\n8.750\n\n\n8\na well made, gritty science fiction movie, it ...\n4.500\n\n\n9\n\"incredibly dumb and utterly predictable story...\n-1.125",
    "crumbs": [
      "Posts",
      "CS",
      "00. DA",
      "03. 자연어 처리",
      "03. 감성 분석"
    ]
  },
  {
    "objectID": "posts/CS/00. DA/03. 자연어 처리/03. 감성 분석.html#실습-3",
    "href": "posts/CS/00. DA/03. 자연어 처리/03. 감성 분석.html#실습-3",
    "title": "03. 감성 분석",
    "section": "실습",
    "text": "실습\n1 함수 생성\n\ndef vader_sentiment(text):\n    analyzer = SentimentIntensityAnalyzer()\n    \n    # VADER 감성 분석\n    senti_score = analyzer.polarity_scores(text)['compound']\n    \n    return senti_score\n\n2 데이터에 적용\n\ndf['vader_sentiment'] = df['review'].apply(vader_sentiment)\n\ndf[['review', 'swn_sentiment', 'vader_sentiment']]\n\n\n\n\n\n\n\n\n\nreview\nswn_sentiment\nvader_sentiment\n\n\n\n\n0\n\"watching time chasers, it obvious that it was...\n-0.375\n-0.9095\n\n\n1\ni saw this film about 20 years ago and remembe...\n-1.500\n-0.9694\n\n\n2\nminor spoilers in new york, joan barnard (elvi...\n-2.250\n-0.2794\n\n\n3\ni went to see this film with a great deal of e...\n-0.500\n-0.9707\n\n\n4\n\"yes, i agree with everyone on this site this ...\n3.000\n0.8049\n\n\n5\n\"jennifer ehle was sparkling in \\\"\"pride and p...\n6.750\n0.9494\n\n\n6\namy poehler is a terrific comedian on saturday...\n0.750\n0.8473\n\n\n7\n\"a plane carrying employees of a large biotech...\n8.750\n0.9885\n\n\n8\na well made, gritty science fiction movie, it ...\n4.500\n0.9887\n\n\n9\n\"incredibly dumb and utterly predictable story...\n-1.125\n-0.7375\n\n\n\n\n\n\n\n\n3 VADER 공식문서",
    "crumbs": [
      "Posts",
      "CS",
      "00. DA",
      "03. 자연어 처리",
      "03. 감성 분석"
    ]
  },
  {
    "objectID": "posts/CS/00. DA/03. 자연어 처리/01. 자연어 전처리 (1).html",
    "href": "posts/CS/00. DA/03. 자연어 처리/01. 자연어 전처리 (1).html",
    "title": "01. 자연어 전처리 (1)",
    "section": "",
    "text": "1 맞춤법과 띄어쓰기 교정\n\nex 1. Oh, Hi helo. Nice to meetyou.\n\nstep 1. 띄어쓰기 수정 : Oh, Hi hello. Nice to meet you.\nstep 2. 문장의 의미를 표현하는데 크게 기여하지 않는 단어 삭제\n\nOh, Hi hello. Nice to meet you.\n\nstep 3. 중복된 의미 단어 제거\n\nOh, Hi hello. Nice to meet you.\n\n\n\n2 컴퓨터가 자연어를 잘 이해할 수 있도록 각 단어에 숫자 인덱스를 부여\n\n{‘Hi’:0, ‘Nice’:1, ‘to’:2, ‘meet’:3, ‘you’:4}",
    "crumbs": [
      "Posts",
      "CS",
      "00. DA",
      "03. 자연어 처리",
      "01. 자연어 전처리 (1)"
    ]
  },
  {
    "objectID": "posts/CS/00. DA/03. 자연어 처리/01. 자연어 전처리 (1).html#ex1",
    "href": "posts/CS/00. DA/03. 자연어 처리/01. 자연어 전처리 (1).html#ex1",
    "title": "01. 자연어 전처리 (1)",
    "section": "",
    "text": "1 맞춤법과 띄어쓰기 교정\n\nex 1. Oh, Hi helo. Nice to meetyou.\n\nstep 1. 띄어쓰기 수정 : Oh, Hi hello. Nice to meet you.\nstep 2. 문장의 의미를 표현하는데 크게 기여하지 않는 단어 삭제\n\nOh, Hi hello. Nice to meet you.\n\nstep 3. 중복된 의미 단어 제거\n\nOh, Hi hello. Nice to meet you.\n\n\n\n2 컴퓨터가 자연어를 잘 이해할 수 있도록 각 단어에 숫자 인덱스를 부여\n\n{‘Hi’:0, ‘Nice’:1, ‘to’:2, ‘meet’:3, ‘you’:4}",
    "crumbs": [
      "Posts",
      "CS",
      "00. DA",
      "03. 자연어 처리",
      "01. 자연어 전처리 (1)"
    ]
  },
  {
    "objectID": "posts/CS/00. DA/03. 자연어 처리/01. 자연어 전처리 (1).html#nltk",
    "href": "posts/CS/00. DA/03. 자연어 처리/01. 자연어 전처리 (1).html#nltk",
    "title": "01. 자연어 전처리 (1)",
    "section": "NLTK",
    "text": "NLTK\n1 터미널에서 아래와 같은 커맨드르 실행\nconda install nltk\n2 패키지에서 함수 로드\n\nfrom nltk.tokenize import word_tokenize\n\n3 nltk에서 제공하는 토큰화 모듈인 punkt를 다운로드\n\npunkt : 마침표나 약어(Mr., Dr.)와 같은 특별한 언어적 특성을 고려하여 토큰화를 할 수 있게 해주는 모듈\n\n\nimport nltk\nnltk.download(\"punkt\")\n\n[nltk_data] Downloading package punkt to\n[nltk_data]     C:\\Users\\rkdcj\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n\n\nTrue",
    "crumbs": [
      "Posts",
      "CS",
      "00. DA",
      "03. 자연어 처리",
      "01. 자연어 전처리 (1)"
    ]
  },
  {
    "objectID": "posts/CS/00. DA/03. 자연어 처리/01. 자연어 전처리 (1).html#단어-토큰화-수행",
    "href": "posts/CS/00. DA/03. 자연어 처리/01. 자연어 전처리 (1).html#단어-토큰화-수행",
    "title": "01. 자연어 전처리 (1)",
    "section": "단어 토큰화 수행",
    "text": "단어 토큰화 수행\n\ntext = \"Although it's not a happily-ever-after ending, it is very realistic.\"\n\n# 단어 토큰화\ntokenized_words = word_tokenize(text)\n\nprint(tokenized_words)\n\n['Although', 'it', \"'s\", 'not', 'a', 'happily-ever-after', 'ending', ',', 'it', 'is', 'very', 'realistic', '.']\n\n\n- 기본적으로 띄어쓰기, 어퍼스트로피(’), 콤마(,)를 기준으로 토큰화를 수행하고 있으며 하이픈(-)은 토큰화의 기준으로 사용하지 않음\n\n어떠한 기준을 가지고 단어 토큰화를 하는게 더 좋다고는할 수 없다. 분석에 활용하려는 코퍼스의 특성에 따라 적절한 토큰화 기준을 사용하면 된다.\nnltk 공식문서",
    "crumbs": [
      "Posts",
      "CS",
      "00. DA",
      "03. 자연어 처리",
      "01. 자연어 전처리 (1)"
    ]
  },
  {
    "objectID": "posts/CS/00. DA/03. 자연어 처리/01. 자연어 전처리 (1).html#등장빈도가-적은-단어",
    "href": "posts/CS/00. DA/03. 자연어 처리/01. 자연어 전처리 (1).html#등장빈도가-적은-단어",
    "title": "01. 자연어 전처리 (1)",
    "section": "등장빈도가 적은 단어",
    "text": "등장빈도가 적은 단어\n\nfrom text import TEXT\n\n\ncorpus = TEXT\n\nprint(corpus)\n\nAfter reading the comments for this movie, I am not sure whether I should be angry, sad or sickened. Seeing comments typical of people who a)know absolutely nothing about the military or b)who base everything they think they know on movies like this or on CNN reports about Abu-Gharib makes me wonder about the state of intellectual stimulation in the world. At the time I type this the number of people in the US military: 1.4 million on Active Duty with another almost 900,000 in the Guard and Reserves for a total of roughly 2.3 million. The number of people indicted for abuses at at Abu-Gharib: Currently less than 20 That makes the total of people indicted .00083% of the total military. Even if you indict every single military member that ever stepped in to Abu-Gharib, you would not come close to making that a whole number.  The flaws in this movie would take YEARS to cover. I understand that it's supposed to be sarcastic, but in reality, the writer and director are trying to make commentary about the state of the military without an enemy to fight. In reality, the US military has been at its busiest when there are not conflicts going on. The military is the first called for disaster relief and humanitarian aid missions. When the tsunami hit Indonesia, devestating the region, the US military was the first on the scene. When the chaos of the situation overwhelmed the local governments, it was military leadership who looked at their people, the same people this movie mocks, and said make it happen. Within hours, food aid was reaching isolated villages. Within days, airfields were built, cargo aircraft started landing and a food distribution system was up and running. Hours and days, not weeks and months. Yes there are unscrupulous people in the US military. But then, there are in every walk of life, every occupation. But to see people on this website decide that 2.3 million men and women are all criminal, with nothing on their minds but thoughts of destruction or mayhem is an absolute disservice to the things that they do every day. One person on this website even went so far as to say that military members are in it for personal gain. Wow! Entry level personnel make just under $8.00 an hour assuming a 40 hour work week. Of course, many work much more than 40 hours a week and those in harm's way typically put in 16-18 hour days for months on end. That makes the pay well under minimum wage. So much for personal gain. I beg you, please make yourself familiar with the world around you. Go to a nearby base, get a visitor pass and meet some of the men and women you are so quick to disparage. You would be surprised. The military no longer accepts people in lieu of prison time. They require a minimum of a GED and prefer a high school diploma. The middle ranks are expected to get a minimum of undergraduate degrees and the upper ranks are encouraged to get advanced degrees.\n\n\n\n\n빈도가 2이하인 단어들만 찾기\n\nfrom collections import Counter\n\n1 토큰화\n\nt_words = word_tokenize(corpus)\n#t_words\n\n2 단어 빈도 수 카운트\n\nvocab = Counter(t_words)\n#vocab\n\n3 단어 빈도수가 2이하인 단어 리스트 추출\n\nuncommon_words = [key for key, value in vocab.items() if value &lt;=2]\n#uncommon_words\n\n4 빈도수가 2이하인 단어들만 제거한 결과를 따로 저장\n\nc_words = [word for word in t_words if word not in uncommon_words]\n#c_words",
    "crumbs": [
      "Posts",
      "CS",
      "00. DA",
      "03. 자연어 처리",
      "01. 자연어 전처리 (1)"
    ]
  },
  {
    "objectID": "posts/CS/00. DA/03. 자연어 처리/01. 자연어 전처리 (1).html#길이가-짧은-단어",
    "href": "posts/CS/00. DA/03. 자연어 처리/01. 자연어 전처리 (1).html#길이가-짧은-단어",
    "title": "01. 자연어 전처리 (1)",
    "section": "길이가 짧은 단어",
    "text": "길이가 짧은 단어\n영어 단어의 경우, 알파벳 하나 또는 두개로 구성된 단어는 코퍼스의 의미를 나타내는 데 중요하지 않을 수 있다.\n그래서 이러한 단어들은 제거하는 것이 좋음\n1 길이가 2이하인 단어 제거\n\nc_f_len = []\n\nfor word in c_words :\n    if len(word) &gt; 2:\n        c_f_len.append(word)\n\n2 정제 전과 후의 결과 비교\n\nprint('정제 전:', c_words[:10])\nprint('정제 후:', c_f_len[:10])\n\n정제 전: ['the', 'for', 'this', 'movie', ',', 'I', 'not', 'I', 'be', ',']\n정제 후: ['the', 'for', 'this', 'movie', 'not', 'people', 'who', 'about', 'the', 'military']",
    "crumbs": [
      "Posts",
      "CS",
      "00. DA",
      "03. 자연어 처리",
      "01. 자연어 전처리 (1)"
    ]
  },
  {
    "objectID": "posts/CS/00. DA/03. 자연어 처리/01. 자연어 전처리 (1).html#함수-생성",
    "href": "posts/CS/00. DA/03. 자연어 처리/01. 자연어 전처리 (1).html#함수-생성",
    "title": "01. 자연어 전처리 (1)",
    "section": "함수 생성",
    "text": "함수 생성\n- 위에서 만든 정제 기준을 언제든 활용할 수 있도록 함수로 작성\n\nfrom collections import Counter\n\n# 등장 빈도 기준 정제 함수\ndef clean_by_freq(tokenized_words, cut_off_count):\n    # 파이썬의 Counter 모듈을 통해 단어의 빈도수 카운트하여 단어 집합 생성\n    vocab = Counter(tokenized_words)\n    \n    # 빈도수가 cut_off_count 이하인 단어 set 추출\n    uncommon_words = {key for key, value in vocab.items() if value &lt;= cut_off_count}\n    \n    # uncommon_words에 포함되지 않는 단어 리스트 생성\n    cleaned_words = [word for word in tokenized_words if word not in uncommon_words]\n\n    return cleaned_words\n\n# 단어 길이 기준 정제 함수\ndef clean_by_len(tokenized_words, cut_off_length):\n    # 길이가 cut_off_length 이하인 단어 제거\n    cleaned_by_freq_len = []\n    \n    for word in tokenized_words:\n        if len(word) &gt; cut_off_length:\n            cleaned_by_freq_len.append(word)\n\n    return cleaned_by_freq_len",
    "crumbs": [
      "Posts",
      "CS",
      "00. DA",
      "03. 자연어 처리",
      "01. 자연어 전처리 (1)"
    ]
  },
  {
    "objectID": "posts/CS/00. DA/03. 자연어 처리/01. 자연어 전처리 (1).html#step1.-불용어-세트-준비",
    "href": "posts/CS/00. DA/03. 자연어 처리/01. 자연어 전처리 (1).html#step1.-불용어-세트-준비",
    "title": "01. 자연어 전처리 (1)",
    "section": "step1. 불용어 세트 준비",
    "text": "step1. 불용어 세트 준비\n- nltk에서는 기본 불용어 목록 179개를 제공한다.\n\n아래와 같은 방법으로 불용어 목록에 접근할 수 있다.\n\n1 불용어 목록 로드\n\nfrom nltk.corpus import stopwords\n\nnltk.download(\"stopwords\")\n\n[nltk_data] Downloading package stopwords to\n[nltk_data]     C:\\Users\\rkdcj\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n\n\nTrue\n\n\n2 불용어들을 세트 자료형으로 저장\n\nstopwords_set = set(stopwords.words(\"english\"))\n\nprint(f\"불용어 개수: {len(stopwords_set)}\")\n\n불용어 개수: 179\n\n\n3 불용어 추가 및 삭제\n\nstopwords_set.add(\"hello\")\nstopwords_set.remove(\"the\")\nstopwords_set.remove(\"me\")\n\nprint(f\"불용어 개수: {len(stopwords_set)}\")\n\n불용어 개수: 178",
    "crumbs": [
      "Posts",
      "CS",
      "00. DA",
      "03. 자연어 처리",
      "01. 자연어 전처리 (1)"
    ]
  },
  {
    "objectID": "posts/CS/00. DA/03. 자연어 처리/01. 자연어 전처리 (1).html#step2.-불용어-제거하기",
    "href": "posts/CS/00. DA/03. 자연어 처리/01. 자연어 전처리 (1).html#step2.-불용어-제거하기",
    "title": "01. 자연어 전처리 (1)",
    "section": "step2. 불용어 제거하기",
    "text": "step2. 불용어 제거하기\n\nstop_words_set = set(stopwords.words('english'))\n\n# 불용어 제거\ncleaned_words = []\n\nfor word in c_f_len:\n    if word not in stop_words_set:\n        cleaned_words.append(word)\n\n\n# 불용어 제거 결과 확인\nprint('불용어 제거 전:', len(c_f_len))\nprint('불용어 제거 후:', len(cleaned_words))\n\n불용어 제거 전: 169\n불용어 제거 후: 67",
    "crumbs": [
      "Posts",
      "CS",
      "00. DA",
      "03. 자연어 처리",
      "01. 자연어 전처리 (1)"
    ]
  },
  {
    "objectID": "posts/CS/00. DA/03. 자연어 처리/01. 자연어 전처리 (1).html#step3.-불용어-처리-함수-만들기",
    "href": "posts/CS/00. DA/03. 자연어 처리/01. 자연어 전처리 (1).html#step3.-불용어-처리-함수-만들기",
    "title": "01. 자연어 전처리 (1)",
    "section": "step3. 불용어 처리 함수 만들기",
    "text": "step3. 불용어 처리 함수 만들기\n\n# 불용어 제거 함수\ndef clean_by_stopwords(tokenized_words, stop_words_set):\n    cleaned_words = []\n    \n    for word in tokenized_words:\n        if word not in stop_words_set:\n            cleaned_words.append(word)\n            \n    return cleaned_words",
    "crumbs": [
      "Posts",
      "CS",
      "00. DA",
      "03. 자연어 처리",
      "01. 자연어 전처리 (1)"
    ]
  },
  {
    "objectID": "posts/CS/00. DA/03. 자연어 처리/01. 자연어 전처리 (1).html#방법-1.-대소문자-통합",
    "href": "posts/CS/00. DA/03. 자연어 처리/01. 자연어 전처리 (1).html#방법-1.-대소문자-통합",
    "title": "01. 자연어 전처리 (1)",
    "section": "방법 1. 대소문자 통합",
    "text": "방법 1. 대소문자 통합\n\ntext = \"What can I do for you? Do your homework now.\"\n\n# 소문자로 변환\nprint(text.lower())\n\nwhat can i do for you? do your homework now.",
    "crumbs": [
      "Posts",
      "CS",
      "00. DA",
      "03. 자연어 처리",
      "01. 자연어 전처리 (1)"
    ]
  },
  {
    "objectID": "posts/CS/00. DA/03. 자연어 처리/01. 자연어 전처리 (1).html#방법-2.-규칙-기반-정규화",
    "href": "posts/CS/00. DA/03. 자연어 처리/01. 자연어 전처리 (1).html#방법-2.-규칙-기반-정규화",
    "title": "01. 자연어 전처리 (1)",
    "section": "방법 2. 규칙 기반 정규화",
    "text": "방법 2. 규칙 기반 정규화\n1 동의어 사전 작성\n\nsynonym_dict = {'US':'USA', 'U.S':'USA', 'Ummm':'Umm', 'Ummmm':'Umm' }\n\n2 단어 토큰화\n\ntext = \"She became a US citizen. Ummmm, I think, maybe and or.\"\nnormalized_words = []\n\ntokenized_words = nltk.word_tokenize(text)\n\n3 동의어 사전에 있는 단어라면, value에 해당하는 값으로 변환\n\nfor word in tokenized_words : \n    if word in synonym_dict.keys() :\n        word = synonym_dict[word]\n    \n    normalized_words.append(word)\n\n4 결과 확인\n\nprint(normalized_words)\n\n['She', 'became', 'a', 'USA', 'citizen', '.', 'Umm', ',', 'I', 'think', ',', 'maybe', 'and', 'or', '.']",
    "crumbs": [
      "Posts",
      "CS",
      "00. DA",
      "03. 자연어 처리",
      "01. 자연어 전처리 (1)"
    ]
  },
  {
    "objectID": "posts/CS/00. DA/03. 자연어 처리/01. 자연어 전처리 (1).html#nltk-어간추출",
    "href": "posts/CS/00. DA/03. 자연어 처리/01. 자연어 전처리 (1).html#nltk-어간추출",
    "title": "01. 자연어 전처리 (1)",
    "section": "NLTK 어간추출",
    "text": "NLTK 어간추출\n1 함수 로드\n\nfrom nltk.stem import PorterStemmer\n\nporter_stemmer = PorterStemmer()\nporter_stemmed_words = []\n\n2 단어 토큰화\n\ntext = \"You are so lovely. I am loving you now.\"\ntokenized_words = nltk.word_tokenize(text)\n\n3 어간 추출\n\nfor word in tokenized_words:\n    stem = porter_stemmer.stem(word)\n    porter_stemmed_words.append(stem)\n\n\nporter_stemmed_words\n\n['you', 'are', 'so', 'love', '.', 'i', 'am', 'love', 'you', 'now', '.']",
    "crumbs": [
      "Posts",
      "CS",
      "00. DA",
      "03. 자연어 처리",
      "01. 자연어 전처리 (1)"
    ]
  },
  {
    "objectID": "posts/CS/00. DA/03. 자연어 처리/01. 자연어 전처리 (1).html#extra.-랭커스터-스테머-알고리즘",
    "href": "posts/CS/00. DA/03. 자연어 처리/01. 자연어 전처리 (1).html#extra.-랭커스터-스테머-알고리즘",
    "title": "01. 자연어 전처리 (1)",
    "section": "extra. 랭커스터 스테머 알고리즘",
    "text": "extra. 랭커스터 스테머 알고리즘\n\nfrom nltk.stem import LancasterStemmer\n\nlancaster_stemmer = LancasterStemmer()\ntext = \"You are so lovely. I am loving you now.\"\nlancaster_stemmed_words = []\n\n# 랭커스터 스테머의 어간 추출\nfor word in tokenized_words:\n    stem = lancaster_stemmer.stem(word)\n    lancaster_stemmed_words.append(stem)\n\n\nlancaster_stemmed_words\n\n['you', 'ar', 'so', 'lov', '.', 'i', 'am', 'lov', 'you', 'now', '.']\n\n\n- 차이점 : 랭커스터 스테머 알고리즘은 뒤에 e와 같은 묵음 처리 부분을 어간에 포함시키지 않는다.",
    "crumbs": [
      "Posts",
      "CS",
      "00. DA",
      "03. 자연어 처리",
      "01. 자연어 전처리 (1)"
    ]
  },
  {
    "objectID": "posts/CS/00. DA/03. 자연어 처리/01. 자연어 전처리 (1).html#step1.-데이터-로드",
    "href": "posts/CS/00. DA/03. 자연어 처리/01. 자연어 전처리 (1).html#step1.-데이터-로드",
    "title": "01. 자연어 전처리 (1)",
    "section": "step1. 데이터 로드",
    "text": "step1. 데이터 로드\n\nimport warnings\n\nwarnings.filterwarnings(action = \"ignore\")\n\n\nimport pandas as pd\n\ndf = pd.read_csv('imdb.tsv', delimiter='\\\\t')\ndf\n\n\n\n\n\n\n\n\n\nreview\n\n\n\n\n0\n\"Watching Time Chasers, it obvious that it was...\n\n\n1\nI saw this film about 20 years ago and remembe...\n\n\n2\nMinor Spoilers In New York, Joan Barnard (Elvi...\n\n\n3\nI went to see this film with a great deal of e...\n\n\n4\n\"Yes, I agree with everyone on this site this ...\n\n\n5\n\"Jennifer Ehle was sparkling in \\\"\"Pride and P...\n\n\n6\nAmy Poehler is a terrific comedian on Saturday...\n\n\n7\n\"A plane carrying employees of a large biotech...\n\n\n8\nA well made, gritty science fiction movie, it ...\n\n\n9\n\"Incredibly dumb and utterly predictable story...",
    "crumbs": [
      "Posts",
      "CS",
      "00. DA",
      "03. 자연어 처리",
      "01. 자연어 전처리 (1)"
    ]
  },
  {
    "objectID": "posts/CS/00. DA/03. 자연어 처리/01. 자연어 전처리 (1).html#step2.-대소문자-통합",
    "href": "posts/CS/00. DA/03. 자연어 처리/01. 자연어 전처리 (1).html#step2.-대소문자-통합",
    "title": "01. 자연어 전처리 (1)",
    "section": "step2. 대소문자 통합",
    "text": "step2. 대소문자 통합\n\ndf['review'] = df['review'].str.lower()",
    "crumbs": [
      "Posts",
      "CS",
      "00. DA",
      "03. 자연어 처리",
      "01. 자연어 전처리 (1)"
    ]
  },
  {
    "objectID": "posts/CS/00. DA/03. 자연어 처리/01. 자연어 전처리 (1).html#step3.-단어-토큰화",
    "href": "posts/CS/00. DA/03. 자연어 처리/01. 자연어 전처리 (1).html#step3.-단어-토큰화",
    "title": "01. 자연어 전처리 (1)",
    "section": "step3. 단어 토큰화",
    "text": "step3. 단어 토큰화\n\ndf['word_tokens'] = df['review'].apply(word_tokenize)",
    "crumbs": [
      "Posts",
      "CS",
      "00. DA",
      "03. 자연어 처리",
      "01. 자연어 전처리 (1)"
    ]
  },
  {
    "objectID": "posts/CS/00. DA/03. 자연어 처리/01. 자연어 전처리 (1).html#step4.-데이터-정제",
    "href": "posts/CS/00. DA/03. 자연어 처리/01. 자연어 전처리 (1).html#step4.-데이터-정제",
    "title": "01. 자연어 전처리 (1)",
    "section": "step4. 데이터 정제",
    "text": "step4. 데이터 정제\n\n%load_ext autoreload\n%autoreload 2\n\nfrom preprocess import clean_by_freq\nfrom preprocess import clean_by_len\nfrom preprocess import clean_by_stopwords\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n\n\n- 이건 뭔데?\n%load_ext autoreload\n%autoreload 2\n\nipynb 파일에서 직접 만든 파이썬 모듈(.py)을 불러와 사용할 때, 파이썬 모듈 파일이 중간에 수정되면 해당 내용이 자동으로 반영되지 않는 문제가 발생\n그래서, preprocess.py 파일을 수정할 때마다 커널을 Restart해야함\n이러한 번거러움을 줄이기 위해 위의 코드를 실행한다.\n\n\nstopwords_set = set(stopwords.words('english'))\n\ndf['cleaned_tokens'] = df['word_tokens'].apply(lambda x: clean_by_freq(x, 1))\ndf['cleaned_tokens'] = df['cleaned_tokens'].apply(lambda x: clean_by_len(x, 2))\ndf['cleaned_tokens'] = df['cleaned_tokens'].apply(lambda x: clean_by_stopwords(x, stopwords_set))",
    "crumbs": [
      "Posts",
      "CS",
      "00. DA",
      "03. 자연어 처리",
      "01. 자연어 전처리 (1)"
    ]
  },
  {
    "objectID": "posts/CS/00. DA/03. 자연어 처리/01. 자연어 전처리 (1).html#step5.-어간-추출",
    "href": "posts/CS/00. DA/03. 자연어 처리/01. 자연어 전처리 (1).html#step5.-어간-추출",
    "title": "01. 자연어 전처리 (1)",
    "section": "step5. 어간 추출",
    "text": "step5. 어간 추출\n\nfrom preprocess import stemming_by_porter\n\ndf['stemmed_tokens'] = df['cleaned_tokens'].apply(stemming_by_porter)",
    "crumbs": [
      "Posts",
      "CS",
      "00. DA",
      "03. 자연어 처리",
      "01. 자연어 전처리 (1)"
    ]
  },
  {
    "objectID": "posts/CS/00. DA/03. 자연어 처리/01. 자연어 전처리 (1).html#step6.-결과-확인",
    "href": "posts/CS/00. DA/03. 자연어 처리/01. 자연어 전처리 (1).html#step6.-결과-확인",
    "title": "01. 자연어 전처리 (1)",
    "section": "step6. 결과 확인",
    "text": "step6. 결과 확인\n\ndf.head()\n\n\n\n\n\n\n\n\n\nreview\nword_tokens\ncleaned_tokens\nstemmed_tokens\n\n\n\n\n0\n\"watching time chasers, it obvious that it was...\n[``, watching, time, chasers, ,, it, obvious, ...\n[one, film, said, really, bad, movie, like, sa...\n[one, film, said, realli, bad, movi, like, sai...\n\n\n1\ni saw this film about 20 years ago and remembe...\n[i, saw, this, film, about, 20, years, ago, an...\n[film, film]\n[film, film]\n\n\n2\nminor spoilers in new york, joan barnard (elvi...\n[minor, spoilers, in, new, york, ,, joan, barn...\n[new, york, joan, barnard, elvire, audrey, bar...\n[new, york, joan, barnard, elvir, audrey, barn...\n\n\n3\ni went to see this film with a great deal of e...\n[i, went, to, see, this, film, with, a, great,...\n[went, film, film, went, jump, send, n't, jump...\n[went, film, film, went, jump, send, n't, jump...\n\n\n4\n\"yes, i agree with everyone on this site this ...\n[``, yes, ,, i, agree, with, everyone, on, thi...\n[site, movie, bad, even, movie, made, movie, s...\n[site, movi, bad, even, movi, made, movi, spec...",
    "crumbs": [
      "Posts",
      "CS",
      "00. DA",
      "03. 자연어 처리",
      "01. 자연어 전처리 (1)"
    ]
  },
  {
    "objectID": "posts/CS/00. DA/02. 데이터 분석과 시각화/04. 인사이트 도출.html",
    "href": "posts/CS/00. DA/02. 데이터 분석과 시각화/04. 인사이트 도출.html",
    "title": "04. 인사이트 도출",
    "section": "",
    "text": "import pandas as pd\n%matplotlib inline\n\nimport matplotlib.pyplot as plt\n\n\ndf = pd.read_csv(\"data/broadcast.csv\", index_col = 0)\ndf\n\n\n\n\n\n\n\n\n\nKBS\nMBC\nSBS\nTV CHOSUN\nJTBC\nChannel A\nMBN\n\n\n\n\n2011\n35.951\n18.374\n11.173\n9.102\n7.380\n3.771\n2.809\n\n\n2012\n36.163\n16.022\n11.408\n8.785\n7.878\n5.874\n3.310\n\n\n2013\n31.989\n16.778\n9.673\n9.026\n7.810\n5.350\n3.825\n\n\n2014\n31.210\n15.663\n9.108\n9.440\n7.490\n5.776\n4.572\n\n\n2015\n27.777\n16.573\n9.099\n9.940\n7.267\n6.678\n5.520\n\n\n2016\n27.583\n14.982\n8.669\n9.829\n7.727\n6.624\n5.477\n\n\n2017\n26.890\n12.465\n8.661\n8.886\n9.453\n6.056\n5.215\n\n\n\n\n\n\n\n\n\ndf.plot(figsize = (8, 4))\n\n\n\n\n\n\n\n\n- 컬럼 추가\n\n각 해의 전체 방송사의 시청율 더하기\n\n\ndf[\"total\"] = df.sum(axis = 1)\n\n- 흠, 시청율은 OTT서비스로 인해 절감하는 것을 볼 수 있다.\n\ndf.plot(y = \"total\", figsize = (8,4))\n\n\n\n\n\n\n\n\n- 지상파와 종편 시청율 구한 후, 컬럼 추가\n\ndf[\"Group 1\"] = df.loc[:, \"KBS\" : \"SBS\" ].sum(axis = 1)\ndf[\"Group 2\"] = df.loc[:, \"TV CHOSUN\" : \"MBN\" ].sum(axis = 1)\n\n\ndf.plot(y = [\"Group 1\", \"Group 2\"], figsize = (8,4))\n\n\n\n\n\n\n\n\n- 아 예전엔 지상파에 인기있는 드라마가 많았는데, 지금은 종편에 재밌는 드라마가 더 많나보다라고 인사이트를 도출할 수 있음",
    "crumbs": [
      "Posts",
      "CS",
      "00. DA",
      "02. 데이터 분석과 시각화",
      "04. 인사이트 도출"
    ]
  },
  {
    "objectID": "posts/CS/00. DA/02. 데이터 분석과 시각화/04. 인사이트 도출.html#ex1",
    "href": "posts/CS/00. DA/02. 데이터 분석과 시각화/04. 인사이트 도출.html#ex1",
    "title": "04. 인사이트 도출",
    "section": "ex1",
    "text": "ex1\n- 시설명에 ‘대학’이 포함되어 있으면 ’대학’, 그렇지 않으면 ‘일반’으로 나누어 ’분류’ column에 입력한다.\n\ndf.시설명.str.contains?\n\nObject `df.시설명.str.contains` not found.\n\n\n\nimport warnings\n\nwarnings.filterwarnings(action = \"ignore\")\n\n\ndf = pd.read_csv(\"data/museum_1.csv\")\n\ndf[\"분류\"] = df.시설명.str.contains(\"대학\")\ndf[\"분류\"].replace([False, True], [\"일반\", \"대학\"], inplace = True)\n\n\ndf.head()\n\n\n\n\n\n\n\n\n\n시설명\n어른관람료\n운영기관전화번호\n분류\n\n\n\n\n0\n필암서원 유물전시관\n500\n061-390-7224\n일반\n\n\n1\n원주역사박물관\n0\n033-737-4371\n일반\n\n\n2\n뮤지엄산미술관\n15000\n033-730-9000\n일반\n\n\n3\n오랜미래신화미술관\n0\n033-746-5256\n일반\n\n\n4\n연세대학교 원주박물관\n0\n033-760-2731\n대학",
    "crumbs": [
      "Posts",
      "CS",
      "00. DA",
      "02. 데이터 분석과 시각화",
      "04. 인사이트 도출"
    ]
  },
  {
    "objectID": "posts/CS/00. DA/02. 데이터 분석과 시각화/04. 인사이트 도출.html#ex1.-평균-나이가-어린-순으로-직업을-나열하기",
    "href": "posts/CS/00. DA/02. 데이터 분석과 시각화/04. 인사이트 도출.html#ex1.-평균-나이가-어린-순으로-직업을-나열하기",
    "title": "04. 인사이트 도출",
    "section": "ex1. 평균 나이가 어린 순으로 직업을 나열하기",
    "text": "ex1. 평균 나이가 어린 순으로 직업을 나열하기\n\ndf = pd.read_csv('data/occupations.csv')\n\ndf.groupby(\"occupation\")[[\"age\"]].mean().sort_values(\"age\",ascending = True)\n\n\n\n\n\n\n\n\n\nage\n\n\noccupation\n\n\n\n\n\nstudent\n22.081633\n\n\nnone\n26.555556\n\n\nentertainment\n29.222222\n\n\nartist\n31.392857\n\n\nhomemaker\n32.571429\n\n\nprogrammer\n33.121212\n\n\ntechnician\n33.148148\n\n\nother\n34.523810\n\n\nscientist\n35.548387\n\n\nsalesman\n35.666667\n\n\nwriter\n36.311111\n\n\nengineer\n36.388060\n\n\nlawyer\n36.750000\n\n\nmarketing\n37.615385\n\n\nexecutive\n38.718750\n\n\nadministrator\n38.746835\n\n\nlibrarian\n40.000000\n\n\nhealthcare\n41.562500\n\n\neducator\n42.010526\n\n\ndoctor\n43.571429\n\n\nretired\n63.071429",
    "crumbs": [
      "Posts",
      "CS",
      "00. DA",
      "02. 데이터 분석과 시각화",
      "04. 인사이트 도출"
    ]
  },
  {
    "objectID": "posts/CS/00. DA/02. 데이터 분석과 시각화/04. 인사이트 도출.html#ex2.-여성-비율이-높은-순으로-직업을-나열",
    "href": "posts/CS/00. DA/02. 데이터 분석과 시각화/04. 인사이트 도출.html#ex2.-여성-비율이-높은-순으로-직업을-나열",
    "title": "04. 인사이트 도출",
    "section": "ex2. 여성 비율이 높은 순으로 직업을 나열",
    "text": "ex2. 여성 비율이 높은 순으로 직업을 나열\n\ng1 = df.groupby(\"occupation\",as_index = False)[\"gender\"].value_counts(normalize = True)\ng1 = g1.loc[g1.gender == \"F\",[\"occupation\", \"proportion\"]].sort_values(\"proportion\", ascending = False).set_index(\"occupation\")\n\ng1[\"proportion\"]\n\noccupation\nhomemaker        0.857143\nhealthcare       0.687500\nlibrarian        0.568627\nartist           0.464286\nadministrator    0.455696\nnone             0.444444\nwriter           0.422222\nmarketing        0.384615\nother            0.342857\nstudent          0.306122\neducator         0.273684\nsalesman         0.250000\nlawyer           0.166667\nentertainment    0.111111\nscientist        0.096774\nexecutive        0.093750\nprogrammer       0.090909\nretired          0.071429\ntechnician       0.037037\nengineer         0.029851\nName: proportion, dtype: float64",
    "crumbs": [
      "Posts",
      "CS",
      "00. DA",
      "02. 데이터 분석과 시각화",
      "04. 인사이트 도출"
    ]
  },
  {
    "objectID": "posts/CS/00. DA/02. 데이터 분석과 시각화/02. 통계 기본 상식.html",
    "href": "posts/CS/00. DA/02. 데이터 분석과 시각화/02. 통계 기본 상식.html",
    "title": "02. 통계 기본 상식식",
    "section": "",
    "text": "- 대게 일반적으로 많은 사람들에게 친숙한 단어임\n- 조금 전문적인 지식(통계, 수학적 지식이 포함된 정의)\n\n모든 데이터셋의 값을 더한 후, 그 개수만큼 나눈 것\n\n- 평균 구하기 예시 1\n\nx = [10.3, 9.7, 10.3, 9.5, 10.1, 10.8, 9.5, 9.4, 10.1, 10.3]\n\nsum(x)/len(x)\n\n10.0",
    "crumbs": [
      "Posts",
      "CS",
      "00. DA",
      "02. 데이터 분석과 시각화",
      "02. 통계 기본 상식식"
    ]
  },
  {
    "objectID": "posts/CS/00. DA/02. 데이터 분석과 시각화/02. 통계 기본 상식.html#상관계수-시각화",
    "href": "posts/CS/00. DA/02. 데이터 분석과 시각화/02. 통계 기본 상식.html#상관계수-시각화",
    "title": "02. 통계 기본 상식식",
    "section": "상관계수 시각화",
    "text": "상관계수 시각화\n\n%matplotlib inline\nimport pandas as pd\nimport seaborn as sns\n\ndf = pd.read_csv('data/exam.csv')\n\ndf.corr()\n\n\n\n\n\n\n\n\n\nmath score\nreading score\nwriting score\n\n\n\n\nmath score\n1.000000\n0.817580\n0.802642\n\n\nreading score\n0.817580\n1.000000\n0.954598\n\n\nwriting score\n0.802642\n0.954598\n1.000000\n\n\n\n\n\n\n\n\n\nsns.heatmap(df.corr(), annot=True)",
    "crumbs": [
      "Posts",
      "CS",
      "00. DA",
      "02. 데이터 분석과 시각화",
      "02. 통계 기본 상식식"
    ]
  },
  {
    "objectID": "posts/CS/00. DA/02. 데이터 분석과 시각화/00. 시각화와 그래프.html",
    "href": "posts/CS/00. DA/02. 데이터 분석과 시각화/00. 시각화와 그래프.html",
    "title": "00. 시각화와 그래프",
    "section": "",
    "text": "- 목적 1 : 분석에 도움이 된다\n\n일반적인 데이터프레임을 보는 것 보다 그래프를 보고 패턴을 찾아내면 이후에 어떤 분석을 할 지 찾아낼 수 있다.\n또한, 보이지 않는 것(이상치, 영향치)들을 파악할 수 있음\n\n- 목적 2 : 리포팅에 도움이 된다.\n\nex : 기획팀에게 보고서를 전달할 때\n\n그래프를 기반으로 보고서를 작성해서 커뮤니케이션을 원할하게 함.",
    "crumbs": [
      "Posts",
      "CS",
      "00. DA",
      "02. 데이터 분석과 시각화",
      "00. 시각화와 그래프"
    ]
  },
  {
    "objectID": "posts/CS/00. DA/02. 데이터 분석과 시각화/00. 시각화와 그래프.html#실습-1.-gdp-데이터",
    "href": "posts/CS/00. DA/02. 데이터 분석과 시각화/00. 시각화와 그래프.html#실습-1.-gdp-데이터",
    "title": "00. 시각화와 그래프",
    "section": "실습 1. gdp 데이터",
    "text": "실습 1. gdp 데이터\n한국(Korea_Rep), 미국(United_States), 영국(United_Kingdom), 독일(Germany), 중국(China), 일본(Japan)의 GDP 그래프 그리기\n\nimport pandas as pd\n\n%matplotlib inline\n\ndf = pd.read_csv(\"data/gdp.csv\",index_col = 0)\n\ndf.plot(y = [\"Korea_Rep\",\"United_States\", \n                \"United_Kingdom\",\"Germany\", \n                \"Germany\", \"China\", \"Japan\"])",
    "crumbs": [
      "Posts",
      "CS",
      "00. DA",
      "02. 데이터 분석과 시각화",
      "00. 시각화와 그래프"
    ]
  },
  {
    "objectID": "posts/CS/00. DA/02. 데이터 분석과 시각화/00. 시각화와 그래프.html#실습-1",
    "href": "posts/CS/00. DA/02. 데이터 분석과 시각화/00. 시각화와 그래프.html#실습-1",
    "title": "00. 시각화와 그래프",
    "section": "실습 1",
    "text": "실습 1\n- 실리콘 밸리 남자 매지너의 인종 분포 그리기\n\n%matplotlib inline\nimport pandas as pd\n\ndf = pd.read_csv('data/silicon_valley_summary.csv')\n\ndf.loc[(df[\"job_category\"] == \"Managers\") & (df.gender == \"Male\") & \n       (df[\"race_ethnicity\"] != \"All\"), \n                   [\"count\", \"race_ethnicity\"]].plot(kind = \"bar\", x = \"race_ethnicity\", y= \"count\")",
    "crumbs": [
      "Posts",
      "CS",
      "00. DA",
      "02. 데이터 분석과 시각화",
      "00. 시각화와 그래프"
    ]
  },
  {
    "objectID": "posts/CS/00. DA/02. 데이터 분석과 시각화/00. 시각화와 그래프.html#실습-1-1",
    "href": "posts/CS/00. DA/02. 데이터 분석과 시각화/00. 시각화와 그래프.html#실습-1-1",
    "title": "00. 시각화와 그래프",
    "section": "실습 1",
    "text": "실습 1\n어도비 전체 직원들의 직군 분포를 파이 그래프로 그리\n\ndf\n\n\n\n\n\n\n\n\n\ncompany\nyear\nrace\ngender\njob_category\ncount\n\n\n\n\n0\n23andMe\n2016\nHispanic_or_Latino\nmale\nExecutives\n0\n\n\n1\n23andMe\n2016\nHispanic_or_Latino\nmale\nManagers\n1\n\n\n2\n23andMe\n2016\nHispanic_or_Latino\nmale\nProfessionals\n7\n\n\n3\n23andMe\n2016\nHispanic_or_Latino\nmale\nTechnicians\n0\n\n\n4\n23andMe\n2016\nHispanic_or_Latino\nmale\nSales workers\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n4435\nSanmina\n2016\nOverall_totals\nNaN\nlaborers and helpers\n4\n\n\n4436\nSanmina\n2016\nOverall_totals\nNaN\nService workers\n57\n\n\n4437\nSanmina\n2016\nOverall_totals\nNaN\nTotals\n5205\n\n\n4438\nSanmina\n2016\nOverall_totals\nNaN\nPrevious_totals\n5615\n\n\n4439\nSanmina\n2016\nOverall_totals\nNaN\nManagers\n591\n\n\n\n\n4440 rows × 6 columns\n\n\n\n\n\nsol1\n- 일단 내가 풀어봤는데 그래프가 살짝 각도가 이상하게 나옴\n\n%matplotlib inline\nimport pandas as pd\n\ndf = pd.read_csv('data/silicon_valley_details.csv')\n\n\ndf1 = df.loc[(df[\"company\"] == \"Adobe\") & (df[\"race\"] == \"Overall_totals\") & (df[\"count\"] !=0)].\\\n        groupby(\"job_category\", as_index = False)[[\"count\"]].sum()\n\ndf1.loc[map(lambda x : x not in [\"Previous_totals\", \"Totals\"],\n        df1[\"job_category\"])].set_index(\"job_category\")\n\n\n\n\n\n\n\n\n\ncount\n\n\njob_category\n\n\n\n\n\nAdministrative support\n323\n\n\nExecutives\n93\n\n\nManagers\n2448\n\n\nProfessionals\n3028\n\n\nSales workers\n1270\n\n\n\n\n\n\n\n\n\n\nsol2\n\n%matplotlib inline\nimport pandas as pd\n\ndf = pd.read_csv(\"data/silicon_valley_details.csv\")\ndf\n\n\n\n\n\n\n\n\n\ncompany\nyear\nrace\ngender\njob_category\ncount\n\n\n\n\n0\n23andMe\n2016\nHispanic_or_Latino\nmale\nExecutives\n0\n\n\n1\n23andMe\n2016\nHispanic_or_Latino\nmale\nManagers\n1\n\n\n2\n23andMe\n2016\nHispanic_or_Latino\nmale\nProfessionals\n7\n\n\n3\n23andMe\n2016\nHispanic_or_Latino\nmale\nTechnicians\n0\n\n\n4\n23andMe\n2016\nHispanic_or_Latino\nmale\nSales workers\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n4435\nSanmina\n2016\nOverall_totals\nNaN\nlaborers and helpers\n4\n\n\n4436\nSanmina\n2016\nOverall_totals\nNaN\nService workers\n57\n\n\n4437\nSanmina\n2016\nOverall_totals\nNaN\nTotals\n5205\n\n\n4438\nSanmina\n2016\nOverall_totals\nNaN\nPrevious_totals\n5615\n\n\n4439\nSanmina\n2016\nOverall_totals\nNaN\nManagers\n591\n\n\n\n\n4440 rows × 6 columns\n\n\n\n\n\nboolean_adobe = df['company'] == 'Adobe'\nboolean_all_races = df['race'] == 'Overall_totals'\ndf[boolean_adobe & boolean_all_races]\n\n\n\n\n\n\n\n\n\ncompany\nyear\nrace\ngender\njob_category\ncount\n\n\n\n\n333\nAdobe\n2016\nOverall_totals\nNaN\nExecutives\n93\n\n\n334\nAdobe\n2016\nOverall_totals\nNaN\nManagers\n2448\n\n\n335\nAdobe\n2016\nOverall_totals\nNaN\nProfessionals\n3028\n\n\n336\nAdobe\n2016\nOverall_totals\nNaN\nTechnicians\n0\n\n\n337\nAdobe\n2016\nOverall_totals\nNaN\nSales workers\n1270\n\n\n338\nAdobe\n2016\nOverall_totals\nNaN\nAdministrative support\n323\n\n\n339\nAdobe\n2016\nOverall_totals\nNaN\nCraft workers\n0\n\n\n340\nAdobe\n2016\nOverall_totals\nNaN\noperatives\n0\n\n\n341\nAdobe\n2016\nOverall_totals\nNaN\nlaborers and helpers\n0\n\n\n342\nAdobe\n2016\nOverall_totals\nNaN\nService workers\n0\n\n\n343\nAdobe\n2016\nOverall_totals\nNaN\nTotals\n7162\n\n\n344\nAdobe\n2016\nOverall_totals\nNaN\nPrevious_totals\n6581\n\n\n\n\n\n\n\n\n\nboolean_adobe = df['company'] == 'Adobe'\nboolean_all_races = df['race'] == 'Overall_totals'\nboolean_count = df['count'] != 0\nboolean_job_category = (df['job_category'] != 'Totals') & (df['job_category'] != 'Previous_totals')\n\ndf_adobe = df[boolean_adobe & boolean_all_races & boolean_count & boolean_job_category]\ndf_adobe\n\n\n\n\n\n\n\n\n\ncompany\nyear\nrace\ngender\njob_category\ncount\n\n\n\n\n333\nAdobe\n2016\nOverall_totals\nNaN\nExecutives\n93\n\n\n334\nAdobe\n2016\nOverall_totals\nNaN\nManagers\n2448\n\n\n335\nAdobe\n2016\nOverall_totals\nNaN\nProfessionals\n3028\n\n\n337\nAdobe\n2016\nOverall_totals\nNaN\nSales workers\n1270\n\n\n338\nAdobe\n2016\nOverall_totals\nNaN\nAdministrative support\n323\n\n\n\n\n\n\n\n\n\ndf_adobe.set_index('job_category', inplace=True)\n\n\ndf_adobe.plot(kind='pie', y= 'count')\n\n\n\n\n\n\n\n\n- 틀린 이유 : 굳이 count가 되어 있는 테이블을 groupby로 sum하는 과정에서 계산이 잘못된 거 같음..\n\n항상 데이터를 먼저 뜯어보는 습관을 기르자…",
    "crumbs": [
      "Posts",
      "CS",
      "00. DA",
      "02. 데이터 분석과 시각화",
      "00. 시각화와 그래프"
    ]
  },
  {
    "objectID": "posts/CS/00. DA/02. 데이터 분석과 시각화/00. 시각화와 그래프.html#실습-1.-국가지표-분석하기",
    "href": "posts/CS/00. DA/02. 데이터 분석과 시각화/00. 시각화와 그래프.html#실습-1.-국가지표-분석하기",
    "title": "00. 시각화와 그래프",
    "section": "실습 1. 국가지표 분석하기",
    "text": "실습 1. 국가지표 분석하기\n\nimport pandas as pd\n%matplotlib inline\n\ndf = pd.read_csv(\"data/world_indexes.csv\")\n\n\ndf.head()\n\n\n\n\n\n\n\n\n\nId\nHuman Development Index HDI-2014\nGini coefficient 2005-2013\nAdolescent birth rate 15-19 per 100k 20102015\nBirth registration funder age 5 2005-2013\nCarbon dioxide emissionsAverage annual growth\nCarbon dioxide emissions per capita 2011 Tones\nChange forest percentable 1900 to 2012\nChange mobile usage 2009 2014\nConsumer price index 2013\n...\nRenewable sources percentage of total 2012\nResearch and development expenditure 2005-2012\nSecondary 2008-2014\nShare of seats in parliament percentage held by womand 2014\nStock of immigrants percentage of population 2013\nTaxes on income profit and capital gain 205 2013\nTertiary -2008-2014\nTotal tax revenue of GDP 2005-2013\nTuberculosis rate per thousands 2012\nUnder-five Mortality 2013 thousands\n\n\n\n\n0\nNorway\n0.943877\n26.83\n7.834\n100.0\n0.778925\n9.192879\n11.914567\n5.22\n104.194175\n...\n47.752676\n1.65474\n111.06130\n39.644970\n13.772622\n31.798391\n74.10112\n27.288097\n0.14\n2.8\n\n\n1\nAustralia\n0.934958\n34.01\n12.059\n100.0\n1.090351\n16.519210\n-4.561812\n30.27\n107.789440\n...\n4.632202\n2.38562\n135.53543\n30.530974\n27.711793\n65.333748\n86.33409\n21.361426\n0.19\n4.0\n\n\n2\nSwitzerland\n0.929613\n32.35\n1.900\n100.0\n-1.101254\n4.625230\n8.567416\n16.72\n99.317229\n...\n49.659398\n2.87046\n96.30638\n28.455285\n28.906998\n22.673299\n55.56190\n9.759124\n0.22\n4.2\n\n\n3\nDenmark\n0.923328\n26.88\n5.101\n100.0\n-1.767733\n7.248329\n23.029974\n1.83\n106.057718\n...\n26.767245\n2.98416\n124.65927\n37.988827\n9.909512\n39.677938\n79.59763\n33.395651\n0.40\n3.5\n\n\n4\nNetherlands\n0.921794\n28.87\n6.165\n100.0\n-0.252734\n10.064490\n5.922602\n-4.31\n107.474154\n...\n6.671366\n2.15676\n129.91277\n36.888889\n11.724418\n23.533104\n77.34356\n19.724059\n0.17\n4.0\n\n\n\n\n5 rows × 66 columns\n\n\n\n\n다음 중 가장 연관성이 깊은 지표를 찾기!\n\n기대 수명 - 인터넷 사용자 비율\n숲 면적 비율 - 탄소 배출 증가율\n인터넷 사용자 비율 - 숲 면적 비율\n기대 수명 - 탄소 배출 증가율\n기대 수명 - 숲 면적 비율\n\n\nimport matplotlib.pyplot as plt\n\nfig, axes = plt.subplots(1,5 ,figsize = (12,4))\n\nax1,ax2,ax3,ax4,ax5 = axes\n\nax1.plot(\"Life expectancy at birth- years\",\n         'Internet users percentage of population 2014', \"b.\", data = df)\n\nax2.plot(\"Forest area percentage of total land area 2012\",\n         'Carbon dioxide emissionsAverage annual growth', \"b.\", data = df)\n\nax3.plot(\"Internet users percentage of population 2014\",\n         'Forest area percentage of total land area 2012', \"b.\", data = df)\n\nax4.plot( 'Life expectancy at birth- years',  \n         'Carbon dioxide emissionsAverage annual growth',  \"b.\", data = df)\n\nax5.plot( 'Life expectancy at birth- years',  \n         'Forest area percentage of total land area 2012',  \"b.\", data = df)\n\n\n\n\n\n\n\n\n- 그래프를 그려 시각화해본 결과 1. 기대수명-인터넷 사용자 비율이 가장 연관성이 있어 보인다.",
    "crumbs": [
      "Posts",
      "CS",
      "00. DA",
      "02. 데이터 분석과 시각화",
      "00. 시각화와 그래프"
    ]
  },
  {
    "objectID": "posts/CS/00. DA/01. DataFrame 다루기/02. 데이터 변형하기.html",
    "href": "posts/CS/00. DA/01. DataFrame 다루기/02. 데이터 변형하기.html",
    "title": "02. 데이터 변형하기",
    "section": "",
    "text": "import pandas as pd",
    "crumbs": [
      "Posts",
      "CS",
      "00. DA",
      "01. DataFrame 다루기",
      "02. 데이터 변형하기"
    ]
  },
  {
    "objectID": "posts/CS/00. DA/01. DataFrame 다루기/02. 데이터 변형하기.html#sol1",
    "href": "posts/CS/00. DA/01. DataFrame 다루기/02. 데이터 변형하기.html#sol1",
    "title": "02. 데이터 변형하기",
    "section": "sol1",
    "text": "sol1\n\ndf[\"합격 여부\"] = [True if (i&gt;=250) & (j&gt;=250) & (i+j &gt;= 600) else False for i,j in zip(df.LC, df.RC)]  \n\n\ndf\n\n\n\n\n\n\n\n\n\nGender\nLC\nRC\n합격 여부\n\n\n\n\n0\nfemale\n315\n320\nTrue\n\n\n1\nfemale\n430\n245\nFalse\n\n\n2\nfemale\n430\n475\nTrue\n\n\n3\nmale\n180\n220\nFalse\n\n\n4\nmale\n325\n350\nTrue\n\n\n5\nfemale\n295\n400\nTrue\n\n\n6\nfemale\n405\n475\nTrue\n\n\n7\nmale\n155\n150\nFalse\n\n\n8\nmale\n280\n315\nFalse\n\n\n9\nfemale\n215\n475\nFalse",
    "crumbs": [
      "Posts",
      "CS",
      "00. DA",
      "01. DataFrame 다루기",
      "02. 데이터 변형하기"
    ]
  },
  {
    "objectID": "posts/CS/00. DA/01. DataFrame 다루기/02. 데이터 변형하기.html#sol2",
    "href": "posts/CS/00. DA/01. DataFrame 다루기/02. 데이터 변형하기.html#sol2",
    "title": "02. 데이터 변형하기",
    "section": "sol2",
    "text": "sol2\n\npass_total = df['LC'] + df['RC'] &gt;= 600\npass_both = (df['LC'] &gt;= 250) & (df['RC'] &gt;= 250)\ndf['합격 여부'] = pass_total & pass_both\n\n\ndf\n\n\n\n\n\n\n\n\n\nGender\nLC\nRC\n합격 여부\n\n\n\n\n0\nfemale\n315\n320\nTrue\n\n\n1\nfemale\n430\n245\nFalse\n\n\n2\nfemale\n430\n475\nTrue\n\n\n3\nmale\n180\n220\nFalse\n\n\n4\nmale\n325\n350\nTrue\n\n\n5\nfemale\n295\n400\nTrue\n\n\n6\nfemale\n405\n475\nTrue\n\n\n7\nmale\n155\n150\nFalse\n\n\n8\nmale\n280\n315\nFalse\n\n\n9\nfemale\n215\n475\nFalse\n\n\n\n\n\n\n\n\n- cowork 할 떄는 리스트 컴프리헨션보다 저렇게 가독성 있는 코드를 짜는게 좋을 것 같다.",
    "crumbs": [
      "Posts",
      "CS",
      "00. DA",
      "01. DataFrame 다루기",
      "02. 데이터 변형하기"
    ]
  },
  {
    "objectID": "posts/CS/00. DA/01. DataFrame 다루기/02. 데이터 변형하기.html#sol1-1",
    "href": "posts/CS/00. DA/01. DataFrame 다루기/02. 데이터 변형하기.html#sol1-1",
    "title": "02. 데이터 변형하기",
    "section": "sol1",
    "text": "sol1\n1 첫 번째 컬럼에 2를 곱하기\n\ndf = pd.read_csv('data/Puzzle_before.csv')\n\n\ndf[\"A\"] = df[\"A\"]*2\n\n2 B~E 열 까지 80점 이상이면 1, 아니면 0으로 값 교체\n\ndf.loc[:,\"B\":\"E\"] = df.loc[:,\"B\":\"E\"].applymap(lambda x : 1 if x &gt;=80 else 0)\n\nC:\\Users\\rkdcj\\AppData\\Local\\Temp\\ipykernel_10248\\2975510319.py:1: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n  df.loc[:,\"B\":\"E\"] = df.loc[:,\"B\":\"E\"].applymap(lambda x : 1 if x &gt;=80 else 0)\n\n\n3 F열의 2번째 값을 99로 교체\n\ndf.loc[2, \"F\"] = 99\n\n\ndf\n\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nE\nF\n\n\n\n\n0\n2\n0\n0\n1\n0\n13\n\n\n1\n4\n0\n0\n1\n0\n24\n\n\n2\n6\n1\n0\n1\n0\n99\n\n\n3\n8\n1\n1\n0\n1\n78\n\n\n4\n10\n0\n1\n0\n1\n61",
    "crumbs": [
      "Posts",
      "CS",
      "00. DA",
      "01. DataFrame 다루기",
      "02. 데이터 변형하기"
    ]
  },
  {
    "objectID": "posts/CS/00. DA/01. DataFrame 다루기/02. 데이터 변형하기.html#sol2-1",
    "href": "posts/CS/00. DA/01. DataFrame 다루기/02. 데이터 변형하기.html#sol2-1",
    "title": "02. 데이터 변형하기",
    "section": "sol2",
    "text": "sol2\n\nimport pandas as pd\n\ndf = pd.read_csv('data/Puzzle_before.csv')\n\ndf['A'] = df['A'] * 2\ndf[df.loc[:, 'B':'E'] &lt; 80] = 0\ndf[df.loc[:, 'B':'E'] &gt;= 80] = 1\ndf.loc[2, 'F'] = 99\n\n# 테스트 코드\ndf\n\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nE\nF\n\n\n\n\n0\n2\n0\n0\n1\n0\n13\n\n\n1\n4\n0\n0\n1\n0\n24\n\n\n2\n6\n1\n0\n1\n0\n99\n\n\n3\n8\n1\n1\n0\n1\n78\n\n\n4\n10\n0\n1\n0\n1\n61",
    "crumbs": [
      "Posts",
      "CS",
      "00. DA",
      "01. DataFrame 다루기",
      "02. 데이터 변형하기"
    ]
  },
  {
    "objectID": "posts/CS/00. DA/00. 데이터 사이언스 시작하기/extra 00. R vs Python.html",
    "href": "posts/CS/00. DA/00. 데이터 사이언스 시작하기/extra 00. R vs Python.html",
    "title": "extra 00. R vs Python",
    "section": "",
    "text": "1 통계분석을 위해 만들어진 언어\n\n1993년 뉴질랜드 오클랜드 대학의 통계학과 교수인 로버트 젠틀맨과 로스 이나카이라는 분이 만들었음(정말, 고마운 분들)\n\n2 통계분석과 시각화 측면에서는 python보다 좋다고 알려짐\n3 또한, 애초에 프로그래밍을 처음 배우는 사람도 손쉽게 배울 수 있도록 만들어짐(만들 당시 개발 목표)\n\n\n\n1 python은 1991년 귀도 반 로섬이 크리스마스에 연구실 출근했다가 문 닫혀서 심심풀이로 만든 언어\n2 코미디 프로그램인 몬티 파이썬의 날아다니는 써커스 Monty Python's Flying Circus에서 python 이름을 따옴\n3 그리고, 데이터 사이언스에서 초기에는 R이 우세했으나 Python이 현재는 앞지름\n\n사실 앞질른 것은 맞는데, 요즘은 그런지 잘 모르겠음, 평가 기준도 애매함.\n\n\n\n\n1 음… 일단 데이터 사이언스라는 직무를 보았을 때 numpy, pandas, tensorflow의 등장으로 python이 훨씬 편해진 것은 맞음\n2 그러나! tidyverse, tidymodel이라는 R의 필살 패키지의 등장으로 그 장벽을 어느 정도 허물은 것 같다.\n3 개인적인 경험\n\n파이프 연산자 %&gt;%는 진짜 너무 편함, python의 백엔드 연산처럼 너무 편하게 쓸 수 있음\nR이 시각화가 더 좋다고 하는데, plotly가 ggplot2보다 훨씬 코드짜기 편하고 그래프 가독성, 인터랙티브 측면에서도 훨씬 좋은 것 같음\n대용량 데이터를 읽어드릴 때 python은 pandas를 이용해서 읽어오는데, 백만 단위가 넘어가면 상당히 오래걸림\nR의 dplyr, data.frame 패키지를 이용하면 이러한 데이터들도 진짜 빨리 읽어올 수 있음!\n\n4 결론\n\n음… 본인에 입맛에 맞는 걸, 그때그때 적절히 사용하면 될 것 같음\n난 아직도, 데이터 50만건만 넘어가도 R로 데이터 전처리하니까…\n그리고 데이터 분석, 모델링이라는 게 결국 전처리가 95%는 차지하는 것 같음\n즉, 두 언어를 비교할 때, 전처리 패키지를 비교하는게 맞다고 생각한다.\nAI 모델 설계할 떄도 아직은 R의 tidymodel 패키지를 안 다루어 보았지만, tidyverse를 다루어 생각해보면 또 압도적이지 않을까라는 생각이 들음\n오늘 그래서, 두 언어에 쓰이는 데이터 로드, 전처리를 코드 길이, 속도 측면에서 비교해보고 싶음",
    "crumbs": [
      "Posts",
      "CS",
      "00. DA",
      "00. 데이터 사이언스 시작하기",
      "extra 00. R vs Python"
    ]
  },
  {
    "objectID": "posts/CS/00. DA/00. 데이터 사이언스 시작하기/extra 00. R vs Python.html#r",
    "href": "posts/CS/00. DA/00. 데이터 사이언스 시작하기/extra 00. R vs Python.html#r",
    "title": "extra 00. R vs Python",
    "section": "",
    "text": "1 통계분석을 위해 만들어진 언어\n\n1993년 뉴질랜드 오클랜드 대학의 통계학과 교수인 로버트 젠틀맨과 로스 이나카이라는 분이 만들었음(정말, 고마운 분들)\n\n2 통계분석과 시각화 측면에서는 python보다 좋다고 알려짐\n3 또한, 애초에 프로그래밍을 처음 배우는 사람도 손쉽게 배울 수 있도록 만들어짐(만들 당시 개발 목표)",
    "crumbs": [
      "Posts",
      "CS",
      "00. DA",
      "00. 데이터 사이언스 시작하기",
      "extra 00. R vs Python"
    ]
  },
  {
    "objectID": "posts/CS/00. DA/00. 데이터 사이언스 시작하기/extra 00. R vs Python.html#python",
    "href": "posts/CS/00. DA/00. 데이터 사이언스 시작하기/extra 00. R vs Python.html#python",
    "title": "extra 00. R vs Python",
    "section": "",
    "text": "1 python은 1991년 귀도 반 로섬이 크리스마스에 연구실 출근했다가 문 닫혀서 심심풀이로 만든 언어\n2 코미디 프로그램인 몬티 파이썬의 날아다니는 써커스 Monty Python's Flying Circus에서 python 이름을 따옴\n3 그리고, 데이터 사이언스에서 초기에는 R이 우세했으나 Python이 현재는 앞지름\n\n사실 앞질른 것은 맞는데, 요즘은 그런지 잘 모르겠음, 평가 기준도 애매함.",
    "crumbs": [
      "Posts",
      "CS",
      "00. DA",
      "00. 데이터 사이언스 시작하기",
      "extra 00. R vs Python"
    ]
  },
  {
    "objectID": "posts/CS/00. DA/00. 데이터 사이언스 시작하기/extra 00. R vs Python.html#일반적으로-알려진-사실에-대한-내-생각",
    "href": "posts/CS/00. DA/00. 데이터 사이언스 시작하기/extra 00. R vs Python.html#일반적으로-알려진-사실에-대한-내-생각",
    "title": "extra 00. R vs Python",
    "section": "",
    "text": "1 음… 일단 데이터 사이언스라는 직무를 보았을 때 numpy, pandas, tensorflow의 등장으로 python이 훨씬 편해진 것은 맞음\n2 그러나! tidyverse, tidymodel이라는 R의 필살 패키지의 등장으로 그 장벽을 어느 정도 허물은 것 같다.\n3 개인적인 경험\n\n파이프 연산자 %&gt;%는 진짜 너무 편함, python의 백엔드 연산처럼 너무 편하게 쓸 수 있음\nR이 시각화가 더 좋다고 하는데, plotly가 ggplot2보다 훨씬 코드짜기 편하고 그래프 가독성, 인터랙티브 측면에서도 훨씬 좋은 것 같음\n대용량 데이터를 읽어드릴 때 python은 pandas를 이용해서 읽어오는데, 백만 단위가 넘어가면 상당히 오래걸림\nR의 dplyr, data.frame 패키지를 이용하면 이러한 데이터들도 진짜 빨리 읽어올 수 있음!\n\n4 결론\n\n음… 본인에 입맛에 맞는 걸, 그때그때 적절히 사용하면 될 것 같음\n난 아직도, 데이터 50만건만 넘어가도 R로 데이터 전처리하니까…\n그리고 데이터 분석, 모델링이라는 게 결국 전처리가 95%는 차지하는 것 같음\n즉, 두 언어를 비교할 때, 전처리 패키지를 비교하는게 맞다고 생각한다.\nAI 모델 설계할 떄도 아직은 R의 tidymodel 패키지를 안 다루어 보았지만, tidyverse를 다루어 생각해보면 또 압도적이지 않을까라는 생각이 들음\n오늘 그래서, 두 언어에 쓰이는 데이터 로드, 전처리를 코드 길이, 속도 측면에서 비교해보고 싶음",
    "crumbs": [
      "Posts",
      "CS",
      "00. DA",
      "00. 데이터 사이언스 시작하기",
      "extra 00. R vs Python"
    ]
  },
  {
    "objectID": "posts/CS/00. DA/00. 데이터 사이언스 시작하기/extra 00. R vs Python.html#샘플데이터-생성",
    "href": "posts/CS/00. DA/00. 데이터 사이언스 시작하기/extra 00. R vs Python.html#샘플데이터-생성",
    "title": "extra 00. R vs Python",
    "section": "샘플데이터 생성",
    "text": "샘플데이터 생성\n- 비교를 위해 가짜 데이터를 만들어 보자(백만개만)\n\nimport pandas as pd\nimport numpy as np\n\n\nX = np.random.random((1000000,10))\nX = pd.DataFrame(X)\n\nname = [\"X\"+str(i) for i in range(10)]\nX.columns = name\n\n\nX.to_csv(\"X.csv\",index = False)",
    "crumbs": [
      "Posts",
      "CS",
      "00. DA",
      "00. 데이터 사이언스 시작하기",
      "extra 00. R vs Python"
    ]
  },
  {
    "objectID": "posts/CS/00. DA/00. 데이터 사이언스 시작하기/extra 00. R vs Python.html#데이터-로드",
    "href": "posts/CS/00. DA/00. 데이터 사이언스 시작하기/extra 00. R vs Python.html#데이터-로드",
    "title": "extra 00. R vs Python",
    "section": "데이터 로드",
    "text": "데이터 로드\n\npython\n\nimport time\n\n\nstart = time.time()\ndf = pd.read_csv(\"X.csv\")\nend  = time.time()\n\n\ntotal = end-start\ntotal\n\n1.315992832183838\n\n\n\nprint(f\"데이터 로드 시간(python) : {total}\")\n\n데이터 로드 시간(python) : 1.315992832183838\n\n\n\n\nR\n\nlibrary(data.table)\nlibrary(tidyverse)\n\n\nstart &lt;- Sys.time()\ndf &lt;- fread(\"X.csv\")\nend &lt;- Sys.time()\n\n\ntotal = end-start\nprint(paste(\"데이터 로드 시간(R) : \",total))\n\n[1] \"데이터 로드 시간(R) :  0.220276832580566\"\n\n\n\n오, 대략 6배 가량 차이남!",
    "crumbs": [
      "Posts",
      "CS",
      "00. DA",
      "00. 데이터 사이언스 시작하기",
      "extra 00. R vs Python"
    ]
  },
  {
    "objectID": "posts/CS/00. DA/00. 데이터 사이언스 시작하기/extra 00. R vs Python.html#데이터-전처리filterselect",
    "href": "posts/CS/00. DA/00. 데이터 사이언스 시작하기/extra 00. R vs Python.html#데이터-전처리filterselect",
    "title": "extra 00. R vs Python",
    "section": "데이터 전처리(filter,select)",
    "text": "데이터 전처리(filter,select)\n- 저장한 데이터에서 x1 ~ x5 까지만 컬럼을 선택하고, x1이 0보다 큰 데이터만 추출해보자\n\npython\n\nimport pandas as pd\nimport time\n\n\ndf = pd.read_csv(\"X.csv\")\n\n\nstart = time.time()\n\nselect_col = [\"X1\",\"X2\",\"X3\",\"X4\",\"X5\"]\n\ndf[select_col].loc[df.X1&gt;0,:]\n\nend  = time.time()\n\n\ntotal = end-start\nprint(f\"데이터 전처리 시간(python) : {total}\")\n\n데이터 전처리 시간(python) : 0.03211045265197754\n\n\n\n\nR\n\nlibrary(data.table)\nlibrary(tidyverse)\n\ndf &lt;- fread(\"X.csv\")\n\n\nstart &lt;- Sys.time()\ndf = df %&gt;% select(X1,X2,X3,X4,X5)  %&gt;% \n            filter(X1 &gt; 0)\nend &lt;- Sys.time()\n\n\ntotal = end-start\nprint(paste(\"데이터 전처리 시간(R) : \",total))\n\n[1] \"데이터 전처리 시간(R) :  0.0224010944366455\"\n\n\n- 전처리도 미세하게 R이 더 빠르다.\n- 심지어 코드 가독성도 R이 더 좋은 것 같음",
    "crumbs": [
      "Posts",
      "CS",
      "00. DA",
      "00. 데이터 사이언스 시작하기",
      "extra 00. R vs Python"
    ]
  },
  {
    "objectID": "posts/CS/00. DA/00. 데이터 사이언스 시작하기/extra 00. R vs Python.html#다중회귀모형-적합",
    "href": "posts/CS/00. DA/00. 데이터 사이언스 시작하기/extra 00. R vs Python.html#다중회귀모형-적합",
    "title": "extra 00. R vs Python",
    "section": "다중회귀모형 적합",
    "text": "다중회귀모형 적합\n\\[x_5 = \\beta_1 x_1+\\beta_2 x_2+\\beta_1 x_3+\\beta_1 x_4+\\beta_1 x_5\\]\n\npython\n\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\nimport time\n\n\ndf = pd.read_csv(\"X.csv\")\n\n\ndf = df[[\"X1\",\"X2\",\"X3\",\"X4\",\"X5\"]]\n\n\nx5 = df[\"X5\"]\nX = df.drop(\"X5\", axis = 1)\n\n\nstart = time.time()\n\nmodel = LinearRegression()\n\nmodel.fit(X,x5)\n\nx5_pred = model.predict(X)\n\nend = time.time()\n\n\ntotal = end-start\n\n\nprint(f\"모형 적합 및 예측 시간(python) : {total}\")\n\n모형 적합 및 예측 시간(python) : 0.1069481372833252\n\n\n\n\nR\n\nlibrary(data.table)\nlibrary(tidyverse)\n\n\ndf &lt;- fread(\"X.csv\")\ndf = df %&gt;% select(X1,X2,X3,X4,X5)  %&gt;% \n            filter(X1 &gt; 0)\n\n\nstart &lt;- Sys.time()\n\nmodel &lt;- lm(X5~. ,data = df)\n\nx5_pred &lt;- predict(model, df)\n\nend &lt;- Sys.time()\n\n\ntotal = end - start\nprint(paste(\"모형 적합 및 예측 시간(R) : \",total))\n\n[1] \"모형 적합 및 예측 시간(R) :  0.293954849243164\"\n\n\n\n간단한 회귀모형 적합에서는 python이 3배 가량 더 빨랐다.",
    "crumbs": [
      "Posts",
      "CS",
      "00. DA",
      "00. 데이터 사이언스 시작하기",
      "extra 00. R vs Python"
    ]
  },
  {
    "objectID": "posts/CS/00. DA/00. 데이터 사이언스 시작하기/01. numpy.html",
    "href": "posts/CS/00. DA/00. 데이터 사이언스 시작하기/01. numpy.html",
    "title": "01. numpy",
    "section": "",
    "text": "- numpy (numerical python)\n\npython에서 복잡한 수식 계산을 위해 만든 모듈\narray라는 데이터 형태를 통해 많은 양의 데이터들을 손쉽게 계산할 수 있음",
    "crumbs": [
      "Posts",
      "CS",
      "00. DA",
      "00. 데이터 사이언스 시작하기",
      "01. numpy"
    ]
  },
  {
    "objectID": "posts/CS/00. DA/00. 데이터 사이언스 시작하기/01. numpy.html#파이썬-리스트를-통해-생성",
    "href": "posts/CS/00. DA/00. 데이터 사이언스 시작하기/01. numpy.html#파이썬-리스트를-통해-생성",
    "title": "01. numpy",
    "section": "1. 파이썬 리스트를 통해 생성",
    "text": "1. 파이썬 리스트를 통해 생성\n1 모듈 불러오기\n\nimport numpy\n\n2 파라미터로 python list 전달\n\na1 = numpy.array([2,3,5,7,11,13,17,19,23,29,31])\na1\n\narray([ 2,  3,  5,  7, 11, 13, 17, 19, 23, 29, 31])\n\n\n3 type확인\n\nndarray? n-dimensional array\n\n\ntype(a1)\n\nnumpy.ndarray\n\n\n4 shape 확인\n\na1.shape\n\n(11,)\n\n\n5 2차원 array\n\na2 = numpy.array([[2,3,5,7],[11,13,17,19],[23,29,31,33]])\na2\n\narray([[ 2,  3,  5,  7],\n       [11, 13, 17, 19],\n       [23, 29, 31, 33]])\n\n\n\ntype(a2)\n\nnumpy.ndarray\n\n\n\na2.shape\n\n(3, 4)\n\n\n5 요소개수 확인\n\na1.size\n\n11\n\n\n\na2.size\n\n12",
    "crumbs": [
      "Posts",
      "CS",
      "00. DA",
      "00. 데이터 사이언스 시작하기",
      "01. numpy"
    ]
  },
  {
    "objectID": "posts/CS/00. DA/00. 데이터 사이언스 시작하기/01. numpy.html#균일한-값으로-생성",
    "href": "posts/CS/00. DA/00. 데이터 사이언스 시작하기/01. numpy.html#균일한-값으로-생성",
    "title": "01. numpy",
    "section": "2. 균일한 값으로 생성",
    "text": "2. 균일한 값으로 생성\n\nnumpy.full(6,7)\n\narray([7, 7, 7, 7, 7, 7])",
    "crumbs": [
      "Posts",
      "CS",
      "00. DA",
      "00. 데이터 사이언스 시작하기",
      "01. numpy"
    ]
  },
  {
    "objectID": "posts/CS/00. DA/00. 데이터 사이언스 시작하기/01. numpy.html#모든-값을-0으로-생성",
    "href": "posts/CS/00. DA/00. 데이터 사이언스 시작하기/01. numpy.html#모든-값을-0으로-생성",
    "title": "01. numpy",
    "section": "3. 모든 값을 0으로 생성",
    "text": "3. 모든 값을 0으로 생성\n\nnumpy.full(6,0)\n\narray([0, 0, 0, 0, 0, 0])\n\n\n\nnumpy.zeros(6, dtype = int)\n\narray([0, 0, 0, 0, 0, 0])",
    "crumbs": [
      "Posts",
      "CS",
      "00. DA",
      "00. 데이터 사이언스 시작하기",
      "01. numpy"
    ]
  },
  {
    "objectID": "posts/CS/00. DA/00. 데이터 사이언스 시작하기/01. numpy.html#모든-값을-1로-생성",
    "href": "posts/CS/00. DA/00. 데이터 사이언스 시작하기/01. numpy.html#모든-값을-1로-생성",
    "title": "01. numpy",
    "section": "4. 모든 값을 1로 생성",
    "text": "4. 모든 값을 1로 생성\n\nnumpy.full(6,1)\n\narray([1, 1, 1, 1, 1, 1])\n\n\n\nnumpy.ones(6, dtype = int)\n\narray([1, 1, 1, 1, 1, 1])",
    "crumbs": [
      "Posts",
      "CS",
      "00. DA",
      "00. 데이터 사이언스 시작하기",
      "01. numpy"
    ]
  },
  {
    "objectID": "posts/CS/00. DA/00. 데이터 사이언스 시작하기/01. numpy.html#랜덤한-값들로-생성",
    "href": "posts/CS/00. DA/00. 데이터 사이언스 시작하기/01. numpy.html#랜덤한-값들로-생성",
    "title": "01. numpy",
    "section": "5. 랜덤한 값들로 생성",
    "text": "5. 랜덤한 값들로 생성\n\nnumpy.random.random(6)\n\narray([0.01877569, 0.64836263, 0.24382533, 0.15821645, 0.10587735,\n       0.0708292 ])\n\n\n\nnumpy.random.random(6)\n\narray([0.08551933, 0.79689483, 0.61449187, 0.55877517, 0.59213215,\n       0.59484228])",
    "crumbs": [
      "Posts",
      "CS",
      "00. DA",
      "00. 데이터 사이언스 시작하기",
      "01. numpy"
    ]
  },
  {
    "objectID": "posts/CS/00. DA/00. 데이터 사이언스 시작하기/01. numpy.html#연속된-배열-생성",
    "href": "posts/CS/00. DA/00. 데이터 사이언스 시작하기/01. numpy.html#연속된-배열-생성",
    "title": "01. numpy",
    "section": "6. 연속된 배열 생성",
    "text": "6. 연속된 배열 생성\n\nnumpy.arange(6)\n\narray([0, 1, 2, 3, 4, 5])\n\n\n\nnumpy.arange(1,7)\n\narray([1, 2, 3, 4, 5, 6])\n\n\n\nnumpy.arange(1,7,2)\n\narray([1, 3, 5])",
    "crumbs": [
      "Posts",
      "CS",
      "00. DA",
      "00. 데이터 사이언스 시작하기",
      "01. numpy"
    ]
  },
  {
    "objectID": "posts/CS/00. DA/00. 데이터 사이언스 시작하기/01. numpy.html#최댓값-최솟값",
    "href": "posts/CS/00. DA/00. 데이터 사이언스 시작하기/01. numpy.html#최댓값-최솟값",
    "title": "01. numpy",
    "section": "1. 최댓값, 최솟값",
    "text": "1. 최댓값, 최솟값\n\nimport numpy as np\n\narray1 = np.array([14, 6, 13, 21, 23, 31, 9, 5])\n\nprint(array1.max()) # 최댓값\nprint(array1.min()) # 최솟값\n\n31\n5",
    "crumbs": [
      "Posts",
      "CS",
      "00. DA",
      "00. 데이터 사이언스 시작하기",
      "01. numpy"
    ]
  },
  {
    "objectID": "posts/CS/00. DA/00. 데이터 사이언스 시작하기/01. numpy.html#평균",
    "href": "posts/CS/00. DA/00. 데이터 사이언스 시작하기/01. numpy.html#평균",
    "title": "01. numpy",
    "section": "2. 평균",
    "text": "2. 평균\n\nimport numpy as np\n\narray1 = np.array([14, 6, 13, 21, 23, 31, 9, 5])\n\nprint(array1.mean()) # 평균값\n\n15.25",
    "crumbs": [
      "Posts",
      "CS",
      "00. DA",
      "00. 데이터 사이언스 시작하기",
      "01. numpy"
    ]
  },
  {
    "objectID": "posts/CS/00. DA/00. 데이터 사이언스 시작하기/01. numpy.html#중앙값",
    "href": "posts/CS/00. DA/00. 데이터 사이언스 시작하기/01. numpy.html#중앙값",
    "title": "01. numpy",
    "section": "3. 중앙값",
    "text": "3. 중앙값\n\nimport numpy as np\n\narray1 = np.array([8, 12, 9, 15, 16])\narray2 = np.array([14, 6, 13, 21, 23, 31, 9, 5])\n\nprint(np.median(array1)) # 중앙값\nprint(np.median(array2)) # 중앙값\n\n12.0\n13.5",
    "crumbs": [
      "Posts",
      "CS",
      "00. DA",
      "00. 데이터 사이언스 시작하기",
      "01. numpy"
    ]
  },
  {
    "objectID": "posts/CS/00. DA/00. 데이터 사이언스 시작하기/01. numpy.html#표준편차-분산",
    "href": "posts/CS/00. DA/00. 데이터 사이언스 시작하기/01. numpy.html#표준편차-분산",
    "title": "01. numpy",
    "section": "4. 표준편차, 분산",
    "text": "4. 표준편차, 분산\n\nimport numpy as np\n\narray1 = np.array([14, 6, 13, 21, 23, 31, 9, 5])\n\nprint(array1.std()) # 표준 편차\nprint(array1.var()) # 분산\n\n8.496322733983215\n72.1875",
    "crumbs": [
      "Posts",
      "CS",
      "00. DA",
      "00. 데이터 사이언스 시작하기",
      "01. numpy"
    ]
  },
  {
    "objectID": "posts/CA/2023-03-19-00. 코드잇 기업분석 (1).html",
    "href": "posts/CA/2023-03-19-00. 코드잇 기업분석 (1).html",
    "title": "00. CA (1)",
    "section": "",
    "text": "코드잇 소개\n- 구독형 교육 서비스를 제공하는 기업\n\n넷플릭스, 멜론처럼 구독 시 원하는 강의를 무제한으로 들을 수 있는 서비스를 제공한다. (굉장히 신선함)\n넷플릭스와 다른점은 콘텐츠를 내부 콘텐츠 팀에서 제작하는 방식!\n그렇기 때문에 커리큘럼에 대한 고민을 많이 한다…(맞는 말임)\n다른 플랫폼들은 외주 형식으로 강의를 만드는데 일관적이지 않다.\n코드잇은 하나의 큰 그림을 그려놓고 세부적으로 기획을 해서 제작하기 때문에 일관성이 높다.(ㅇㅇ 확실히 외주를 맡기면 강사마다 강의하는 방식과 사용하는 함수 같은 것들이 다르니 좀 혼동 될 때가 많았음)\n또한, 코드잇은 강의를 짬내서 만드는 게 아닌 풀타임으로 콘텐츠 제작에 몰두하기 때문에 강의 퀄리티가 높을 수 밖에 없는 것 같음\n새로운 사람이 와도 일관되게 좋은 강의가 나오도록 가이드를 계속해서 개선을 하고 있다.\n\n\n\n\n콘텐츠 프로듀서?\n- 흔히 있는 강사를 말하는 것이 아니다!!\n- 강사가 아닌 콘텐츠 프로듀서라고 이름을 붙인 이유!\n\n단순히 강의를 하는 포지션이 아니라 정제된 콘텐츠를 만드는 것이 핵심\n해당 분야에 대한 리서치, 관련된 강의, 이를 통해 어떻게 최적화된 커리큘럼을 만들 수 있는지!\n주요 업무는 리서치, 공부, 기획, 콘텐츠 제작인 것 같다….!\n학원 강사와의 차이점 : 팀으로 움직인다. \\(\\to\\) 한 가지 주제에 대해서 두 명의 프로듀서가 참여를 한다던지, 영상 편집은 디자이너… 이렇게 팀으로 움직이기 때문에 이러한 점이 강사와의 차이점이다.\n애니메이션을 주로 활용해서 컨텐츠를 제작함.(필요에 따라 사람이 직접 나오는 영상을 촬영함)\n\n\n\n강의 체계\n- 토픽이라는 단위로 구성\n- 하나의 토픽 안에 영상들이 들어가 있음\n\n보통 2, 3개월 정도에 걸려서 하나의 토픽을 만든다.\n일단, 강의를 몇 개 들어보고 차별성을 직접 느껴보는 게 좋을 것 같음.\n\n\\(x+y\\)\n\\(2x + y\\)",
    "crumbs": [
      "Posts",
      "CA",
      "00. CA (1)"
    ]
  },
  {
    "objectID": "posts/b2b/2024-02-25-00. B2B 세일즈.html",
    "href": "posts/b2b/2024-02-25-00. B2B 세일즈.html",
    "title": "00. B2B 세일즈",
    "section": "",
    "text": "- 정의 : 기업이 기업을 대상으로 제품이나 서비스를 영업하는 것\n\n자신 또는 자사의 제품이 고객이 가지고 있는 pain point를 해결하는데 어떻게 활용될 수 있는지 제시하는 것!\n\n- 미션\n\n왕성한 영업 활동? (X) \\(\\to\\) 활동 &lt; 결과\n고객과의 관계? (X) \\(\\to\\) 중요하지만 주가 되지 않음\n좋은 콘텐츠 만들기? (X)\n기.승.전 매출 (O)\n\n\n\n\n\n\n\n\n\n\n\n목록\nB2B\nB2C\n\n\n\n\n대상\n기업(조직)\n개인\n\n\n의사 결정자\n현업, 구매부, 재무팀 등, 의사결정자, 이해관계자 포함\n개인\n\n\n구매 동기\n이윤추구\n개인 삶의 질 향상\n\n\n구매 결정 프로세스\n복잡함\n관여도에 따라 다르지만, 상대적으로 짧고 간단함\n\n\n기존 제품 / 서비스 교체\n복잡합\n간단\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n목록\n마케팅\n세일즈\n\n\n\n\n대상\n불특정 다수(1:다)\n잠재고객 (1:1)\n\n\n목적\n알리기\n매출 전환(Conversation)\n\n\n활동 예시\n검색엔진 최적화 콘텐츠 발행, 웹이나 기획 등\n가망 고객 발굴(Propecting), 이메일/콜링 캠페인, 데모, 응대\n\n\nCall to Action(고객 유도 행동)\n홈페이지 방문, 소개서, 다운로드\n문의, 미팅 및 견적 요청\n\n\n\n\n\n\n\n\n\n\n\n1 VISTROR, LEAD(마케팅 영역) : 유용한 컨텐츠를 이용하여 잠재고객 발굴\n\n우리에게 관심을 가지고 마케팅 활동을 통해 연락처를 제공한 사람들(잠재고객)\n\n2 PROSPECT (세일즈 영역 ) : 데모, 미팅을 통해 제품/서비스의 혜택을 알리고 고객이 당면한 문제를 어떻게 해결할 수 있는지 알림\n\n잠재고객 중 실제 고객이 될 가능성이 높은 대상을 선별, 세일즈 활동을 활발하게 하는 대상\n\n3 CLIENT (세일즈 영역) : 협상/계약\n\n우리가 제공하는 제품/서비스를 사용하고 있는 기업\n\n4 LOYAL CLIENT (세일즈 영역) : 재구매, 서비스 연장 + 가치 있는 제품을 추가적으로 소개\n\\(\\divideontimes\\) 세일즈 퍼널 : 구매자가 여정을 거치면서 그 숫자가 줄어두는 것이 깔대기 모양과 비슷하여 ’퍼널’이라고 불림\n\n\n\n\n\n\n\n\n\n\n목록\nHUNTER\nFARMER\n\n\n\n\n정의\n지속적인 새로운 고객 발굴\n기존 고객과 기존 영역 성장\n\n\n목표\n새로운 고객발굴을 통한 매출 증대\n기존 고객의 매출 성장\n\n\n고객 발굴 상황\nOutbound\nInbound\n\n\n세일즈 상대\n새로운 잠재 기업\n기존 사용 부서 및 새 구매 부서\n\n\n성과 측정\n새 기업 고객 수, 매출액\n실 사용 부서(사용자) 수, 매출액\n\n\n이름\nNew Sales, 고객 개발\n어카운트 매니저, Client Success\n\n\n\n\n\n\n\n1 B2B와 B2C의 가장 큰 차이는 대상이며, 구매 결정 프로세스와 서비스 교체에 있어서 B2B가 상대적으로 복잡하다.\n2 마케팅과 세일즈의 차이는 마케팅은 알리기(Awareness) 가 핵심, 세일즈는 문의를 늘려 매출로 전환하는 것이 핵심이다.",
    "crumbs": [
      "Posts",
      "B2b",
      "00. B2B 세일즈"
    ]
  },
  {
    "objectID": "posts/b2b/2024-02-25-00. B2B 세일즈.html#b2b-vs-b2c",
    "href": "posts/b2b/2024-02-25-00. B2B 세일즈.html#b2b-vs-b2c",
    "title": "00. B2B 세일즈",
    "section": "",
    "text": "목록\nB2B\nB2C\n\n\n\n\n대상\n기업(조직)\n개인\n\n\n의사 결정자\n현업, 구매부, 재무팀 등, 의사결정자, 이해관계자 포함\n개인\n\n\n구매 동기\n이윤추구\n개인 삶의 질 향상\n\n\n구매 결정 프로세스\n복잡함\n관여도에 따라 다르지만, 상대적으로 짧고 간단함\n\n\n기존 제품 / 서비스 교체\n복잡합\n간단",
    "crumbs": [
      "Posts",
      "B2b",
      "00. B2B 세일즈"
    ]
  },
  {
    "objectID": "posts/b2b/2024-02-25-00. B2B 세일즈.html#마케팅-vs-세일즈",
    "href": "posts/b2b/2024-02-25-00. B2B 세일즈.html#마케팅-vs-세일즈",
    "title": "00. B2B 세일즈",
    "section": "",
    "text": "목록\n마케팅\n세일즈\n\n\n\n\n대상\n불특정 다수(1:다)\n잠재고객 (1:1)\n\n\n목적\n알리기\n매출 전환(Conversation)\n\n\n활동 예시\n검색엔진 최적화 콘텐츠 발행, 웹이나 기획 등\n가망 고객 발굴(Propecting), 이메일/콜링 캠페인, 데모, 응대\n\n\nCall to Action(고객 유도 행동)\n홈페이지 방문, 소개서, 다운로드\n문의, 미팅 및 견적 요청",
    "crumbs": [
      "Posts",
      "B2b",
      "00. B2B 세일즈"
    ]
  },
  {
    "objectID": "posts/b2b/2024-02-25-00. B2B 세일즈.html#구매자-여정에서-세일즈의-위치",
    "href": "posts/b2b/2024-02-25-00. B2B 세일즈.html#구매자-여정에서-세일즈의-위치",
    "title": "00. B2B 세일즈",
    "section": "",
    "text": "1 VISTROR, LEAD(마케팅 영역) : 유용한 컨텐츠를 이용하여 잠재고객 발굴\n\n우리에게 관심을 가지고 마케팅 활동을 통해 연락처를 제공한 사람들(잠재고객)\n\n2 PROSPECT (세일즈 영역 ) : 데모, 미팅을 통해 제품/서비스의 혜택을 알리고 고객이 당면한 문제를 어떻게 해결할 수 있는지 알림\n\n잠재고객 중 실제 고객이 될 가능성이 높은 대상을 선별, 세일즈 활동을 활발하게 하는 대상\n\n3 CLIENT (세일즈 영역) : 협상/계약\n\n우리가 제공하는 제품/서비스를 사용하고 있는 기업\n\n4 LOYAL CLIENT (세일즈 영역) : 재구매, 서비스 연장 + 가치 있는 제품을 추가적으로 소개\n\\(\\divideontimes\\) 세일즈 퍼널 : 구매자가 여정을 거치면서 그 숫자가 줄어두는 것이 깔대기 모양과 비슷하여 ’퍼널’이라고 불림",
    "crumbs": [
      "Posts",
      "B2b",
      "00. B2B 세일즈"
    ]
  },
  {
    "objectID": "posts/b2b/2024-02-25-00. B2B 세일즈.html#b2b-세일즈에-다양한-역할",
    "href": "posts/b2b/2024-02-25-00. B2B 세일즈.html#b2b-세일즈에-다양한-역할",
    "title": "00. B2B 세일즈",
    "section": "",
    "text": "목록\nHUNTER\nFARMER\n\n\n\n\n정의\n지속적인 새로운 고객 발굴\n기존 고객과 기존 영역 성장\n\n\n목표\n새로운 고객발굴을 통한 매출 증대\n기존 고객의 매출 성장\n\n\n고객 발굴 상황\nOutbound\nInbound\n\n\n세일즈 상대\n새로운 잠재 기업\n기존 사용 부서 및 새 구매 부서\n\n\n성과 측정\n새 기업 고객 수, 매출액\n실 사용 부서(사용자) 수, 매출액\n\n\n이름\nNew Sales, 고객 개발\n어카운트 매니저, Client Success",
    "crumbs": [
      "Posts",
      "B2b",
      "00. B2B 세일즈"
    ]
  },
  {
    "objectID": "posts/b2b/2024-02-25-00. B2B 세일즈.html#summary",
    "href": "posts/b2b/2024-02-25-00. B2B 세일즈.html#summary",
    "title": "00. B2B 세일즈",
    "section": "",
    "text": "1 B2B와 B2C의 가장 큰 차이는 대상이며, 구매 결정 프로세스와 서비스 교체에 있어서 B2B가 상대적으로 복잡하다.\n2 마케팅과 세일즈의 차이는 마케팅은 알리기(Awareness) 가 핵심, 세일즈는 문의를 늘려 매출로 전환하는 것이 핵심이다.",
    "crumbs": [
      "Posts",
      "B2b",
      "00. B2B 세일즈"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "끄적끄적",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nMay 18, 2024\n\n\n00. Intro\n\n\nGC \n\n\n\n\nMay 17, 2024\n\n\n00. Intro\n\n\nGC \n\n\n\n\nMay 17, 2024\n\n\n01. 내용 기반 추천\n\n\nGC \n\n\n\n\nMay 15, 2024\n\n\n02. 자연어 전처리 (2)\n\n\nGC \n\n\n\n\nMay 15, 2024\n\n\n03. 감성 분석\n\n\nGC \n\n\n\n\nMay 15, 2024\n\n\n04. 한국어 자연어 처리\n\n\nGC \n\n\n\n\nMay 14, 2024\n\n\n00. Intro\n\n\nGC \n\n\n\n\nMay 14, 2024\n\n\n01. 자연어 전처리 (1)\n\n\nGC \n\n\n\n\nApr 30, 2024\n\n\n04. 인사이트 도출\n\n\nGC \n\n\n\n\nApr 23, 2024\n\n\n03. EDA\n\n\nGC \n\n\n\n\nApr 22, 2024\n\n\n00. 시각화와 그래프\n\n\nGC \n\n\n\n\nApr 22, 2024\n\n\n01. Seaborn 시각화\n\n\nGC \n\n\n\n\nApr 22, 2024\n\n\n02. 통계 기본 상식식\n\n\nGC \n\n\n\n\nApr 19, 2024\n\n\n01. DataFrame 인덱싱\n\n\nGC \n\n\n\n\nApr 19, 2024\n\n\n02. 데이터 변형하기\n\n\nGC \n\n\n\n\nApr 19, 2024\n\n\n03. 큰 데이터 다루기\n\n\nGC \n\n\n\n\nApr 14, 2024\n\n\n00. Intro\n\n\nGC \n\n\n\n\nApr 14, 2024\n\n\n01. numpy\n\n\nGC \n\n\n\n\nApr 14, 2024\n\n\n02. pandas\n\n\ngc \n\n\n\n\nApr 14, 2024\n\n\nextra 00. R vs Python\n\n\ngc \n\n\n\n\nMar 18, 2024\n\n\n00. CA (1)\n\n\nGC \n\n\n\n\nMar 17, 2024\n\n\n01. 제안전략수립\n\n\ngc \n\n\n\n\nFeb 25, 2023\n\n\n00. B2B 세일즈\n\n\nGC \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/b2b/2024-03-17-01. 제안전략수립.html",
    "href": "posts/b2b/2024-03-17-01. 제안전략수립.html",
    "title": "01. 제안전략수립",
    "section": "",
    "text": "- 사업의 개념과 도메인 지식이 필요\n- 문제정의\n\n\n\nB2B 산업의 환경구조 \\(\\to\\) 환경분석 프레임\n\n\n\n- 고객사에 대한 충분한 이해가 필요\n\n고객정의(구매센터) \\(\\to\\) 니즈 정의\n\n\n\n\n\n세분화 타겟팅 \\(\\to\\) 포지셔닝\n아이디어 도출\n\n\n\n\n\n가치제안서/실행전략 \\(\\to\\) 비즈니스 모델",
    "crumbs": [
      "Posts",
      "B2b",
      "01. 제안전략수립"
    ]
  },
  {
    "objectID": "posts/b2b/2024-03-17-01. 제안전략수립.html#제안컨설팅을-위한-사업화-프로세스",
    "href": "posts/b2b/2024-03-17-01. 제안전략수립.html#제안컨설팅을-위한-사업화-프로세스",
    "title": "01. 제안전략수립",
    "section": "",
    "text": "- 사업의 개념과 도메인 지식이 필요\n- 문제정의\n\n\n\nB2B 산업의 환경구조 \\(\\to\\) 환경분석 프레임\n\n\n\n- 고객사에 대한 충분한 이해가 필요\n\n고객정의(구매센터) \\(\\to\\) 니즈 정의\n\n\n\n\n\n세분화 타겟팅 \\(\\to\\) 포지셔닝\n아이디어 도출\n\n\n\n\n\n가치제안서/실행전략 \\(\\to\\) 비즈니스 모델",
    "crumbs": [
      "Posts",
      "B2b",
      "01. 제안전략수립"
    ]
  },
  {
    "objectID": "posts/b2b/2024-03-17-01. 제안전략수립.html#biz-체계의-이해",
    "href": "posts/b2b/2024-03-17-01. 제안전략수립.html#biz-체계의-이해",
    "title": "01. 제안전략수립",
    "section": "Biz 체계의 이해",
    "text": "Biz 체계의 이해\n- biz는 Business의 약자로 상품이나 서비스를 생산하고 판매하며, 수익을 얻는 활동을 의미함\n- biz 체계\n\n비즈니스의 전반적인 구조와 기능을 나타냄\n기업이 제품이나 서비스를 제공하고, 조직 구조, 프로세스, 기술, 인력 등을 포함한 다양한 요소를 통합하여 운영하는 방식을 의미한다\n\n- biz체게는 일반적으로 처음에는 고객중심으로 시작했다가 관리 중심으로 변환함\n\n관리 중심으로 시작한 기업은… 결과가 좋지않음 (관리가 고객의 이익을 침해하면 안됨…)\n\n기업의 존재 이유는 고객이고 기업의 목적은 시장을 창조하는 것이다 -피터 드리커-",
    "crumbs": [
      "Posts",
      "B2b",
      "01. 제안전략수립"
    ]
  },
  {
    "objectID": "posts/b2b/2024-03-17-01. 제안전략수립.html#b2b-사업의-본질",
    "href": "posts/b2b/2024-03-17-01. 제안전략수립.html#b2b-사업의-본질",
    "title": "01. 제안전략수립",
    "section": "B2B 사업의 본질",
    "text": "B2B 사업의 본질\n- B2B : Bussiness-to-Bussiness의 약어로, 기업 간에 이루어지는 거래를 나타냄\n\nB2B 비즈니스는 B2B 고객의 성공적인 전략실행을 위한 솔루션을 제시하는 것이다.",
    "crumbs": [
      "Posts",
      "B2b",
      "01. 제안전략수립"
    ]
  },
  {
    "objectID": "posts/b2b/2024-03-17-01. 제안전략수립.html#문제의-종류",
    "href": "posts/b2b/2024-03-17-01. 제안전략수립.html#문제의-종류",
    "title": "01. 제안전략수립",
    "section": "문제의 종류",
    "text": "문제의 종류\n\n(1) 발생형\n- 시간축 : 과거\n- 발생원인 : 기준이탈 \\(\\cdot\\) 미달\n- 성격 : 이미 일어나버린 문제\n\n\n(2) 탐색형\n- 시간축 : 현재\n- 발생원인 : 개선 \\(\\cdot\\) 개량 \\(\\cdot\\) 강화\n- 성격 : 더 잘해보고 싶은 문제\n\n\n(3) 설정형\n- 시간축 : 미래\n- 발생원인 : 개발 \\(\\cdot\\) 기획 \\(\\cdot\\) 리스크 회피\n- 성격 : 앞으로 어떻게 할 것인가의 문제\n\\(\\divideontimes\\) 우리는 일반적으로 설정형으로 문제를 정의하고 해결할 줄 알아야 한다.\n\n\n(4) 요약",
    "crumbs": [
      "Posts",
      "B2b",
      "01. 제안전략수립"
    ]
  },
  {
    "objectID": "posts/b2b/2024-03-17-01. 제안전략수립.html#example.-문제정의",
    "href": "posts/b2b/2024-03-17-01. 제안전략수립.html#example.-문제정의",
    "title": "01. 제안전략수립",
    "section": "example. 문제정의",
    "text": "example. 문제정의",
    "crumbs": [
      "Posts",
      "B2b",
      "01. 제안전략수립"
    ]
  },
  {
    "objectID": "posts/b2b/2024-03-17-01. 제안전략수립.html#forecasting",
    "href": "posts/b2b/2024-03-17-01. 제안전략수립.html#forecasting",
    "title": "01. 제안전략수립",
    "section": "Forecasting",
    "text": "Forecasting\n- 현재 시점에서 미래를 보는 사고법\n\n활용 가능 자료 수집 \\(\\to\\) 자료분석 \\(\\to\\) Ouput 도출\n\n- 자료 분석에 많은 시간을 들이고 정작 전략 수집에는 소홀하게 됨, 특히 기존 유사 프로젝트와 비슷한 결론이 나올 가능성이 높음",
    "crumbs": [
      "Posts",
      "B2b",
      "01. 제안전략수립"
    ]
  },
  {
    "objectID": "posts/b2b/2024-03-17-01. 제안전략수립.html#backcasting",
    "href": "posts/b2b/2024-03-17-01. 제안전략수립.html#backcasting",
    "title": "01. 제안전략수립",
    "section": "Backcasting",
    "text": "Backcasting\n- 미래 시점에서 현재를 보는 사고법\n\nOuput 추론/상상 \\(\\to\\) Output 실현을 위한 조건과 가정 정의 \\(\\to\\) 조건과 가정을 구현하기 위한 방법 정의\n\n- 역가치사슬분석, 보도자료기반 가치정의\n\nProcess\n1 Awarensss : 이상상황 정의\n\n주요 참여자 정의\n이상적 가치 정의 \\(\\to\\) 전체 최적화를 고려 (풍선효과 등을 막기 위한 방안)\n\n\\(\\divideontimes\\) 풍선효과 : 풍선의 한쪽을 누르면 다른 쪽이 불룩 튀어나오는 모습을 빗댄 표현으로, 어떤 현상이나 문제를 억제하면 다른 현상이나 문제가 새로이 불거져 나오는 상황\n2 interests : 이해관계자 정의\n\n이상적 가치 제공 및 운영 시 참여/고려 되는 이해관계자 도출\n이해관계자 니즈 정의\n\n3 Down to Action : 목표 도달을 위한 활동/조건 정의\n\n이상목표 달성을 위한 세부활동 정의 : 참여자/조건 별\n\n4 Baseline : 현 상황 정의\n5 Gap Analysis : Gap 및 장애요소 도출\n6 핵심 성공요소 정의 및 세부 실행계획 정의",
    "crumbs": [
      "Posts",
      "B2b",
      "01. 제안전략수립"
    ]
  },
  {
    "objectID": "posts/b2b/2024-03-17-01. 제안전략수립.html#sic",
    "href": "posts/b2b/2024-03-17-01. 제안전략수립.html#sic",
    "title": "01. 제안전략수립",
    "section": "SIC",
    "text": "SIC\n- success image canvas\n- 고려사항 : 사람, 기술, 재료, 방법 등\n- 주요요소\n\n경쟁사/대체재의 미래 제시가치\n필요 인프라 (H/W 및 S/W)\n이해관계자 : 보안/협력자, 관련기관 등\n필요자원",
    "crumbs": [
      "Posts",
      "B2b",
      "01. 제안전략수립"
    ]
  },
  {
    "objectID": "posts/CS/00. DA/00. 데이터 사이언스 시작하기/00. Intro.html",
    "href": "posts/CS/00. DA/00. 데이터 사이언스 시작하기/00. Intro.html",
    "title": "00. Intro",
    "section": "",
    "text": "1 데이터 사이언스를 배워야 하는 이유\n\n분야를 막론하고 데이터는 너무나 중요하게 여겨지고 있음(음악 추천, 대선 예측 등등….)\n\n2 정의 : 한 가지로 정의되어 있지 않음\n\n위키피디아 : 데이터 마이닝과 유사하게 정형, 비정형 형태를 포함한 다양한 데이터로부터 지식과 인사이트를 추출하는데 과학적 방법론, 프로세스 알고리즘, 시스템을 동원하는 융합 분야다.\njournal of Data Science : 데이터와 연관된 모든 것을 의미.\nDrew Conway : 프로그래밍, 수학과 통계, 특정분야에 대한 전문성을 가지고 데이터로부터 현실 문제를 해결하는 것\n결론 : 가치를 더할 수 있는일을 데이터를 활용하여 해결하는 것!",
    "crumbs": [
      "Posts",
      "CS",
      "00. DA",
      "00. 데이터 사이언스 시작하기",
      "00. Intro"
    ]
  },
  {
    "objectID": "posts/CS/00. DA/00. 데이터 사이언스 시작하기/00. Intro.html#오해-1",
    "href": "posts/CS/00. DA/00. 데이터 사이언스 시작하기/00. Intro.html#오해-1",
    "title": "00. Intro",
    "section": "오해 1",
    "text": "오해 1\n인공지능과 딥러닝만이 DS가 아니다!\n- DS 단계\n\n데이터 수집\n데이터를 옮기고 저장\n데이터를 정리\n데이터 분석, A/B test\n인공지능, 딥러닝\n\n\n1 ~ 3은 데이터 엔지니어의 역할이기도 하다.\n또한, 실제 기업에서는 1 ~ 4 까지만 가도 유의미한 가치를 창출할 수 있다.",
    "crumbs": [
      "Posts",
      "CS",
      "00. DA",
      "00. 데이터 사이언스 시작하기",
      "00. Intro"
    ]
  },
  {
    "objectID": "posts/CS/00. DA/00. 데이터 사이언스 시작하기/00. Intro.html#오해-2",
    "href": "posts/CS/00. DA/00. 데이터 사이언스 시작하기/00. Intro.html#오해-2",
    "title": "00. Intro",
    "section": "오해 2",
    "text": "오해 2\n다시 말하지만, 데이터 사이언스는 단순히 수학, 통계, 컴퓨팅 능력이 아닌 해당 비즈니스 영역에서 협업을 통해 문제를 해결할 줄 알아야한다.",
    "crumbs": [
      "Posts",
      "CS",
      "00. DA",
      "00. 데이터 사이언스 시작하기",
      "00. Intro"
    ]
  },
  {
    "objectID": "posts/CS/00. DA/00. 데이터 사이언스 시작하기/00. Intro.html#r",
    "href": "posts/CS/00. DA/00. 데이터 사이언스 시작하기/00. Intro.html#r",
    "title": "00. Intro",
    "section": "R",
    "text": "R\n1 통계와 시각화에 특화된 언어\n2 그러나, R을 배운다고 다른 프로그래밍을 잘하는 것은 아님…",
    "crumbs": [
      "Posts",
      "CS",
      "00. DA",
      "00. 데이터 사이언스 시작하기",
      "00. Intro"
    ]
  },
  {
    "objectID": "posts/CS/00. DA/00. 데이터 사이언스 시작하기/00. Intro.html#python",
    "href": "posts/CS/00. DA/00. 데이터 사이언스 시작하기/00. Intro.html#python",
    "title": "00. Intro",
    "section": "Python",
    "text": "Python\n1 웹사이트 개발, 이미지 처리, 업무 자동화, 데이터 시각화, 게임 개발, 앱 서버 개발 등 다른 영역에 다채롭게 활용할 수 있음\n2 DS 토픽에 한정해서는 R이 인기가 많았으나, numpy, pandas, tensorflow의 등장으로 python이 R을 앞지름",
    "crumbs": [
      "Posts",
      "CS",
      "00. DA",
      "00. 데이터 사이언스 시작하기",
      "00. Intro"
    ]
  },
  {
    "objectID": "posts/CS/00. DA/00. 데이터 사이언스 시작하기/00. Intro.html#내-생각",
    "href": "posts/CS/00. DA/00. 데이터 사이언스 시작하기/00. Intro.html#내-생각",
    "title": "00. Intro",
    "section": "내 생각",
    "text": "내 생각\n1 흠…. 이건 좀 내 의견은 반대이다.. 내가 둘 다 배워봐서 그런걸 수도 있지만, R이 tidyverse, tidymodel에 등장으로 훨씬 처음에 배우기 좋은 것 같음\n2 그리고 도메인, 개별 성향에 따라 다르기 때문에 무작정 어떤 언어가 더 쉽다고 말할 순 없는 것 같다..\n3 그냥, python으로 할 수 있는 범위가 많기 때문에 사람들이 배우기 더 쉬워하는 것 같다.",
    "crumbs": [
      "Posts",
      "CS",
      "00. DA",
      "00. 데이터 사이언스 시작하기",
      "00. Intro"
    ]
  },
  {
    "objectID": "posts/CS/00. DA/00. 데이터 사이언스 시작하기/00. Intro.html#문제-정의하기",
    "href": "posts/CS/00. DA/00. 데이터 사이언스 시작하기/00. Intro.html#문제-정의하기",
    "title": "00. Intro",
    "section": "1. 문제 정의하기",
    "text": "1. 문제 정의하기\n1 목표 설정\n2 기간 설정\n3 평가 방법 설정\n4 필요한 데이터 설정",
    "crumbs": [
      "Posts",
      "CS",
      "00. DA",
      "00. 데이터 사이언스 시작하기",
      "00. Intro"
    ]
  },
  {
    "objectID": "posts/CS/00. DA/00. 데이터 사이언스 시작하기/00. Intro.html#데이터-모으기",
    "href": "posts/CS/00. DA/00. 데이터 사이언스 시작하기/00. Intro.html#데이터-모으기",
    "title": "00. Intro",
    "section": "2. 데이터 모으기",
    "text": "2. 데이터 모으기\n1 웹 크롤링, 자료 모으기, 파일 읽고 쓰기",
    "crumbs": [
      "Posts",
      "CS",
      "00. DA",
      "00. 데이터 사이언스 시작하기",
      "00. Intro"
    ]
  },
  {
    "objectID": "posts/CS/00. DA/00. 데이터 사이언스 시작하기/00. Intro.html#데이터-다듬기",
    "href": "posts/CS/00. DA/00. 데이터 사이언스 시작하기/00. Intro.html#데이터-다듬기",
    "title": "00. Intro",
    "section": "3. 데이터 다듬기",
    "text": "3. 데이터 다듬기\n1 데이터 관찰, 오류 제거, 정리",
    "crumbs": [
      "Posts",
      "CS",
      "00. DA",
      "00. 데이터 사이언스 시작하기",
      "00. Intro"
    ]
  },
  {
    "objectID": "posts/CS/00. DA/00. 데이터 사이언스 시작하기/00. Intro.html#데이터-분석",
    "href": "posts/CS/00. DA/00. 데이터 사이언스 시작하기/00. Intro.html#데이터-분석",
    "title": "00. Intro",
    "section": "4. 데이터 분석",
    "text": "4. 데이터 분석\n1 데이터 파악, 변형, 통계 분석, 인사이트 도출",
    "crumbs": [
      "Posts",
      "CS",
      "00. DA",
      "00. 데이터 사이언스 시작하기",
      "00. Intro"
    ]
  },
  {
    "objectID": "posts/CS/00. DA/00. 데이터 사이언스 시작하기/00. Intro.html#커뮤니케이션",
    "href": "posts/CS/00. DA/00. 데이터 사이언스 시작하기/00. Intro.html#커뮤니케이션",
    "title": "00. Intro",
    "section": "5. 커뮤니케이션",
    "text": "5. 커뮤니케이션\n1 다양한 시각화, 커뮤니케이션, 리포트를 활용해 구성원들과 소통하며 문제 해결",
    "crumbs": [
      "Posts",
      "CS",
      "00. DA",
      "00. 데이터 사이언스 시작하기",
      "00. Intro"
    ]
  },
  {
    "objectID": "posts/CS/00. DA/00. 데이터 사이언스 시작하기/02. pandas.html",
    "href": "posts/CS/00. DA/00. 데이터 사이언스 시작하기/02. pandas.html",
    "title": "02. pandas",
    "section": "",
    "text": "- numpy를 이용해서 만들어진 모듈\n- 데이터를 읽고, 쓰고, 저장하고 시각화하는 기능이 포함되어 있음\n- 표 형식의 데이터(데이터프레임)를 다루는데 필수적인 모듈!\n- numpy와 달리 다양한 자료형을 표 형식으로 저장할 수 있음!(\\(\\star\\star\\))",
    "crumbs": [
      "Posts",
      "CS",
      "00. DA",
      "00. 데이터 사이언스 시작하기",
      "02. pandas"
    ]
  },
  {
    "objectID": "posts/CS/00. DA/00. 데이터 사이언스 시작하기/02. pandas.html#리스트",
    "href": "posts/CS/00. DA/00. 데이터 사이언스 시작하기/02. pandas.html#리스트",
    "title": "02. pandas",
    "section": "1. 리스트",
    "text": "1. 리스트\n\nimport pandas as pd\nimport numpy as np\n\n\nl = [[\"a\", 50, 86], [\"b\", 89, 31], [\"ikjoong\", 68, 91]]\n\npd.DataFrame(l)\n\n\n\n\n\n\n\n\n\n0\n1\n2\n\n\n\n\n0\na\n50\n86\n\n\n1\nb\n89\n31\n\n\n2\nikjoong\n68\n91",
    "crumbs": [
      "Posts",
      "CS",
      "00. DA",
      "00. 데이터 사이언스 시작하기",
      "02. pandas"
    ]
  },
  {
    "objectID": "posts/CS/00. DA/00. 데이터 사이언스 시작하기/02. pandas.html#numpy",
    "href": "posts/CS/00. DA/00. 데이터 사이언스 시작하기/02. pandas.html#numpy",
    "title": "02. pandas",
    "section": "2. numpy",
    "text": "2. numpy\n\npd.DataFrame(np.array(l))\n\n\n\n\n\n\n\n\n\n0\n1\n2\n\n\n\n\n0\na\n50\n86\n\n\n1\nb\n89\n31\n\n\n2\nikjoong\n68\n91",
    "crumbs": [
      "Posts",
      "CS",
      "00. DA",
      "00. 데이터 사이언스 시작하기",
      "02. pandas"
    ]
  },
  {
    "objectID": "posts/CS/00. DA/00. 데이터 사이언스 시작하기/02. pandas.html#series",
    "href": "posts/CS/00. DA/00. 데이터 사이언스 시작하기/02. pandas.html#series",
    "title": "02. pandas",
    "section": "3. Series",
    "text": "3. Series\n\nl2 = [\n    pd.Series([\"a\", 50, 86]),\n    pd.Series([\"b\", 89, 31]),\n    pd.Series([\"c\", 68, 91]),\n        \n]\n\npd.DataFrame(l2)\n\n\n\n\n\n\n\n\n\n0\n1\n2\n\n\n\n\n0\na\n50\n86\n\n\n1\nb\n89\n31\n\n\n2\nc\n68\n91",
    "crumbs": [
      "Posts",
      "CS",
      "00. DA",
      "00. 데이터 사이언스 시작하기",
      "02. pandas"
    ]
  },
  {
    "objectID": "posts/CS/00. DA/00. 데이터 사이언스 시작하기/02. pandas.html#dictionary",
    "href": "posts/CS/00. DA/00. 데이터 사이언스 시작하기/02. pandas.html#dictionary",
    "title": "02. pandas",
    "section": "4. Dictionary",
    "text": "4. Dictionary\n\ndic = {\"name\" : [\"a\",\"b\",\"c\"],\n       \"s1\" : [50,89,68],\n       \"s2\" : [86,31,91]}\n\ndf = pd.DataFrame(dic)\ndf\n\n\n\n\n\n\n\n\n\nname\ns1\ns2\n\n\n\n\n0\na\n50\n86\n\n\n1\nb\n89\n31\n\n\n2\nc\n68\n91\n\n\n\n\n\n\n\n\n\ndf.set_index(\"name\")\n\n\n\n\n\n\n\n\n\ns1\ns2\n\n\nname\n\n\n\n\n\n\na\n50\n86\n\n\nb\n89\n31\n\n\nc\n68\n91",
    "crumbs": [
      "Posts",
      "CS",
      "00. DA",
      "00. 데이터 사이언스 시작하기",
      "02. pandas"
    ]
  },
  {
    "objectID": "posts/CS/00. DA/01. DataFrame 다루기/01. DataFrame 인덱싱.html",
    "href": "posts/CS/00. DA/01. DataFrame 다루기/01. DataFrame 인덱싱.html",
    "title": "01. DataFrame 인덱싱",
    "section": "",
    "text": "import\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n\n데이터 로드\n\ndf = pd.read_csv(\"data/broadcast.csv\", encoding = \"utf-8\", index_col = 0)\n\n\ndf\n\n\n\n\n\n\n\n\n\nKBS\nMBC\nSBS\nTV CHOSUN\nJTBC\nChannel A\nMBN\n\n\n\n\n2011\n35.951\n18.374\n11.173\n9.102\n7.380\n3.771\n2.809\n\n\n2012\n36.163\n16.022\n11.408\n8.785\n7.878\n5.874\n3.310\n\n\n2013\n31.989\n16.778\n9.673\n9.026\n7.810\n5.350\n3.825\n\n\n2014\n31.210\n15.663\n9.108\n9.440\n7.490\n5.776\n4.572\n\n\n2015\n27.777\n16.573\n9.099\n9.940\n7.267\n6.678\n5.520\n\n\n2016\n27.583\n14.982\n8.669\n9.829\n7.727\n6.624\n5.477\n\n\n2017\n26.890\n12.465\n8.661\n8.886\n9.453\n6.056\n5.215\n\n\n\n\n\n\n\n\n\n\n인덱싱\n- ex1. 2016년 kbs 시청률 받아오기\n\ndf.loc[2016, \"KBS\"]\n\n27.583\n\n\n- ex2. JTBC의 시청률 확인\n\ndf[\"JTBC\"]\n\n2011    7.380\n2012    7.878\n2013    7.810\n2014    7.490\n2015    7.267\n2016    7.727\n2017    9.453\nName: JTBC, dtype: float64\n\n\n- ex3. SBS와 JTBC의 시청률만 확인\n\ndf[[\"SBS\",\"JTBC\"]]\n\n\n\n\n\n\n\n\n\nSBS\nJTBC\n\n\n\n\n2011\n11.173\n7.380\n\n\n2012\n11.408\n7.878\n\n\n2013\n9.673\n7.810\n\n\n2014\n9.108\n7.490\n\n\n2015\n9.099\n7.267\n\n\n2016\n8.669\n7.727\n\n\n2017\n8.661\n9.453\n\n\n\n\n\n\n\n\n- ex4. 삼송카드, 현디카드 요일별 문화생활비 분석\n\nsamsong_df = pd.read_csv('data/samsong.csv')\nhyundee_df = pd.read_csv('data/hyundee.csv')\n\n\nsamsong_df\n\n\n\n\n\n\n\n\n\n요일\n식비\n교통비\n문화생활비\n기타\n\n\n\n\n0\nMON\n19420\n2560\n4308\n3541\n\n\n1\nTUE\n16970\n2499\n7644\n2903\n\n\n2\nWED\n15091\n2511\n5674\n2015\n\n\n3\nTHU\n17880\n2545\n8621\n3012\n\n\n4\nFRI\n27104\n2993\n23052\n2508\n\n\n5\nSAT\n29055\n2803\n15330\n4901\n\n\n6\nSUN\n23509\n1760\n19030\n4230\n\n\n\n\n\n\n\n\n\nhyundee_df\n\n\n\n\n\n\n\n\n\n요일\n식비\n교통비\n문화생활비\n기타\n\n\n\n\n0\nMON\n22420\n2574\n5339\n5546\n\n\n1\nTUE\n19940\n2689\n3524\n2501\n\n\n2\nWED\n18086\n2281\n5364\n2234\n\n\n3\nTHU\n18863\n2155\n9942\n3252\n\n\n4\nFRI\n35144\n2463\n33511\n2342\n\n\n5\nSAT\n34952\n2812\n19397\n4324\n\n\n6\nSUN\n28513\n2680\n19925\n4577\n\n\n\n\n\n\n\n\n\nday = samsong_df[\"요일\"]\nsamsong = samsong_df[\"문화생활비\"]\nhyundee = hyundee_df[\"문화생활비\"]\n\ndf = pd.DataFrame([day,samsong,hyundee]).T\n\ndf.columns = [\"day\", \"samsong\", \"hyundee\"]\ndf\n\n\n\n\n\n\n\n\n\nday\nsamsong\nhyundee\n\n\n\n\n0\nMON\n4308\n5339\n\n\n1\nTUE\n7644\n3524\n\n\n2\nWED\n5674\n5364\n\n\n3\nTHU\n8621\n9942\n\n\n4\nFRI\n23052\n33511\n\n\n5\nSAT\n15330\n19397\n\n\n6\nSUN\n19030\n19925\n\n\n\n\n\n\n\n\n- ex5. KBS ~ SBS, 2012 ~ 2017 까지의 시청률 데이터만 확인\n\ndf = pd.read_csv('data/broadcast.csv', index_col=0)\n\n\ndf.loc[2012:2017, \"KBS\" : \"SBS\"]\n\n\n\n\n\n\n\n\n\nKBS\nMBC\nSBS\n\n\n\n\n2012\n36.163\n16.022\n11.408\n\n\n2013\n31.989\n16.778\n9.673\n\n\n2014\n31.210\n15.663\n9.108\n\n\n2015\n27.777\n16.573\n9.099\n\n\n2016\n27.583\n14.982\n8.669\n\n\n2017\n26.890\n12.465\n8.661\n\n\n\n\n\n\n\n\n- ex6. KBS 시청률이 30이 넘은 데이터만 확인해보기\n\ndf.loc[df.KBS &gt; 30, \"KBS\"]\n\n2011    35.951\n2012    36.163\n2013    31.989\n2014    31.210\nName: KBS, dtype: float64\n\n\n- ex7. SBS가 TV CHOSUN보다 더 시청률이 낮았던 시기의 데이터 확인\n\ndf.loc[df.SBS &lt; df[\"TV CHOSUN\"], [\"SBS\",\"TV CHOSUN\"]]\n\n\n\n\n\n\n\n\n\nSBS\nTV CHOSUN\n\n\n\n\n2014\n9.108\n9.440\n\n\n2015\n9.099\n9.940\n\n\n2016\n8.669\n9.829\n\n\n2017\n8.661\n8.886",
    "crumbs": [
      "Posts",
      "CS",
      "00. DA",
      "01. DataFrame 다루기",
      "01. DataFrame 인덱싱"
    ]
  },
  {
    "objectID": "posts/CS/00. DA/01. DataFrame 다루기/03. 큰 데이터 다루기.html",
    "href": "posts/CS/00. DA/01. DataFrame 다루기/03. 큰 데이터 다루기.html",
    "title": "03. 큰 데이터 다루기",
    "section": "",
    "text": "import pandas as pd",
    "crumbs": [
      "Posts",
      "CS",
      "00. DA",
      "01. DataFrame 다루기",
      "03. 큰 데이터 다루기"
    ]
  },
  {
    "objectID": "posts/CS/00. DA/01. DataFrame 다루기/03. 큰 데이터 다루기.html#주어진-데이터에-총-몇개의-도시와-몇-개의-나라가-있는지-출력",
    "href": "posts/CS/00. DA/01. DataFrame 다루기/03. 큰 데이터 다루기.html#주어진-데이터에-총-몇개의-도시와-몇-개의-나라가-있는지-출력",
    "title": "03. 큰 데이터 다루기",
    "section": "(1) 주어진 데이터에 총 몇개의 도시와 몇 개의 나라가 있는지 출력",
    "text": "(1) 주어진 데이터에 총 몇개의 도시와 몇 개의 나라가 있는지 출력\n\nsol1\n\ndf = pd.read_csv('data/world_cities.csv')\n\nprint(f'{len(df[\"City / Urban area\"].unique())}/{len(df[\"Country\"].unique())}') \n\n249/61\n\n\n\n\nsol2\n\ndf['City / Urban area'].value_counts().shape\n\n(249,)\n\n\n\ndf['Country'].value_counts().shape\n\n(61,)",
    "crumbs": [
      "Posts",
      "CS",
      "00. DA",
      "01. DataFrame 다루기",
      "03. 큰 데이터 다루기"
    ]
  },
  {
    "objectID": "posts/CS/00. DA/01. DataFrame 다루기/03. 큰 데이터 다루기.html#인구-밀도명sqkm-가-10000-이-넘는-도시는-총-몇-개인지-확인",
    "href": "posts/CS/00. DA/01. DataFrame 다루기/03. 큰 데이터 다루기.html#인구-밀도명sqkm-가-10000-이-넘는-도시는-총-몇-개인지-확인",
    "title": "03. 큰 데이터 다루기",
    "section": "(2) 인구 밀도(명/sqKm) 가 10000 이 넘는 도시는 총 몇 개인지 확인",
    "text": "(2) 인구 밀도(명/sqKm) 가 10000 이 넘는 도시는 총 몇 개인지 확인\n\nsol1\n\nsum((df[\"Population\"]/df[\"Land area (in sqKm)\"])&gt;10000)\n\n19",
    "crumbs": [
      "Posts",
      "CS",
      "00. DA",
      "01. DataFrame 다루기",
      "03. 큰 데이터 다루기"
    ]
  },
  {
    "objectID": "posts/CS/00. DA/01. DataFrame 다루기/03. 큰 데이터 다루기.html#인구-밀도가-가장-높은-도시",
    "href": "posts/CS/00. DA/01. DataFrame 다루기/03. 큰 데이터 다루기.html#인구-밀도가-가장-높은-도시",
    "title": "03. 큰 데이터 다루기",
    "section": "(3) 인구 밀도가 가장 높은 도시",
    "text": "(3) 인구 밀도가 가장 높은 도시\n\nsol1\n\nmax_value = max(df[\"Population\"]/df[\"Land area (in sqKm)\"])\n\n\ndf.loc[df[\"Population\"]/df[\"Land area (in sqKm)\"] == max_value, \"City / Urban area\"]\n\n75    Mumbai\nName: City / Urban area, dtype: object",
    "crumbs": [
      "Posts",
      "CS",
      "00. DA",
      "01. DataFrame 다루기",
      "03. 큰 데이터 다루기"
    ]
  },
  {
    "objectID": "posts/CS/00. DA/01. DataFrame 다루기/03. 큰 데이터 다루기.html#도시가-4개인-나라-출력",
    "href": "posts/CS/00. DA/01. DataFrame 다루기/03. 큰 데이터 다루기.html#도시가-4개인-나라-출력",
    "title": "03. 큰 데이터 다루기",
    "section": "(4) 도시가 4개인 나라 출력",
    "text": "(4) 도시가 4개인 나라 출력\n\nsol\n\ndf[\"Country\"].value_counts()[df[\"Country\"].value_counts() == 4]\n\nCountry\nItaly    4\nName: count, dtype: int64",
    "crumbs": [
      "Posts",
      "CS",
      "00. DA",
      "01. DataFrame 다루기",
      "03. 큰 데이터 다루기"
    ]
  },
  {
    "objectID": "posts/CS/00. DA/01. DataFrame 다루기/03. 큰 데이터 다루기.html#sol2-2",
    "href": "posts/CS/00. DA/01. DataFrame 다루기/03. 큰 데이터 다루기.html#sol2-2",
    "title": "03. 큰 데이터 다루기",
    "section": "sol2",
    "text": "sol2\n\nimport pandas as pd\n\ndf = pd.read_csv('data/enrolment_2.csv')\n\n# 과목별 인원 가져오기\nallowed = df[\"status\"] == \"allowed\"\ncourse_counts = df.loc[allowed, \"course name\"].value_counts()\n\n# 각 강의실 규모에 해당되는 과목 리스트 만들기\nauditorium_list = list(course_counts[course_counts &gt;= 80].index)\nlarge_room_list = list(course_counts[(80 &gt; course_counts) & (course_counts &gt;= 40)].index)\nmedium_room_list = list(course_counts[(40 &gt; course_counts) & (course_counts &gt;= 15)].index)\nsmall_room_list = list(course_counts[(15 &gt; course_counts) & (course_counts &gt; 4)].index)\n\n# not allowed 과목에 대해 값 지정해주기\nnot_allowed = df[\"status\"] == \"not allowed\"\ndf.loc[not_allowed, \"room assignment\"] = \"not assigned\"\n\n# allowed 과목에 대해 값 지정해주기\nfor course in auditorium_list:\n    df.loc[(df[\"course name\"] == course) & allowed, \"room assignment\"] = \"Auditorium\"\n\nfor course in large_room_list:\n    df.loc[(df[\"course name\"] == course) & allowed, \"room assignment\"] = \"Large room\"\n    \nfor course in medium_room_list:\n    df.loc[(df[\"course name\"] == course) & allowed, \"room assignment\"] = \"Medium room\"\n    \nfor course in small_room_list:\n    df.loc[(df[\"course name\"] == course) & allowed, \"room assignment\"] = \"Small room\"\n    \n# 정답 출력\ndf\n\n\n\n\n\n\n\n\n\nid\nyear\ncourse name\nstatus\nroom assignment\n\n\n\n\n0\n2777729\n1\ninformation technology\nnot allowed\nnot assigned\n\n\n1\n2777730\n2\nscience\nallowed\nAuditorium\n\n\n2\n2777765\n1\narts\nallowed\nAuditorium\n\n\n3\n2777766\n2\narts\nallowed\nAuditorium\n\n\n4\n2777785\n1\nmba\nallowed\nSmall room\n\n\n...\n...\n...\n...\n...\n...\n\n\n1995\n2796805\n3\ncomputer application\nallowed\nMedium room\n\n\n1996\n2796812\n1\nnursing\nallowed\nMedium room\n\n\n1997\n2796813\n2\nnursing\nallowed\nMedium room\n\n\n1998\n2796814\n3\nnursing\nallowed\nMedium room\n\n\n1999\n2796815\n4\nnursing\nallowed\nMedium room\n\n\n\n\n2000 rows × 5 columns\n\n\n\n\n- 큼.. 이건 내 코드가 더 효율적인 것 같음(가독성도!)",
    "crumbs": [
      "Posts",
      "CS",
      "00. DA",
      "01. DataFrame 다루기",
      "03. 큰 데이터 다루기"
    ]
  },
  {
    "objectID": "posts/CS/00. DA/01. DataFrame 다루기/03. 큰 데이터 다루기.html#sol1-5",
    "href": "posts/CS/00. DA/01. DataFrame 다루기/03. 큰 데이터 다루기.html#sol1-5",
    "title": "03. 큰 데이터 다루기",
    "section": "sol1",
    "text": "sol1\n\nimport pandas as pd\nimport warnings\n\nwarnings.filterwarnings(action = \"ignore\")\ndf = pd.read_csv('data/enrolment_3.csv')\n\n\nallowed = df.status == \"allowed\"\n\n- 고유값만 출력\n\nu_df = df.loc[allowed,  [\"course name\", \"room assignment\"]].\\\n                drop_duplicates().sort_values([\"room assignment\",\"course name\"])\n\n\nu_df.head()\n\n\n\n\n\n\n\n\n\ncourse name\nroom assignment\n\n\n\n\n2\narts\nAuditorium\n\n\n60\ncommerce\nAuditorium\n\n\n1\nscience\nAuditorium\n\n\n202\neducation\nLarge room\n\n\n24\nenglish\nLarge room\n\n\n\n\n\n\n\n\n- room_index\n\nroom_index = u_df[\"room assignment\"].value_counts().index\nroom_values = u_df[\"room assignment\"].value_counts().values\n\nroom_index, room_values\n\n(Index(['Small room', 'Medium room', 'Auditorium', 'Large room'], dtype='object', name='room assignment'),\n array([53, 24,  3,  2], dtype=int64))\n\n\n- 룸 넘버링\n\nnew_df = pd.DataFrame()\nfor k in range(4) :\n    temp = u_df.loc[u_df[\"room assignment\"] == room_index[k],:]\n    temp[\"room number\"] = [i + \"-\" + str(j) for i,j in zip(temp[\"room assignment\"], range(1, room_values[k]+1))]\n    new_df = pd.concat([new_df,temp], axis = 0)\n\n\nnew_df = new_df[[\"course name\",\"room number\"]]\n\n\ndf1 = pd.merge(df, new_df, on = \"course name\",how = \"left\")\n\n\ndf1.loc[df1[\"room assignment\"] == \"not assigned\", \"room number\"] = \"not assigned\"\n\n\ndf1[\"room number\"] = [i.replace(\" room\", \"\") for i in df1[\"room number\"]]\ndf = df1.drop(\"room assignment\", axis = 1)\ndf\n\n\n\n\n\n\n\n\n\nid\nyear\ncourse name\nstatus\nroom number\n\n\n\n\n0\n2777729\n1\ninformation technology\nnot allowed\nnot assigned\n\n\n1\n2777730\n2\nscience\nallowed\nAuditorium-3\n\n\n2\n2777765\n1\narts\nallowed\nAuditorium-1\n\n\n3\n2777766\n2\narts\nallowed\nAuditorium-1\n\n\n4\n2777785\n1\nmba\nallowed\nSmall-34\n\n\n...\n...\n...\n...\n...\n...\n\n\n1995\n2796805\n3\ncomputer application\nallowed\nMedium-7\n\n\n1996\n2796812\n1\nnursing\nallowed\nMedium-22\n\n\n1997\n2796813\n2\nnursing\nallowed\nMedium-22\n\n\n1998\n2796814\n3\nnursing\nallowed\nMedium-22\n\n\n1999\n2796815\n4\nnursing\nallowed\nMedium-22\n\n\n\n\n2000 rows × 5 columns\n\n\n\n\n\n전체코드\n\nimport pandas as pd\n\ndf = pd.read_csv('data/enrolment_3.csv')\n\n# 여기에 코드를 작성하세요\nallowed = df.status == \"allowed\"\nu_df = df.loc[allowed,  [\"course name\", \"room assignment\"]].\\\n                drop_duplicates().sort_values([\"room assignment\",\"course name\"])\nroom_index = u_df[\"room assignment\"].value_counts().index\nroom_values = u_df[\"room assignment\"].value_counts().values\n\nnew_df = pd.DataFrame()\nfor k in range(4) :\n    temp = u_df.loc[u_df[\"room assignment\"] == room_index[k],:]\n    temp[\"room number\"] = [i + \"-\" + str(j) for i,j in zip(temp[\"room assignment\"], range(1, room_values[k]+1))]\n    new_df = pd.concat([new_df,temp], axis = 0)\nnew_df = new_df[[\"course name\",\"room number\"]]\ndf1 = pd.merge(df, new_df, on = \"course name\",how = \"left\")\ndf1.loc[df1[\"room assignment\"] == \"not assigned\", \"room number\"] = \"not assigned\"\ndf1[\"room number\"] = [i.replace(\" room\", \"\") for i in df1[\"room number\"]]\ndf = df1.drop(\"room assignment\", axis = 1)\ndf\n\n\n\n\n\n\n\n\n\nid\nyear\ncourse name\nstatus\nroom number\n\n\n\n\n0\n2777729\n1\ninformation technology\nnot allowed\nnot assigned\n\n\n1\n2777730\n2\nscience\nallowed\nAuditorium-3\n\n\n2\n2777765\n1\narts\nallowed\nAuditorium-1\n\n\n3\n2777766\n2\narts\nallowed\nAuditorium-1\n\n\n4\n2777785\n1\nmba\nallowed\nSmall-34\n\n\n...\n...\n...\n...\n...\n...\n\n\n1995\n2796805\n3\ncomputer application\nallowed\nMedium-7\n\n\n1996\n2796812\n1\nnursing\nallowed\nMedium-22\n\n\n1997\n2796813\n2\nnursing\nallowed\nMedium-22\n\n\n1998\n2796814\n3\nnursing\nallowed\nMedium-22\n\n\n1999\n2796815\n4\nnursing\nallowed\nMedium-22\n\n\n\n\n2000 rows × 5 columns",
    "crumbs": [
      "Posts",
      "CS",
      "00. DA",
      "01. DataFrame 다루기",
      "03. 큰 데이터 다루기"
    ]
  },
  {
    "objectID": "posts/CS/00. DA/01. DataFrame 다루기/03. 큰 데이터 다루기.html#sol2-3",
    "href": "posts/CS/00. DA/01. DataFrame 다루기/03. 큰 데이터 다루기.html#sol2-3",
    "title": "03. 큰 데이터 다루기",
    "section": "sol2",
    "text": "sol2\n\nimport pandas as pd\n\ndf = pd.read_csv('data/enrolment_3.csv')\n\n# 과목별 인원 가져오기\nallowed = df[\"status\"] == \"allowed\"\ncourse_counts = df.loc[allowed, \"course name\"].value_counts()\n\n# 각 강의실 규모에 해당되는 과목 리스트 만들기\nauditorium_list = list(course_counts[course_counts &gt;= 80].index)\nlarge_room_list = list(course_counts[(80 &gt; course_counts) & (course_counts &gt;= 40)].index)\nmedium_room_list = list(course_counts[(40 &gt; course_counts) & (course_counts &gt;= 15)].index)\nsmall_room_list = list(course_counts[(15 &gt; course_counts) & (course_counts &gt; 4)].index)\n\n# 강의실 이름 붙이기\nfor i in range(len(auditorium_list)):\n    df.loc[(df[\"course name\"] == sorted(auditorium_list)[i]) & allowed, \"room assignment\"] = \"Auditorium-\" + str(i + 1)\n\nfor i in range(len(large_room_list)):\n    df.loc[(df[\"course name\"] == sorted(large_room_list)[i]) & allowed, \"room assignment\"] = \"Large-\" + str(i + 1)\n    \nfor i in range(len(medium_room_list)):\n    df.loc[(df[\"course name\"] == sorted(medium_room_list)[i]) & allowed, \"room assignment\"] = \"Medium-\" + str(i + 1)\n    \nfor i in range(len(small_room_list)):\n    df.loc[(df[\"course name\"] == sorted(small_room_list)[i]) & allowed, \"room assignment\"] = \"Small-\" + str(i + 1)\n\n# column 이름 바꾸기\ndf.rename(columns={\"room assignment\": \"room number\"}, inplace = True)\n    \n# 테스트 코드\ndf\n\n\n\n\n\n\n\n\n\nid\nyear\ncourse name\nstatus\nroom number\n\n\n\n\n0\n2777729\n1\ninformation technology\nnot allowed\nnot assigned\n\n\n1\n2777730\n2\nscience\nallowed\nAuditorium-3\n\n\n2\n2777765\n1\narts\nallowed\nAuditorium-1\n\n\n3\n2777766\n2\narts\nallowed\nAuditorium-1\n\n\n4\n2777785\n1\nmba\nallowed\nSmall-34\n\n\n...\n...\n...\n...\n...\n...\n\n\n1995\n2796805\n3\ncomputer application\nallowed\nMedium-7\n\n\n1996\n2796812\n1\nnursing\nallowed\nMedium-22\n\n\n1997\n2796813\n2\nnursing\nallowed\nMedium-22\n\n\n1998\n2796814\n3\nnursing\nallowed\nMedium-22\n\n\n1999\n2796815\n4\nnursing\nallowed\nMedium-22\n\n\n\n\n2000 rows × 5 columns",
    "crumbs": [
      "Posts",
      "CS",
      "00. DA",
      "01. DataFrame 다루기",
      "03. 큰 데이터 다루기"
    ]
  },
  {
    "objectID": "posts/CS/00. DA/02. 데이터 분석과 시각화/01. Searborn 시각화.html",
    "href": "posts/CS/00. DA/02. 데이터 분석과 시각화/01. Searborn 시각화.html",
    "title": "01. Seaborn 시각화",
    "section": "",
    "text": "- 기존 pandas에서 제공하는 그래프보다 다양하고 근사한 그래프를 그릴 수 있다\n- Statistical Data Visualizaion 이라고 소개하고 있음\n- 용어 정리\n\nPDF(확률밀도함수) : 데이터셋의 분포를 나타낸다.\n\n특정 구간에 대한 확률값을 구하고 싶으면 해당 구간의 면적을 구하면 된다.\n그래프 아래의 모든 면적을 더하면 1이 된다.\n연속형 변수, 예를 들어 어떤 학생의 키가 173.5일 확률은 0이다. 왜냐하면 면적아래의 넓이를 특정값으로 구할 수 없기 때문이다.\n주사위, 동전을 던지는 예제에서는 특정 값에 대한 학률을 구할 수 있음!",
    "crumbs": [
      "Posts",
      "CS",
      "00. DA",
      "02. 데이터 분석과 시각화",
      "01. Seaborn 시각화"
    ]
  },
  {
    "objectID": "posts/CS/00. DA/02. 데이터 분석과 시각화/01. Searborn 시각화.html#kde-활용-예시",
    "href": "posts/CS/00. DA/02. 데이터 분석과 시각화/01. Searborn 시각화.html#kde-활용-예시",
    "title": "01. Seaborn 시각화",
    "section": "kde 활용 예시",
    "text": "kde 활용 예시\n- 기존의 그래프 그리기\n\ndf.plot(kind = \"hist\", y = \"Height\", bins = 15)\n\n\n\n\n\n\n\n\n- seaborn을 사용하면 히스토그램위에 kde를 얹을 수 있다.\n\nsns.displot(df[\"Height\"], bins = 15, kde = True)\n\n\n\n\n\n\n\n\n- boxplot 그리기\n\n기존의 방식\n\n\ndf.plot(kind = \"box\", y = \"Height\")\n\n\n\n\n\n\n\n\n- 오 boxplot에서 kde plot 그리기 \\(\\to\\) sns.violinplot\n\n근데 난 별로…\n\n\nsns.violinplot(y = df[\"Height\"])\n\n\n\n\n\n\n\n\n- 키와 몸무게의 연광성 보기\n\n기존의 방식\n\n\ndf.plot(kind = \"scatter\", x = \"Height\", y = \"Weight\")\n\n\n\n\n\n\n\n\n- seaborn\n\nsns.kdeplot(df[\"Height\"], df[\"Weight\"])\n\nC:\\Users\\rkdcj\\Anaconda3\\lib\\site-packages\\seaborn\\_decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n  warnings.warn(\n\n\n\n\n\n\n\n\n\n\n등고선처럼 표시가 되어있다?\n\n- 각각에 대한 kde그리기\n\nsns.kdeplot(df[\"Height\"])\n\n\n\n\n\n\n\n\n\nsns.kdeplot(df[\"Weight\"])\n\n\n\n\n\n\n\n\n- 아 아래 그래프는 x축에는 키에 대한 kde, y축에는 몸무게에 대한 kde를 그린 것이다.\n\nsns.kdeplot(df[\"Height\"], df[\"Weight\"])\n\nC:\\Users\\rkdcj\\Anaconda3\\lib\\site-packages\\seaborn\\_decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n  warnings.warn(\n\n\n\n\n\n\n\n\n\n- 음… 한번에 보자\n\nfig, axes = plt.subplots(1,3, figsize = (12,4))\n\nax1,ax2,ax3 = axes\n\nsns.kdeplot(df[\"Height\"], ax = ax1)\nsns.kdeplot(df[\"Weight\"], ax = ax2)\nsns.kdeplot(df[\"Height\"], df[\"Weight\"], ax = ax3)\n\nfig.tight_layout()\n\nC:\\Users\\rkdcj\\Anaconda3\\lib\\site-packages\\seaborn\\_decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n  warnings.warn(\n\n\n\n\n\n\n\n\n\n\n3번쨰 그림 해석 : 선끼리 가까우면 가파르게 올라가는 것, 멀어지면 완만하게 올라가거나 내려가는 것을 의마한다.\n근데.. 이거 굳이 쓰지말자..해석하는데 머리 아프다..",
    "crumbs": [
      "Posts",
      "CS",
      "00. DA",
      "02. 데이터 분석과 시각화",
      "01. Seaborn 시각화"
    ]
  },
  {
    "objectID": "posts/CS/00. DA/02. 데이터 분석과 시각화/03. EDA.html",
    "href": "posts/CS/00. DA/02. 데이터 분석과 시각화/03. EDA.html",
    "title": "03. EDA",
    "section": "",
    "text": "- Exploratory Data Analysis\n\n데이터셋을 다양한 관점에서 살펴보고 탐색하며 인사이트를 찾는 것!\n데이터의 분포, 연관성 등을 살펴보며 데이터를 분석한다.\n이를 통해 중요한 비즈니스 문제 등을 해결하기 위한 인사이트를 도출할 수 있다.\nEDA는 뚜렷한 공식이 없다. 그러나 시각화 기법이 가장 많이 사용된다.",
    "crumbs": [
      "Posts",
      "CS",
      "00. DA",
      "02. 데이터 분석과 시각화",
      "03. EDA"
    ]
  },
  {
    "objectID": "posts/CS/00. DA/02. 데이터 분석과 시각화/03. EDA.html#ex1",
    "href": "posts/CS/00. DA/02. 데이터 분석과 시각화/03. EDA.html#ex1",
    "title": "03. EDA",
    "section": "ex1",
    "text": "ex1\noccupations.csv 을 보고, 여성분들이 가장 많이 종사하고 있는 직종이 무엇인지 파악해 보세요.\n- 상위직종 3개 골라보기\n\ndf = pd.read_csv(\"data/occupations.csv\")\n\ndf.loc[df.gender == \"F\"].occupation.value_counts()\n\noccupation\nstudent          60\nother            36\nadministrator    36\nlibrarian        29\neducator         26\nwriter           19\nartist           13\nhealthcare       11\nmarketing        10\nhomemaker         6\nprogrammer        6\nnone              4\nexecutive         3\nscientist         3\nsalesman          3\nengineer          2\nlawyer            2\nentertainment     2\nretired           1\ntechnician        1\nName: count, dtype: int64",
    "crumbs": [
      "Posts",
      "CS",
      "00. DA",
      "02. 데이터 분석과 시각화",
      "03. EDA"
    ]
  },
  {
    "objectID": "posts/CS/00. DA/02. 데이터 분석과 시각화/03. EDA.html#ex2",
    "href": "posts/CS/00. DA/02. 데이터 분석과 시각화/03. EDA.html#ex2",
    "title": "03. EDA",
    "section": "ex2",
    "text": "ex2\n- 남자는?\n\ndf.loc[df.gender == \"M\"].occupation.value_counts()\n\noccupation\nstudent          136\nother             69\neducator          69\nengineer          65\nprogrammer        60\nadministrator     43\nexecutive         29\nscientist         28\ntechnician        26\nwriter            26\nlibrarian         22\nentertainment     16\nmarketing         16\nartist            15\nretired           13\nlawyer            10\nsalesman           9\ndoctor             7\nnone               5\nhealthcare         5\nhomemaker          1\nName: count, dtype: int64",
    "crumbs": [
      "Posts",
      "CS",
      "00. DA",
      "02. 데이터 분석과 시각화",
      "03. EDA"
    ]
  },
  {
    "objectID": "posts/CS/00. DA/02. 데이터 분석과 시각화/03. EDA.html#ex1-1",
    "href": "posts/CS/00. DA/02. 데이터 분석과 시각화/03. EDA.html#ex1-1",
    "title": "03. EDA",
    "section": "ex1",
    "text": "ex1\n5라고 대답한 사람들은 아침에 일어나는 걸 아주 어려워 하는 사람들이고, 1이라고 대답한 사람들은 아침에 쉽게 일어난다.\n\n아침에 일찍 일어나느 사람들이 가장 좋아할 만한 음악 장르는 무엇인가?\n\n\ndf.select_dtypes(\"number\").corr()[\"Getting up\"].sort_values(ascending = False)\n\nGetting up                1.000000\nCheating in school        0.163920\nFun with friends          0.117071\nEntertainment spending    0.116225\nRock                      0.105245\n                            ...   \nReliability              -0.139184\nHealthy eating           -0.145313\nWorkaholism              -0.163420\nFinances                 -0.202493\nPrioritising workload    -0.256098\nName: Getting up, Length: 139, dtype: float64",
    "crumbs": [
      "Posts",
      "CS",
      "00. DA",
      "02. 데이터 분석과 시각화",
      "03. EDA"
    ]
  },
  {
    "objectID": "posts/CS/00. DA/02. 데이터 분석과 시각화/03. EDA.html#ex1.-타이타닉-eda",
    "href": "posts/CS/00. DA/02. 데이터 분석과 시각화/03. EDA.html#ex1.-타이타닉-eda",
    "title": "03. EDA",
    "section": "ex1. 타이타닉 EDA",
    "text": "ex1. 타이타닉 EDA\n\ndf = pd.read_csv(\"data/titanic.csv\")\ndf.head()\n\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n\n\n\n\n\n\n\n\n\nQ1\n- 타이타닉의 승객은 30대와 40대가 가장 많다?\n\n\n: 20대와 30대가 가장 많다.\n\n\n\nsns.histplot(data = df, x = \"Age\")\n\n\n\n\n\n\n\n\n\n\nQ2\n- 가장 높은 요금을 낸 사람은 30대이다.\n\nimport plotly.express as px\nimport plotly.io as pio\npio.renderers.default = \"plotly_mimetype+notebook_connected\"\n\n\ndf[\"Age\"].describe()\n\ncount    714.000000\nmean      29.699118\nstd       14.526497\nmin        0.420000\n25%       20.125000\n50%       28.000000\n75%       38.000000\nmax       80.000000\nName: Age, dtype: float64\n\n\n\nimport numpy as np\n\n\nbins = [-np.inf, 19,29,39,49,59, np.inf]\nlabels = [\"20대 미만\", \"20대\", \"30대\", \"40대\", \"50대\", \"60대 이상\"] \n\n\ndf[\"Age_label\"] =  pd.cut(df[\"Age\"], bins = bins, labels = labels)\n\n\nimport warnings\n\nwarnings.filterwarnings(action = \"ignore\")\n\n\ndf.groupby(\"Age_label\")[\"Fare\"].sum()\n\nAge_label\n20대 미만    5214.3376\n20대       6001.3662\n30대       6743.0081\n40대       3382.2044\n50대       2300.8000\n60대 이상    1130.1667\nName: Fare, dtype: float64\n\n\n- (O) : 30대가 가장 많은 요금을 지불했음\n\n\nQ3\n- (X) : 생존자가 사망자보다 더 많다.\n\ndf[\"Survived\"].value_counts()\n\nSurvived\n0    549\n1    342\nName: count, dtype: int64\n\n\n\n\nQ4\n- (O) : 가장 많은 사람이 탑승한 곳은 3등실이다.\n\ndf.Pclass.value_counts()\n\nPclass\n3    491\n1    216\n2    184\nName: count, dtype: int64\n\n\n\n\nQ5\n- (O) : 가장 생존율이 높은 객실 등급은 1등실이다.\n\ndf.groupby(\"Pclass\")[\"Survived\"].value_counts(normalize=True)\n\nPclass  Survived\n1       1           0.629630\n        0           0.370370\n2       0           0.527174\n        1           0.472826\n3       0           0.757637\n        1           0.242363\nName: proportion, dtype: float64\n\n\n\n\nQ6\n- (X) : 나이가 어릴수록 생존율이 높다.\n\ndf.select_dtypes(\"number\").corr()[\"Age\"]\n\nPassengerId    0.036847\nSurvived      -0.077221\nPclass        -0.369226\nAge            1.000000\nSibSp         -0.308247\nParch         -0.189119\nFare           0.096067\nName: Age, dtype: float64\n\n\n- 저렇게 보면 나이가 어릴수록 생존율이 높다고 볼 수 있음\n\n그러나 분포의 그렇게 큰 차이가 보이지 않으므로 정확한 답을 내리기 어렵다….\n\n\nsns.stripplot(data=df, x=\"Survived\", y=\"Age\", hue = \"Survived\")\n\n\n\n\n\n\n\n\n\n\nQ7\n- (O) : 나이보다 성별이 생존율에 더 많은 영향을 미친다.\n\ndf[\"Sex\"] = df[\"Sex\"].replace([\"male\",\"female\"],[0,1])\n\n\ndf.select_dtypes(\"number\").corr()[\"Survived\"].sort_values(ascending = False)\n\nSurvived       1.000000\nSex            0.543351\nFare           0.257307\nParch          0.081629\nPassengerId   -0.005007\nSibSp         -0.035322\nAge           -0.077221\nPclass        -0.338481\nName: Survived, dtype: float64\n\n\n\n# 생존 여부에 따른 나이 및 성별 분포\nsns.stripplot(data=df, x=\"Survived\", y=\"Age\", hue=\"Sex\")\n\n\n\n\n\n\n\n\n- 나이 분포는 비슷한 데 비해, 성별의 분포는 확연히 차이가 난다.\n\n따라서, 생존율은 나이보다는 확실히 성별에 영향을 많이 받았다는 걸 알 수 있습니다.",
    "crumbs": [
      "Posts",
      "CS",
      "00. DA",
      "02. 데이터 분석과 시각화",
      "03. EDA"
    ]
  },
  {
    "objectID": "posts/CS/00. DA/03. 자연어 처리/00. Intro.html",
    "href": "posts/CS/00. DA/03. 자연어 처리/00. Intro.html",
    "title": "00. Intro",
    "section": "",
    "text": "- 언어의 종류\n\n인공어 : 정보 전달을 위해 인위적으로 만들어진 언어\n\n\n프로그래밍 언어(C, Python, R 등등…)\n\n\n자연어 : 사람들의 일상 생활에서 자연 발생된 언어\n\n\n영어, 중국어, 한국어 등등…\n\n- 자연어 처리 (NLP, Natural Language Processing)\n\n자연어 데이터를 컴퓨터가 처리할 수 있는 형태로 가공하여 의미 있는 분석을 하는 모든 과정\n\n\n\n- NLU, Natural Language Understanding\n\n감성분석, 스팸메일 분류 등\n\n- NLG, Natural Language Generation\n\n신문기사 작성, 챗 GPT 등\n\n- 위 둘은, 자연어 처리의 하위 분야들로, 이름 그대로 자연어의 의미를 잏하고, 새로운 자연어를 생성하기 위한 기술들이 포함된다.",
    "crumbs": [
      "Posts",
      "CS",
      "00. DA",
      "03. 자연어 처리",
      "00. Intro"
    ]
  },
  {
    "objectID": "posts/CS/00. DA/03. 자연어 처리/00. Intro.html#종류",
    "href": "posts/CS/00. DA/03. 자연어 처리/00. Intro.html#종류",
    "title": "00. Intro",
    "section": "",
    "text": "- NLU, Natural Language Understanding\n\n감성분석, 스팸메일 분류 등\n\n- NLG, Natural Language Generation\n\n신문기사 작성, 챗 GPT 등\n\n- 위 둘은, 자연어 처리의 하위 분야들로, 이름 그대로 자연어의 의미를 잏하고, 새로운 자연어를 생성하기 위한 기술들이 포함된다.",
    "crumbs": [
      "Posts",
      "CS",
      "00. DA",
      "03. 자연어 처리",
      "00. Intro"
    ]
  },
  {
    "objectID": "posts/CS/00. DA/03. 자연어 처리/00. Intro.html#규칙-기반-접근법",
    "href": "posts/CS/00. DA/03. 자연어 처리/00. Intro.html#규칙-기반-접근법",
    "title": "00. Intro",
    "section": "규칙 기반 접근법",
    "text": "규칙 기반 접근법\n- Rule Based Approach\n- 100일, 100미터, 100원 이렇듯이 앞에 숫자가 오고, 뒤에 문자가 온다는 규칙성이 있음\n\n해당 규칙성을 토대로 숫자 뒤에 나오는 단어에 따라 부여된 의미들을 파악할 수 있다.\n또한, 규칙 기반 접근법은 일정한 패턴을 가지는 자연어에 대해서 안정적으로 좋은 성능을 낼 수 있기 때문에 실제 상용 서비스에서도 많이 활용되고 있다.",
    "crumbs": [
      "Posts",
      "CS",
      "00. DA",
      "03. 자연어 처리",
      "00. Intro"
    ]
  },
  {
    "objectID": "posts/CS/00. DA/03. 자연어 처리/00. Intro.html#통계-기반-접근법",
    "href": "posts/CS/00. DA/03. 자연어 처리/00. Intro.html#통계-기반-접근법",
    "title": "00. Intro",
    "section": "통계 기반 접근법",
    "text": "통계 기반 접근법\n- Statistiacl Based Approach\n그러나 자연어에는 일정한 규칙이 없는 경우도 많다…. 그럴 때, 통계 기반 접근법을 사용!\n- 출현 빈도 술를 기반으로 문서에서 중요한 내용 파악 등\n- 또한, 머신러닝, 딥러닝을 활용한 자연어 처리 기법도 통계 기반 접근법이다.\n- 일관된 패턴을 파악하기 어려운 자연어에섣고 유의미한 정보를 찾을 수 이기 때문에 통계 기반 접근법은 많은 관심을 받고 있음",
    "crumbs": [
      "Posts",
      "CS",
      "00. DA",
      "03. 자연어 처리",
      "00. Intro"
    ]
  },
  {
    "objectID": "posts/CS/00. DA/03. 자연어 처리/00. Intro.html#특성-1.-중의성",
    "href": "posts/CS/00. DA/03. 자연어 처리/00. Intro.html#특성-1.-중의성",
    "title": "00. Intro",
    "section": "특성 1. 중의성",
    "text": "특성 1. 중의성\n문장 1. She had the lead in a new film\n문장 2. She found lead.\n위 문장에서 lead는 이끈다, 납이라는 두가지 의미를 가짐\n이런 중의적인 단어, 문장을 파악하는 것은 사람에게도 쉽지 않다. 마찬가지로 컴퓨터도 중의적인 문장에 굉장히 취약함…",
    "crumbs": [
      "Posts",
      "CS",
      "00. DA",
      "03. 자연어 처리",
      "00. Intro"
    ]
  },
  {
    "objectID": "posts/CS/00. DA/03. 자연어 처리/00. Intro.html#특성-2.-의미-중복성",
    "href": "posts/CS/00. DA/03. 자연어 처리/00. Intro.html#특성-2.-의미-중복성",
    "title": "00. Intro",
    "section": "특성 2. 의미 중복성",
    "text": "특성 2. 의미 중복성\n문장 1. 비행기가 1시에 떠날 예정입니다.\n문장 2. 비행기 출발 시간은 1시입니다.\n이렇게 동일한 의미를 다양한 방식으로 표현하는걸 패러프레이징(Paraphrasing)이라고 하며 컴퓨터가 이해하하는 데 굉장히 어려움…",
    "crumbs": [
      "Posts",
      "CS",
      "00. DA",
      "03. 자연어 처리",
      "00. Intro"
    ]
  },
  {
    "objectID": "posts/CS/00. DA/03. 자연어 처리/00. Intro.html#특성-3.-관계성",
    "href": "posts/CS/00. DA/03. 자연어 처리/00. Intro.html#특성-3.-관계성",
    "title": "00. Intro",
    "section": "특성 3. 관계성",
    "text": "특성 3. 관계성\n파랑과 빨강 중 분홍과 더 유사한 단어? \\(\\to\\) 빨강\n단어 간에는 의미를 기준으로 관계가 형성됨. 그러나 컴퓨터는 이를 이해하기 어려움…",
    "crumbs": [
      "Posts",
      "CS",
      "00. DA",
      "03. 자연어 처리",
      "00. Intro"
    ]
  },
  {
    "objectID": "posts/CS/00. DA/03. 자연어 처리/00. Intro.html#특성-4.-한국어",
    "href": "posts/CS/00. DA/03. 자연어 처리/00. Intro.html#특성-4.-한국어",
    "title": "00. Intro",
    "section": "특성 4. 한국어",
    "text": "특성 4. 한국어\n일반적으로 한국어는 다른 언어보다 자연어 처리가 더 어렵다….\n\n(1) 접사와 조사 처리\n한국어는 교착어, 즉 어근에 붙는 접사에 따라 의미가 변하는 언어이다.\n보통 자연어 러치를 하려면 어떠한 단어의 어근을 찾아서 사용하는 게 일반적이다. 그러나 한국어는 교착어이기 때문에 조사의 종류에 따라 의미 차이에 큰 변화가 생김\n\n\n(2) 유연한 어순\n\n나는 공부하러 간다.\n공부하러 나는 간다.\n나는 간다. 공부하러\n\n위 같은 유연한 어순 때문에 컴퓨터가 다음에 출현할 단어를 예측하는 것이 어렵다.\n\n\n(3) 잘 지켜지지 않는 띄어쓰기\n\n이건 뭐 한국인이라면 다들 알법한 문제이다….",
    "crumbs": [
      "Posts",
      "CS",
      "00. DA",
      "03. 자연어 처리",
      "00. Intro"
    ]
  },
  {
    "objectID": "posts/CS/00. DA/03. 자연어 처리/02. 자연어 전처리 (2).html",
    "href": "posts/CS/00. DA/03. 자연어 처리/02. 자연어 전처리 (2).html",
    "title": "02. 자연어 전처리 (2)",
    "section": "",
    "text": "문장 단위 토큰화를 수행하는 경우\n1 품사 태깅\n\n어떠한 단어의 품사는 그 단어 자체의 의미와 함께 문장 안에서 사용된 위치에 따라 달라질 수 있음\n이럴 경우, 문장 간의 구분이 된 상태에서 단어의 품사를 정해야 하기 때문에 문장 단위로 먼저 토큰화를 수행해야 한다.\n\n\n\n1 필요한 패키지와 함수 불러오기\n\nfrom nltk.tokenize import sent_tokenize\nimport nltk\n\nnltk.download('punkt')\n\n[nltk_data] Downloading package punkt to\n[nltk_data]     C:\\Users\\rkdcj\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n\n\nTrue\n\n\n2 문장 토큰화\n\ntext = \"My email address is 'abcde@codeit.com'. Send it to Mr.Kim.\"\n\ntokenized_sents = sent_tokenize(text)\n\n\n문장이 끝나는 지점의 마침표를 기준으로 토큰화가 수행됨.\n\n\ntokenized_sents\n\n[\"My email address is 'abcde@codeit.com'.\", 'Send it to Mr.Kim.']",
    "crumbs": [
      "Posts",
      "CS",
      "00. DA",
      "03. 자연어 처리",
      "02. 자연어 전처리 (2)"
    ]
  },
  {
    "objectID": "posts/CS/00. DA/03. 자연어 처리/02. 자연어 전처리 (2).html#nltk",
    "href": "posts/CS/00. DA/03. 자연어 처리/02. 자연어 전처리 (2).html#nltk",
    "title": "02. 자연어 전처리 (2)",
    "section": "",
    "text": "1 필요한 패키지와 함수 불러오기\n\nfrom nltk.tokenize import sent_tokenize\nimport nltk\n\nnltk.download('punkt')\n\n[nltk_data] Downloading package punkt to\n[nltk_data]     C:\\Users\\rkdcj\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n\n\nTrue\n\n\n2 문장 토큰화\n\ntext = \"My email address is 'abcde@codeit.com'. Send it to Mr.Kim.\"\n\ntokenized_sents = sent_tokenize(text)\n\n\n문장이 끝나는 지점의 마침표를 기준으로 토큰화가 수행됨.\n\n\ntokenized_sents\n\n[\"My email address is 'abcde@codeit.com'.\", 'Send it to Mr.Kim.']",
    "crumbs": [
      "Posts",
      "CS",
      "00. DA",
      "03. 자연어 처리",
      "02. 자연어 전처리 (2)"
    ]
  },
  {
    "objectID": "posts/CS/00. DA/03. 자연어 처리/02. 자연어 전처리 (2).html#extra",
    "href": "posts/CS/00. DA/03. 자연어 처리/02. 자연어 전처리 (2).html#extra",
    "title": "02. 자연어 전처리 (2)",
    "section": "extra",
    "text": "extra\nNLTK의 pos_tag()함수는 Penn Treebank POS Tags를 기준으로 품사를 태깅한다.\n\n각 품사의 대웅하는 태그는 다음과 같다.",
    "crumbs": [
      "Posts",
      "CS",
      "00. DA",
      "03. 자연어 처리",
      "02. 자연어 전처리 (2)"
    ]
  },
  {
    "objectID": "posts/CS/00. DA/03. 자연어 처리/02. 자연어 전처리 (2).html#제로-패딩",
    "href": "posts/CS/00. DA/03. 자연어 처리/02. 자연어 전처리 (2).html#제로-패딩",
    "title": "02. 자연어 전처리 (2)",
    "section": "제로 패딩",
    "text": "제로 패딩\n- 가장 긴 문장의 길이를 구하여 해당 값을 기준으로 문장 길이를 맞추는 방법\n\n비어있는 값은 0으로 채운다.\n\n1 기존 정수 인코딩한 데이터에서 최대 토큰이 몇 개인지 확인\n\nmax_len = max(len(item) for item in df['integer_encoded'])\n\nprint('토큰의 최대 개수:', max_len)\n\n토큰의 최대 개수: 200\n\n\n2 해당 값을 기준으로 다른 문장(코퍼스)들의 길이가 200이 되도록 0을 채워 넣음\n\nfor tokens in df['integer_encoded']:\n    while len(tokens) &lt; max_len:\n        tokens.append(0)\n\ndf[['integer_encoded']]\n\n\n\n\n\n\n\n\n\ninteger_encoded\n\n\n\n\n0\n[8, 11, 2, 54, 8, 16, 5, 1, 12, 54, 8, 16, 5, ...\n\n\n1\n[2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n\n\n2\n[55, 56, 19, 57, 58, 59, 57, 60, 61, 25, 62, 6...\n\n\n3\n[2, 2, 30, 69, 3, 30, 31, 3, 69, 70, 71, 30, 7...\n\n\n4\n[72, 1, 5, 17, 1, 1, 8, 1, 73, 74, 1, 1, 74, 1...\n\n\n5\n[35, 36, 78, 78, 35, 36, 79, 79, 35, 36, 0, 0,...\n\n\n6\n[80, 1, 3, 21, 81, 82, 21, 21, 80, 3, 82, 83, ...\n\n\n7\n[85, 86, 37, 38, 87, 39, 86, 88, 40, 89, 41, 9...\n\n\n8\n[120, 1, 1, 121, 52, 10, 121, 122, 53, 1, 52, ...\n\n\n9\n[123, 123, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n\n\n\n\n\n\n\n\n- 결과 확인\n\n[len(i) for i in df['integer_encoded']]\n\n[200, 200, 200, 200, 200, 200, 200, 200, 200, 200]",
    "crumbs": [
      "Posts",
      "CS",
      "00. DA",
      "03. 자연어 처리",
      "02. 자연어 전처리 (2)"
    ]
  },
  {
    "objectID": "posts/CS/00. DA/03. 자연어 처리/04. 한국어 자연어 처리.html",
    "href": "posts/CS/00. DA/03. 자연어 처리/04. 한국어 자연어 처리.html",
    "title": "04. 한국어 자연어 처리",
    "section": "",
    "text": "네이버 맞춤법 검사기를 이용한 파이썬용 한글 맞춤법 검사 라이브러리\n띄어 쓰기 교정을 포함한 여러 맞춤법 오류를 교정해줌\n1 패키지 설치\n\n링크로 넘어가 code를 다운로드 한다.\n다운로드한 파일을 현재 작업폴더로 옮긴 후 압축 해제\n그리고 알맞게 경로 이동 후 다름과 같은 명령어 실행\n\n!python setup.py install\n2 맞춤법 교정\n\n패키지가 만들어진지 조금 오래 돼서 다음과 같은 오류가 발생한다….\n\n\nfrom hanspell import spell_checker\n\ntext = \"아버지가방에들어가신다나는오늘코딩을했다\"\n\nhanspell_sent = spell_checker.check(text)\nprint(hanspell_sent.checked)\n\nKeyError: 'result'\n\n\n\n\n\n\nsoyspacing : 형태소 분석, 품사 판별, 띄어쓰기 교정 모듈 등을 제공하는 soynlp의 띄어쓰기 교정 모듈입니다. 이 띄어쓰기 교정 모듈은 대량의 코퍼스에서 띄어쓰기 패턴을 학습한 모델을 생성 한 후, 학습한 모델을 통해 패턴대로 띄어쓰기를 교정합니다.\nPyKoSpacinb : 전희원님이 개발한 띄어쓰기 교정기입니다. 대용량 코퍼스를 학습하여 만들어진 띄어쓰기 딥러닝 모델로 뛰어난 성능을 가지고 있습니다.",
    "crumbs": [
      "Posts",
      "CS",
      "00. DA",
      "03. 자연어 처리",
      "04. 한국어 자연어 처리"
    ]
  },
  {
    "objectID": "posts/CS/00. DA/03. 자연어 처리/04. 한국어 자연어 처리.html#py-hanspell",
    "href": "posts/CS/00. DA/03. 자연어 처리/04. 한국어 자연어 처리.html#py-hanspell",
    "title": "04. 한국어 자연어 처리",
    "section": "",
    "text": "네이버 맞춤법 검사기를 이용한 파이썬용 한글 맞춤법 검사 라이브러리\n띄어 쓰기 교정을 포함한 여러 맞춤법 오류를 교정해줌\n1 패키지 설치\n\n링크로 넘어가 code를 다운로드 한다.\n다운로드한 파일을 현재 작업폴더로 옮긴 후 압축 해제\n그리고 알맞게 경로 이동 후 다름과 같은 명령어 실행\n\n!python setup.py install\n2 맞춤법 교정\n\n패키지가 만들어진지 조금 오래 돼서 다음과 같은 오류가 발생한다….\n\n\nfrom hanspell import spell_checker\n\ntext = \"아버지가방에들어가신다나는오늘코딩을했다\"\n\nhanspell_sent = spell_checker.check(text)\nprint(hanspell_sent.checked)\n\nKeyError: 'result'",
    "crumbs": [
      "Posts",
      "CS",
      "00. DA",
      "03. 자연어 처리",
      "04. 한국어 자연어 처리"
    ]
  },
  {
    "objectID": "posts/CS/00. DA/03. 자연어 처리/04. 한국어 자연어 처리.html#extra.-이외-교정-도구",
    "href": "posts/CS/00. DA/03. 자연어 처리/04. 한국어 자연어 처리.html#extra.-이외-교정-도구",
    "title": "04. 한국어 자연어 처리",
    "section": "",
    "text": "soyspacing : 형태소 분석, 품사 판별, 띄어쓰기 교정 모듈 등을 제공하는 soynlp의 띄어쓰기 교정 모듈입니다. 이 띄어쓰기 교정 모듈은 대량의 코퍼스에서 띄어쓰기 패턴을 학습한 모델을 생성 한 후, 학습한 모델을 통해 패턴대로 띄어쓰기를 교정합니다.\nPyKoSpacinb : 전희원님이 개발한 띄어쓰기 교정기입니다. 대용량 코퍼스를 학습하여 만들어진 띄어쓰기 딥러닝 모델로 뛰어난 성능을 가지고 있습니다.",
    "crumbs": [
      "Posts",
      "CS",
      "00. DA",
      "03. 자연어 처리",
      "04. 한국어 자연어 처리"
    ]
  },
  {
    "objectID": "posts/CS/00. DA/03. 자연어 처리/04. 한국어 자연어 처리.html#konlpy",
    "href": "posts/CS/00. DA/03. 자연어 처리/04. 한국어 자연어 처리.html#konlpy",
    "title": "04. 한국어 자연어 처리",
    "section": "KoNLPy",
    "text": "KoNLPy\n- 한국어 자연어 처리를 위한 파이썬 패키지\n- 공식문서\n1 패키지 설치\n!pip install --upgrade pip\n!pip install konlpy\n\n추가설치사항 참고\n복잡함, 설치 다 끝내구 주피터 한번 껐다가 켜야함\n\n2 패키지 로드\n\nimport konlpy\n\n총 5개의 형태소 분석 모듈이 존재\n\nKkma\nKomoran\nOkt\nHannanum\nMecab(이 모듈은 Windows를 지원하지 않음)\n\n3 형태소 분석기 생성\n\nfrom konlpy.tag import Kkma, Komoran, Okt, Hannanum\n\nkkma = Kkma()\nkomoran = Komoran()\nokt = Okt()\nhannanum = Hannanum()\n\n4 형태소 분석 결과 확인\n\ntext = \"아버지가 방에 들어가신다 나는 오늘 코딩을 했다\"\n\nprint(\"Kkma: \", kkma.morphs(text))\nprint(\"Komoran: \", komoran.morphs(text))\nprint(\"Okt: \", okt.morphs(text))\nprint(\"Hannanum: \", hannanum.morphs(text))\n\nKkma:  ['아버지', '가', '방', '에', '들어가', '시', 'ㄴ다', '나', '는', '오늘', '코딩', '을', '하', '었', '다']\nKomoran:  ['아버지', '가', '방', '에', '들어가', '시', 'ㄴ다', '나', '는', '오늘', '코', '딩', '을', '하', '았', '다']\nOkt:  ['아버지', '가', '방', '에', '들어가신다', '나', '는', '오늘', '코딩', '을', '했다']\nHannanum:  ['아버지', '가', '방', '에', '들', '어', '가', '시ㄴ다', '나', '는', '오늘', '코딩', '을', '하', '었다']",
    "crumbs": [
      "Posts",
      "CS",
      "00. DA",
      "03. 자연어 처리",
      "04. 한국어 자연어 처리"
    ]
  },
  {
    "objectID": "posts/CS/00. DA/03. 자연어 처리/04. 한국어 자연어 처리.html#이외-형태소-분석기들",
    "href": "posts/CS/00. DA/03. 자연어 처리/04. 한국어 자연어 처리.html#이외-형태소-분석기들",
    "title": "04. 한국어 자연어 처리",
    "section": "이외 형태소 분석기들",
    "text": "이외 형태소 분석기들\n\nsoynlp : soynlp에서는 L tokenizer, MaxScoreTokenizer와 같은 형태소 분석기도 제공하고 있습니다. 형태소 분석기 외에도 명사 추출기 등 한국어 자연어 분석을 위한 다양한 기능이 있습니다.\nKhaiii : 2018년에 카카오가 공개한 오픈소스 한국어 형태소 분석기입니다.\nGoogle sentencepiece : 2018년에 구글에서 공개한 형태소 분석 패키지 입니다.",
    "crumbs": [
      "Posts",
      "CS",
      "00. DA",
      "03. 자연어 처리",
      "04. 한국어 자연어 처리"
    ]
  },
  {
    "objectID": "posts/CS/00. DA/03. 자연어 처리/04. 한국어 자연어 처리.html#korquad",
    "href": "posts/CS/00. DA/03. 자연어 처리/04. 한국어 자연어 처리.html#korquad",
    "title": "04. 한국어 자연어 처리",
    "section": "KorQuAD",
    "text": "KorQuAD\n2018년에 LG CNS에서 구축하여 공개한 한국어 질의응답 데이터셋\n현재 약 10만 건의 데이터가 있으며, 버전이 업데이트 될 때마다 질의 응답 쌍이 계속 추가되고 있음\n\nKorQuAD 데이터 바로가기",
    "crumbs": [
      "Posts",
      "CS",
      "00. DA",
      "03. 자연어 처리",
      "04. 한국어 자연어 처리"
    ]
  },
  {
    "objectID": "posts/CS/00. DA/03. 자연어 처리/04. 한국어 자연어 처리.html#네이버-영화리뷰",
    "href": "posts/CS/00. DA/03. 자연어 처리/04. 한국어 자연어 처리.html#네이버-영화리뷰",
    "title": "04. 한국어 자연어 처리",
    "section": "네이버 영화리뷰",
    "text": "네이버 영화리뷰\n대표적인 감성 지수에 대한 데이터\n\n네이버 영화 리뷰 데이터 바로가기",
    "crumbs": [
      "Posts",
      "CS",
      "00. DA",
      "03. 자연어 처리",
      "04. 한국어 자연어 처리"
    ]
  },
  {
    "objectID": "posts/CS/00. DA/03. 자연어 처리/04. 한국어 자연어 처리.html#한국어-위키-백과",
    "href": "posts/CS/00. DA/03. 자연어 처리/04. 한국어 자연어 처리.html#한국어-위키-백과",
    "title": "04. 한국어 자연어 처리",
    "section": "한국어 위키 백과",
    "text": "한국어 위키 백과\n자연어 데이터를 쉽게 다운로드 받을 수 있도록 하지 않음\n따라서 크롤링을 수행해야함. 다음 시간에는 크롤링을 학습하여 자동화까지 구축해보자!",
    "crumbs": [
      "Posts",
      "CS",
      "00. DA",
      "03. 자연어 처리",
      "04. 한국어 자연어 처리"
    ]
  },
  {
    "objectID": "posts/CS/01. AI/07. 추천 시스템/00. Intro.html",
    "href": "posts/CS/01. AI/07. 추천 시스템/00. Intro.html",
    "title": "00. Intro",
    "section": "",
    "text": "0. Intro\n- 넷플릭스, 음악 추천, 페이스북(인스타) 친구 추천 등등 모두 AI를 활용한 추천 시스템을 이용한다.\n- 좋은 추천 시스템 \\(\\to\\) 사회적 이익, 많은 매출 등으로 이어진다.\n- 대부분의 경우 머신러닝을 활용해서 추천 시스템이 만들어진다.\n- 어떤 머신러닝 주제들보다 우리 생활 속에 깊게 자리잡고 있음\n- 정의 : 어떤 작업(추천)을 할 때, 경험(유저의 행동 패턴)을 통해, 그 작업에 대한 성능(추천을 더 정확하게!)이 향상되는 프로그램(\\(\\star\\star\\star\\))\n\nexample : 상품에 대한 영화 평점데이터(rating data)\n\n목적 : 아래와 같이 빈칸(-)들 즉, 주어진 평점 데이터를 바탕으로 어떤 대상이 어떤 영화를 보았을 때 평점이 얼마인지 예측하는 것!\n\n\n\n\n\n러브 액츄얼리\n반지의 제왕\n해리 포터\n극한 직업\n\n\n\n\n현승\n5\n1\n-\n5\n\n\n영훈\n-\n2\n2\n0\n\n\n동욱\n-\n4\n0\n1\n\n\n종훈\n3\n-\n4\n3\n\n\n우재\n5\n4\n-\n2\n\n\n…\n…\n…\n…\n…\n\n\n\n\n즉, 유저와 상품의 관계를 표현한 데이터 사용\n유저와 상호 작요이 없었던 상품에 대한 선호도를 예측\n선호도가 높게 예측되는 상품들을 유저에게 추천\n\n\n- 사용되는 데이터 종류\n\n직접 데이터(explicit data)\n\n유저가 직접적으로 상품에 대한 만족/선호도를 표시한 데이터\n유저 영화 평점 데이터, 유튜브 좋아요/싫어요 데이터\n유저 선호도를 정확하게 나타낸다.\n\n간접 데이터(implicit data)\n\n유저가 직접적 선호도를 표시하지는 않았지만 유추할 수 있는 데이터\n유튜브 유저 영상 시청 데이터\n아마존 유저 구매 데이터\n데이터 형태는 똑같이 표현함\n무엇을 의미하는 지 정확하지 않음\n데이터를 수집하기 쉬워 직접 데이터보다 자주 이용된다.(\\(\\star\\star\\star\\))",
    "crumbs": [
      "Posts",
      "CS",
      "01. AI",
      "07. 추천 시스템",
      "00. Intro"
    ]
  }
]