{"title":"01. 자연어 전처리","markdown":{"yaml":{"title":"01. 자연어 전처리","author":"GC","date":"05/14/24"},"headingText":"01. 자연어 전처리란?","containsRefs":false,"markdown":"\n\n\n## ex1\n\n`1` 맞춤법과 띄어쓰기 교정\n\n> ex 1.  Oh, Hi helo. Nice to meetyou.\n> \n> * step 1. 띄어쓰기 수정 : Oh, Hi hello. Nice to `meet you`.\n>\n> * step 2. 문장의 의미를 표현하는데 크게 기여하지 않는 단어 삭제\n>   * ~Oh~, Hi hello. Nice to `meet you`.\n>  \n> * step 3. 중복된 의미 단어 제거\n>   * ~Oh~, Hi ~hello~. Nice to `meet you`.\n\n`2` 컴퓨터가 자연어를 잘 이해할 수 있도록 각 단어에 숫자 인덱스를 부여\n\n* {'Hi':0, 'Nice':1, 'to':2, 'meet':3, 'you':4}\n\n***\n\n# 자연어 전처리 과정들\n\n아래의 순서가 일반적이나 정해진 표준은 아님. 분석의 목적과 활용할 자연어 데이터의 특성에 따라 적용해야 하는 전처리 단계가 다 다르고, 각 단계를 적용하는 순서에도 차이가 발생할 수 있음\n\n`1` 토큰화: 자연어 데이터를 분석을 위한 작은 단위(토큰)로 분리\n\n`2` 정제: 분석에 큰 의미가 없는 데이터들을 제거\n\n`3` 정규화: 표현 방법이 다르지만 의미가 같은 단어들을 통합\n\n`4` 정수 인코딩: 컴퓨터가 이해하기 쉽도록 자연어 데이터에 정수 인덱스를 부여\n\n***\n\n# 02. 단어 토큰화(Word Tokenization)\n\n`-` 코퍼스(Corpus) : 분석에 활용하기 위한 자연어 데이터(말뭉치)\n\n`-` 이 코퍼스를 분석에 활용하려면 먼저 의미있는 작은 단위로 나눠야함\n\n* `의미있는 작은 단위` $\\to$ **토큰(Token)**\n\n* 위 같은 과정을 토큰화(Tokenization)이라고 한다.\n\n`-` 토큰화에는 단어 토큰화와 문장 토큰화가 있으며, 분석 목적에 맞게 둘 중 필요한 토큰화 방식을 적절하게 사용해야함.\n\n* 이번 챕터에서는 단어 토큰화에 대해 학습할 것임\n\n## NLTK\n\n`1` 터미널에서 아래와 같은 커맨드르 실행\n\n```python\nconda install nltk\n```\n\n`2` 패키지에서 함수 로드\n\n`3` `nltk`에서 제공하는 토큰화 모듈인 `punkt`를 다운로드\n\n* `punkt` : 마침표나 약어(Mr., Dr.)와 같은 특별한 언어적 특성을 고려하여 토큰화를 할 수 있게 해주는 모듈\n\n## 단어 토큰화 수행\n\n`-` 기본적으로 띄어쓰기, 어퍼스트로피('), 콤마(,)를 기준으로 토큰화를 수행하고 있으며 `하이픈(-)`은 토큰화의 기준으로 사용하지 않음\n\n* 어떠한 기준을 가지고 단어 토큰화를 하는게 더 좋다고는할 수 없다. 분석에 활용하려는 코퍼스의 특성에 따라 적절한 토큰화 기준을 사용하면 된다.\n\n* [nltk 공식문서](https://www.nltk.org/api/nltk.tokenize.html)\n\n***\n\n# 03. 정제(Cleaning)\n\n`-` 분석 목적에 적합하지 않은 단어들을 코퍼스에서 제거하는 과정\n\n## 등장빈도가 적은 단어\n\n### 빈도가 2이하인 단어들만 찾기\n\n`1` 토큰화\n\n`2` 단어 빈도 수 카운트\n\n`3` 단어 빈도수가 2이하인 단어 리스트 추출\n\n`4` 빈도수가 2이하인 단어들만 제거한 결과를 따로 저장\n\n***\n\n## 길이가 짧은 단어\n\n영어 단어의 경우, 알파벳 하나 또는 두개로 구성된 단어는 코퍼스의 의미를 나타내는 데 중요하지 않을 수 있다.\n\n그래서 이러한 단어들은 제거하는 것이 좋음\n\n`1` 길이가 2이하인 단어 제거\n\n`2` 정제 전과 후의 결과 비교\n\n## 함수 생성\n\n`-` 위에서 만든 정제 기준을 언제든 활용할 수 있도록 함수로 작성\n\n***\n\n# 04. 불용어(stopwords)\n\n정의 : 코퍼스에서 큰 의미가 없거나, 분석 목적에서 벗어나는 단어\n\n## step1. 불용어 세트 준비\n\n`-` `nltk`에서는 기본 불용어 목록 179개를 제공한다.\n\n* 아래와 같은 방법으로 불용어 목록에 접근할 수 있다.\n\n`1` 불용어 목록 로드\n\n`2` 불용어들을 세트 자료형으로 저장\n\n`3` 불용어 추가 및 삭제\n\n## step2. 불용어 제거하기\n\n## step3. 불용어 처리 함수 만들기\n\n***\n\n# 05. 정규화(Normalization)\n\n의미가 같은 단어들을 하나로 통일하는 과정\n\n## 방법 1. 대소문자 통합\n\n## 방법 2. 규칙 기반 정규화\n\n`1` 동의어 사전 작성\n\n`2` 단어 토큰화\n\n`3` 동의어 사전에 있는 단어라면, `value`에 해당하는 값으로 변환\n\n`4` 결과 확인\n\n***\n\n# 06. 어간 추출(Stemming)\n\n어간 : 특정한 단어의 핵심이 되는 부분\n\n어간 추출 : 단어에서 어간을 찾아내 추출하는 것\n\n`-` 포터 스테머 알고리즘(Porter Stemmer Algorithm) : 대표적인 어간 추출 알고리즘\n\n* 단순히 어미만 잘라내는 방식\n\n> ex\n> * Formalize $\\to$ Formal\n> * Relational $\\to$ Relate\n> * **Activate $\\to$ Activ(?)**\n> * Encouragement $\\to$ Encourage\n\n\n* 단순히 어미를 잘라내기 때문에 `Activate` $\\to$ `Activ(?)`처럼 사전에 없는 단어가 추출되는 경우도 있음\n\n* 따라서 코퍼스이 특성이나 분석 여건에 따라 어간 추출을 하는게 적합한지 잘 판단해야 한다. 그렇지 않으면 분석에 활용돼야 하는 중요한 단어가 손실될 수 있음\n\n## NLTK 어간추출\n\n`1` 함수 로드\n\n`2` 단어 토큰화\n\n`3` 어간 추출\n\n## extra. 랭커스터 스테머 알고리즘\n\n`-` 차이점 : 랭커스터 스테머 알고리즘은 뒤에 `e`와 같은 묵음 처리 부분을 어간에 포함시키지 않는다.\n\n***\n\n# 07. 실습. IMDB\n\n`-` 데이터셋 : IMDb는 The Internet Movie Database의 약자로, 약 200만개 이상의 영화 관련 정보들이 저장되어 있는 데이터 베이스이다.\n\n* 이중 10개의 데이터만 가져와서 사용\n\n## step1. 데이터 로드\n\n## step2. 대소문자 통합\n\n## step3. 단어 토큰화\n\n## step4. 데이터 정제\n\n`-` 이건 뭔데?\n\n```python\n%load_ext autoreload\n%autoreload 2\n```\n\n* `ipynb` 파일에서 직접 만든 파이썬 모듈(`.py`)을 불러와 사용할 때, 파이썬 모듈 파일이 중간에 수정되면 해당 내용이 자동으로 반영되지 않는 문제가 발생\n\n* 그래서, `preprocess.py` 파일을 수정할 때마다 커널을 `Restart`해야함\n\n* 이러한 번거러움을 줄이기 위해 위의 코드를 실행한다.\n\n## step5. 어간 추출\n\n## step6. 결과 확인\n","srcMarkdownNoYaml":"\n\n# 01. 자연어 전처리란?\n\n## ex1\n\n`1` 맞춤법과 띄어쓰기 교정\n\n> ex 1.  Oh, Hi helo. Nice to meetyou.\n> \n> * step 1. 띄어쓰기 수정 : Oh, Hi hello. Nice to `meet you`.\n>\n> * step 2. 문장의 의미를 표현하는데 크게 기여하지 않는 단어 삭제\n>   * ~Oh~, Hi hello. Nice to `meet you`.\n>  \n> * step 3. 중복된 의미 단어 제거\n>   * ~Oh~, Hi ~hello~. Nice to `meet you`.\n\n`2` 컴퓨터가 자연어를 잘 이해할 수 있도록 각 단어에 숫자 인덱스를 부여\n\n* {'Hi':0, 'Nice':1, 'to':2, 'meet':3, 'you':4}\n\n***\n\n# 자연어 전처리 과정들\n\n아래의 순서가 일반적이나 정해진 표준은 아님. 분석의 목적과 활용할 자연어 데이터의 특성에 따라 적용해야 하는 전처리 단계가 다 다르고, 각 단계를 적용하는 순서에도 차이가 발생할 수 있음\n\n`1` 토큰화: 자연어 데이터를 분석을 위한 작은 단위(토큰)로 분리\n\n`2` 정제: 분석에 큰 의미가 없는 데이터들을 제거\n\n`3` 정규화: 표현 방법이 다르지만 의미가 같은 단어들을 통합\n\n`4` 정수 인코딩: 컴퓨터가 이해하기 쉽도록 자연어 데이터에 정수 인덱스를 부여\n\n***\n\n# 02. 단어 토큰화(Word Tokenization)\n\n`-` 코퍼스(Corpus) : 분석에 활용하기 위한 자연어 데이터(말뭉치)\n\n`-` 이 코퍼스를 분석에 활용하려면 먼저 의미있는 작은 단위로 나눠야함\n\n* `의미있는 작은 단위` $\\to$ **토큰(Token)**\n\n* 위 같은 과정을 토큰화(Tokenization)이라고 한다.\n\n`-` 토큰화에는 단어 토큰화와 문장 토큰화가 있으며, 분석 목적에 맞게 둘 중 필요한 토큰화 방식을 적절하게 사용해야함.\n\n* 이번 챕터에서는 단어 토큰화에 대해 학습할 것임\n\n## NLTK\n\n`1` 터미널에서 아래와 같은 커맨드르 실행\n\n```python\nconda install nltk\n```\n\n`2` 패키지에서 함수 로드\n\n`3` `nltk`에서 제공하는 토큰화 모듈인 `punkt`를 다운로드\n\n* `punkt` : 마침표나 약어(Mr., Dr.)와 같은 특별한 언어적 특성을 고려하여 토큰화를 할 수 있게 해주는 모듈\n\n## 단어 토큰화 수행\n\n`-` 기본적으로 띄어쓰기, 어퍼스트로피('), 콤마(,)를 기준으로 토큰화를 수행하고 있으며 `하이픈(-)`은 토큰화의 기준으로 사용하지 않음\n\n* 어떠한 기준을 가지고 단어 토큰화를 하는게 더 좋다고는할 수 없다. 분석에 활용하려는 코퍼스의 특성에 따라 적절한 토큰화 기준을 사용하면 된다.\n\n* [nltk 공식문서](https://www.nltk.org/api/nltk.tokenize.html)\n\n***\n\n# 03. 정제(Cleaning)\n\n`-` 분석 목적에 적합하지 않은 단어들을 코퍼스에서 제거하는 과정\n\n## 등장빈도가 적은 단어\n\n### 빈도가 2이하인 단어들만 찾기\n\n`1` 토큰화\n\n`2` 단어 빈도 수 카운트\n\n`3` 단어 빈도수가 2이하인 단어 리스트 추출\n\n`4` 빈도수가 2이하인 단어들만 제거한 결과를 따로 저장\n\n***\n\n## 길이가 짧은 단어\n\n영어 단어의 경우, 알파벳 하나 또는 두개로 구성된 단어는 코퍼스의 의미를 나타내는 데 중요하지 않을 수 있다.\n\n그래서 이러한 단어들은 제거하는 것이 좋음\n\n`1` 길이가 2이하인 단어 제거\n\n`2` 정제 전과 후의 결과 비교\n\n## 함수 생성\n\n`-` 위에서 만든 정제 기준을 언제든 활용할 수 있도록 함수로 작성\n\n***\n\n# 04. 불용어(stopwords)\n\n정의 : 코퍼스에서 큰 의미가 없거나, 분석 목적에서 벗어나는 단어\n\n## step1. 불용어 세트 준비\n\n`-` `nltk`에서는 기본 불용어 목록 179개를 제공한다.\n\n* 아래와 같은 방법으로 불용어 목록에 접근할 수 있다.\n\n`1` 불용어 목록 로드\n\n`2` 불용어들을 세트 자료형으로 저장\n\n`3` 불용어 추가 및 삭제\n\n## step2. 불용어 제거하기\n\n## step3. 불용어 처리 함수 만들기\n\n***\n\n# 05. 정규화(Normalization)\n\n의미가 같은 단어들을 하나로 통일하는 과정\n\n## 방법 1. 대소문자 통합\n\n## 방법 2. 규칙 기반 정규화\n\n`1` 동의어 사전 작성\n\n`2` 단어 토큰화\n\n`3` 동의어 사전에 있는 단어라면, `value`에 해당하는 값으로 변환\n\n`4` 결과 확인\n\n***\n\n# 06. 어간 추출(Stemming)\n\n어간 : 특정한 단어의 핵심이 되는 부분\n\n어간 추출 : 단어에서 어간을 찾아내 추출하는 것\n\n`-` 포터 스테머 알고리즘(Porter Stemmer Algorithm) : 대표적인 어간 추출 알고리즘\n\n* 단순히 어미만 잘라내는 방식\n\n> ex\n> * Formalize $\\to$ Formal\n> * Relational $\\to$ Relate\n> * **Activate $\\to$ Activ(?)**\n> * Encouragement $\\to$ Encourage\n\n\n* 단순히 어미를 잘라내기 때문에 `Activate` $\\to$ `Activ(?)`처럼 사전에 없는 단어가 추출되는 경우도 있음\n\n* 따라서 코퍼스이 특성이나 분석 여건에 따라 어간 추출을 하는게 적합한지 잘 판단해야 한다. 그렇지 않으면 분석에 활용돼야 하는 중요한 단어가 손실될 수 있음\n\n## NLTK 어간추출\n\n`1` 함수 로드\n\n`2` 단어 토큰화\n\n`3` 어간 추출\n\n## extra. 랭커스터 스테머 알고리즘\n\n`-` 차이점 : 랭커스터 스테머 알고리즘은 뒤에 `e`와 같은 묵음 처리 부분을 어간에 포함시키지 않는다.\n\n***\n\n# 07. 실습. IMDB\n\n`-` 데이터셋 : IMDb는 The Internet Movie Database의 약자로, 약 200만개 이상의 영화 관련 정보들이 저장되어 있는 데이터 베이스이다.\n\n* 이중 10개의 데이터만 가져와서 사용\n\n## step1. 데이터 로드\n\n## step2. 대소문자 통합\n\n## step3. 단어 토큰화\n\n## step4. 데이터 정제\n\n`-` 이건 뭔데?\n\n```python\n%load_ext autoreload\n%autoreload 2\n```\n\n* `ipynb` 파일에서 직접 만든 파이썬 모듈(`.py`)을 불러와 사용할 때, 파이썬 모듈 파일이 중간에 수정되면 해당 내용이 자동으로 반영되지 않는 문제가 발생\n\n* 그래서, `preprocess.py` 파일을 수정할 때마다 커널을 `Restart`해야함\n\n* 이러한 번거러움을 줄이기 위해 위의 코드를 실행한다.\n\n## step5. 어간 추출\n\n## step6. 결과 확인\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":false,"code-overflow":"scroll","code-link":false,"code-line-numbers":true,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../../../styles.css"],"toc":true,"output-file":"01. 자연어 전처리.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.4.551","editor":"visual","code-copy":true,"title":"01. 자연어 전처리","author":"GC","date":"05/14/24"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}