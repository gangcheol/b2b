{
 "cells": [
  {
   "cell_type": "raw",
   "id": "164081b7-ab1e-4702-a734-cda18f1d2e4d",
   "metadata": {},
   "source": [
    "---\n",
    "title : \"02. 자연어 전처리 (2)\"\n",
    "author : \"GC\"\n",
    "date : \"05/15/24\"\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa14b10b-75f9-4d09-8309-fcc6485dc2f5",
   "metadata": {},
   "source": [
    "# 01. 문장 토큰화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04b51b2-78d1-427d-ac32-9fb6ef2b59ea",
   "metadata": {},
   "source": [
    "문장 단위 토큰화를 수행하는 경우"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46dc8217-8f14-48e2-b6c3-bdaa3a2bc1df",
   "metadata": {},
   "source": [
    "`1` 품사 태깅\n",
    "\n",
    "* 어떠한 단어의 품사는 그 단어 자체의 의미와 함께 문장 안에서 사용된 위치에 따라 달라질 수 있음\n",
    "\n",
    "* 이럴 경우, 문장 간의 구분이 된 상태에서 단어의 품사를 정해야 하기 때문에 문장 단위로 먼저 토큰화를 수행해야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff6d7ed-f252-4a90-ac54-9875907ca02b",
   "metadata": {},
   "source": [
    "## nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f2417f-1b96-4a5a-9e54-fbb2ddc77810",
   "metadata": {},
   "source": [
    "`1` 필요한 패키지와 함수 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "ae05c77b-45a6-4496-b722-f4d84abf0745",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\rkdcj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4ceda8-5b59-4867-92ea-dd60670d50b3",
   "metadata": {},
   "source": [
    "`2` 문장 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "27ec33e1-f2b6-4253-a2f7-fba3a5d17c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"My email address is 'abcde@codeit.com'. Send it to Mr.Kim.\"\n",
    "\n",
    "tokenized_sents = sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8e421a-f566-45d9-8777-9aa4db11bde5",
   "metadata": {},
   "source": [
    "* 문장이 끝나는 지점의 마침표를 기준으로 토큰화가 수행됨."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "db527da1-b54e-4ff7-9307-cfda27290d93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"My email address is 'abcde@codeit.com'.\", 'Send it to Mr.Kim.']"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_sents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7e31c2-0f6e-4d98-8f87-4b14d9928ab8",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f818690-e544-4234-8028-65a3f686d40a",
   "metadata": {},
   "source": [
    "# 02. 품사 태깅"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c596a20-2427-4a40-9849-8bd34cff1c01",
   "metadata": {},
   "source": [
    "문장안에 해당 단어가 어떠한 품사로 사용되었는지 태깅을 표시하는 작업"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10673d6-e670-4b64-9d7d-f3c0a2a9249f",
   "metadata": {},
   "source": [
    "`1` 패키지, 함수 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "aa6a3a1a-5edd-462a-afa9-08a566a607a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3624d9c7-dedf-4e3c-ac33-e345b1b0367b",
   "metadata": {},
   "source": [
    "`2` 문장 기준 토큰화 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "d9ff0eda-851f-4bc3-8f46-22dc0f76197b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Watching Time Chasers, it obvious that it was made by a bunch of friends. Maybe they were sitting around one day in film school and said, \\\"Hey, let\\'s pool our money together and make a really bad movie!\\\" Or something like that.\"\n",
    "\n",
    "tokenized_sents = sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "0cbcb934-dde8-4c0f-8971-7d7e9637ca3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Watching Time Chasers, it obvious that it was made by a bunch of friends.',\n",
       " 'Maybe they were sitting around one day in film school and said, \"Hey, let\\'s pool our money together and make a really bad movie!\"',\n",
       " 'Or something like that.']"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_sents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b4fdb2-2802-490f-8410-a0e6cd7e6b77",
   "metadata": {},
   "source": [
    "`3` 토큰화 된 문장을 순회하며 순차적으로 단어 토큰화와 품사 태깅 작업을 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "bcb06196-7edf-4aff-a308-d475fa766653",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tagged_words = []\n",
    "for sentence in tokenized_sents:\n",
    "    # 단어 토큰화\n",
    "    tokenized_words = word_tokenize(sentence)\n",
    "\n",
    "    # 품사 태깅\n",
    "    pos_tagged = pos_tag(tokenized_words)\n",
    "    pos_tagged_words.extend(pos_tagged)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2603f66-f27f-4b8e-9e88-4680b310cf08",
   "metadata": {},
   "source": [
    "* 다음과 같이 품사가 태깅된 것을 확인할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "c874b546-0338-4e69-aa54-7915b41dd0b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Watching', 'VBG'), ('Time', 'NNP'), ('Chasers', 'NNPS'), (',', ',')]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tagged_words[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea4c8d4-968d-4abb-b6a1-77c9033b032f",
   "metadata": {},
   "source": [
    "`4` 품사 태깅 함수 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "38b13367-a248-4031-a97f-a87742cefea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "# 품사 태깅 함수\n",
    "def pos_tagger(tokenized_sents):\n",
    "    pos_tagged_words = []\n",
    "\n",
    "    for sentence in tokenized_sents:\n",
    "        # 단어 토큰화\n",
    "        tokenized_words = word_tokenize(sentence)\n",
    "    \n",
    "        # 품사 태깅\n",
    "        pos_tagged = pos_tag(tokenized_words)\n",
    "        pos_tagged_words.extend(pos_tagged)\n",
    "    \n",
    "    return pos_tagged_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5180a10a-c138-4a8f-a85f-f9e62a6c1f7c",
   "metadata": {},
   "source": [
    "## extra"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b9e906-e223-4b57-b6f8-ff3694c5a3f1",
   "metadata": {},
   "source": [
    "`NLTK`의 `pos_tag()`함수는 `Penn Treebank POS Tags`를 기준으로 품사를 태깅한다.\n",
    "\n",
    "* 각 품사의 대웅하는 태그는 다음과 같다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05999d22-558c-410a-9b0d-40f3673dbe3e",
   "metadata": {},
   "source": [
    "<img src=\"https://blog.kakaocdn.net/dn/ujEIL/btqSjBtuJmr/LVR89K812qTO4AopMNVWn0/img.png\" srcset=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&amp;fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FujEIL%2FbtqSjBtuJmr%2FLVR89K812qTO4AopMNVWn0%2Fimg.png\" data-filename=\"제목 없음.png\" data-origin-width=\"500\" data-origin-height=\"793\" data-ke-mobilestyle=\"widthContent\" onerror=\"this.onerror=null; this.src='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png'; this.srcset='//t1.daumcdn.net/tistory_admin/static/images/no-image-v1.png';\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c6c360-a6a4-429f-b647-a8d560a7fc43",
   "metadata": {},
   "source": [
    "# 03. 표제어 추출"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503bebe0-4b91-4c06-a91c-a4d2a79c978e",
   "metadata": {},
   "source": [
    "`-` 표제어(Lemma) : 단어의 사전적 어원 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac363ec-87b7-4864-bd13-c583461ccffe",
   "metadata": {},
   "source": [
    "> ex1. am, are, is는 서로 다른 단어이지만 표제어는 동일하게 `be`이다.\n",
    "> * 영어 코퍼스에는 특히 많은 be 동사들을 모두 표제어로 통합시키는 작업이 필요할 수 있음 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4cefe6-4008-47c5-bd31-53b521c88125",
   "metadata": {},
   "source": [
    "`1` 단어 토큰화 & 품사 태깅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "a5567000-8a28-4aee-9366-940bd9310fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'You are the happiest person.'\n",
    "tokenized_words = word_tokenize(text)\n",
    "\n",
    "# 품사 태그\n",
    "tagged_words = pos_tag(tokenized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "5e292f95-bbc8-425f-aeed-7ba1bd0d52ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('You', 'PRP'),\n",
       " ('are', 'VBP'),\n",
       " ('the', 'DT'),\n",
       " ('happiest', 'JJS'),\n",
       " ('person', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59c2a4f-452e-49b6-b502-4d8b7c039d6b",
   "metadata": {},
   "source": [
    "`2` WordNet pos Tag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307ed9a3-1e92-45f4-bed0-3f246369453a",
   "metadata": {},
   "source": [
    "* 앞서 nltk의 품사 태그는 `Penn Treebank POS Tag`를 사용한다고 했음.\n",
    "\n",
    "* 그러나 표제어 추출에 사용되는 함수는 `WordNet Pos Tag`를 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414c27c3-a6bf-494c-8652-18a567eb0fd2",
   "metadata": {},
   "source": [
    "|품사 태그|품사|\n",
    "|:---|:---|\n",
    "|`n` (wn.NOUN)|명사|\n",
    "|`a` (wn.ADJ)|형용사|\n",
    "|`r` (wn.ADV)|부사|\n",
    "|`r` (wn.VERB)|동사|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9892f3eb-b232-4bfe-a694-312266f5ba65",
   "metadata": {},
   "source": [
    "* POS Tag를 살펴보면 `NN, NNp, NNPS`처럼 N으로 시작하는 태그는 모두 명사를 의미하는 것 같음\n",
    "\n",
    "* 또한, `JJ, JJR, JJS`처럼 `J`로 시작하는 태그는 모두 형용사(Adjective)를 의미한다.\n",
    "\n",
    "* 이제 이 규칙성을 이용해서 `pos tag`를 `wordnet tag`로 바꿔주자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "d7965f6b-726b-4fc5-9f66-093d23759841",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "def penn_to_wn(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wn.ADJ\n",
    "    elif tag.startswith('N'):\n",
    "        return wn.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wn.ADV\n",
    "    elif tag.startswith('V'):\n",
    "        return wn.VERB\n",
    "    else:\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a8d7b6-f0ea-4530-a4fc-a00c758ef068",
   "metadata": {},
   "source": [
    "`3` 표제어 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "0c0d1cbb-3969-4793-b228-02a5155a4bb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\rkdcj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\rkdcj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "3a74091f-cd88-4320-8b4b-090c8edbaf22",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_words = []\n",
    "\n",
    "for word, tag in tagged_words:\n",
    "    # WordNet Pos Tag로 변환\n",
    "    wn_tag = penn_to_wn(tag)\n",
    "\n",
    "    # 품사를 기준으로 표제어 추출\n",
    "    if wn_tag in (wn.NOUN, wn.ADJ, wn.ADV, wn.VERB):\n",
    "        lemmatized_words.append(lemmatizer.lemmatize(word, wn_tag))\n",
    "    else:\n",
    "        lemmatized_words.append(word) ## wordnet post tag에 포함되지 않는 품사들은 변환하지 않고 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "202cf653-5c7d-4c6c-b239-15d4b6fa71a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "표제어 추출 전 : ['You', 'are', 'the', 'happiest', 'person', '.']\n",
      "표제어 추출 후 : ['You', 'be', 'the', 'happy', 'person', '.']\n"
     ]
    }
   ],
   "source": [
    "# 표제어 추출 확인\n",
    "print('표제어 추출 전 :', tokenized_words)\n",
    "print('표제어 추출 후 :', lemmatized_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0fb6d8b-6c12-4da8-9919-fec4576cf3d5",
   "metadata": {},
   "source": [
    "`4` 함수로 작성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "c506f9d3-aebf-40fb-94bb-7b236f2d497d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_lemmatizer(pos_tagged_words):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_words = []\n",
    "\n",
    "    for word, tag in pos_tagged_words:\n",
    "        wn_tag = penn_to_wn(tag)\n",
    "\n",
    "        if wn_tag in (wn.NOUN, wn.ADJ, wn.ADV, wn.VERB):\n",
    "            lemmatized_words.append(lemmatizer.lemmatize(word, wn_tag))\n",
    "        else:\n",
    "            lemmatized_words.append(word)\n",
    "\n",
    "    return lemmatized_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfcaa182-f8a0-463f-acbe-8b334719f27a",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76e7eb3-d016-4e76-a7d3-3bc749e1f642",
   "metadata": {},
   "source": [
    "# 04. 실습 1. imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "5c777343-b788-4c6b-a378-8ab3bae0a3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action = \"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "f7da8835-b1b3-4f12-af6c-47724e67c438",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('imdb.tsv', delimiter = \"\\\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2904d9e-7fe7-45cd-b56f-bc121bd3bfd7",
   "metadata": {},
   "source": [
    "`1` 대소문자 통합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "de34d79c-83c9-41f6-8bed-83432a8aa343",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review'] = df['review'].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7d36b6-c971-48a6-9f8a-83a8b4026e59",
   "metadata": {},
   "source": [
    "`2` 문장 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "924a821d-3743-4769-b090-9e117c903499",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sent_tokens'] = df['review'].apply(sent_tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e317eb1-c852-4cbd-8001-0d3f053b03b7",
   "metadata": {},
   "source": [
    "`3` 품사 태깅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "ba3e26e4-5e66-4d65-9a5b-9771d0cec215",
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocess import pos_tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "a52f95b5-1c20-4a9d-81eb-87dc36f68e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['pos_tagged_tokens'] = df['sent_tokens'].apply(pos_tagger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "62f7b7e0-3f4d-4521-a8a6-fb37f1caafeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('``', '``'), ('watching', 'JJ'), ('time', 'NN'), ('chasers', 'NNS'), (',', ',')]\n"
     ]
    }
   ],
   "source": [
    "print(df['pos_tagged_tokens'][0][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52891a62-6a7f-4f90-bf47-fc1ff88d0bce",
   "metadata": {},
   "source": [
    "`4` 표제어 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "0e24f153-50a2-4c05-9a07-6e30688bd6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocess import words_lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "80241b58-eda6-40f6-931a-a3f08a125383",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['lemmatized_tokens'] = df['pos_tagged_tokens'].apply(words_lemmatizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "e9f56019-362f-483c-a8f7-a340c9bbe5b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['``', 'watching', 'time', 'chaser', ',']\n"
     ]
    }
   ],
   "source": [
    "print(df['lemmatized_tokens'][0][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fb0cf2-92ee-4acb-a334-a537746e91cc",
   "metadata": {},
   "source": [
    "`5` 추가 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73222468-7882-4f62-a69a-396c9462ae4d",
   "metadata": {},
   "source": [
    "* 빈도 1 이하, 단어 길이 2 이하, 불용어 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "80a580fc-9dbb-42ea-9aa9-1e30ab358775",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rkdcj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cleaned_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[make, one, film, say, make, really, bad, movi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[film, film]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[new, york, joan, barnard, elvire, audrey, bar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[film, film, jump, send, n't, jump, radio, n't...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[site, movie, bad, even, movie, movie, make, m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[ehle, northam, wonderful, wonderful, ehle, no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[role, movie, n't, author, book, funny, author...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[plane, ceo, search, rescue, mission, call, ce...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[gritty, movie, movie, keep, sci-fi, good, kee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[girl, girl]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      cleaned_tokens\n",
       "0  [make, one, film, say, make, really, bad, movi...\n",
       "1                                       [film, film]\n",
       "2  [new, york, joan, barnard, elvire, audrey, bar...\n",
       "3  [film, film, jump, send, n't, jump, radio, n't...\n",
       "4  [site, movie, bad, even, movie, movie, make, m...\n",
       "5  [ehle, northam, wonderful, wonderful, ehle, no...\n",
       "6  [role, movie, n't, author, book, funny, author...\n",
       "7  [plane, ceo, search, rescue, mission, call, ce...\n",
       "8  [gritty, movie, movie, keep, sci-fi, good, kee...\n",
       "9                                       [girl, girl]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from preprocess import *\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "stopwords_set = set(stopwords.words('english'))\n",
    "\n",
    "df['cleaned_tokens'] = df['lemmatized_tokens'].apply(lambda x: clean_by_freq(x, 1))\n",
    "df['cleaned_tokens'] = df['cleaned_tokens'].apply(lambda x: clean_by_len(x, 2))\n",
    "df['cleaned_tokens'] = df['cleaned_tokens'].apply(lambda x: clean_by_stopwords(x, stopwords_set))\n",
    "\n",
    "df[['cleaned_tokens']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac694fa-7584-414a-8010-c91dc02df8c2",
   "metadata": {},
   "source": [
    "`6` 전처리 후 코퍼스로 통합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "f033a3f5-4dd9-47dc-bbfd-7df8fa41af92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine(sentence):\n",
    "    return ' '.join(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "27167b63-404d-4686-9fcf-3a380c0d5683",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>combined_corpus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>make one film say make really bad movie like s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>film film</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>new york joan barnard elvire audrey barnard jo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>film film jump send n't jump radio n't send re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>site movie bad even movie movie make movie spe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ehle northam wonderful wonderful ehle northam ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>role movie n't author book funny author author...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>plane ceo search rescue mission call ceo harla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>gritty movie movie keep sci-fi good keep suspe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>girl girl</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     combined_corpus\n",
       "0  make one film say make really bad movie like s...\n",
       "1                                          film film\n",
       "2  new york joan barnard elvire audrey barnard jo...\n",
       "3  film film jump send n't jump radio n't send re...\n",
       "4  site movie bad even movie movie make movie spe...\n",
       "5  ehle northam wonderful wonderful ehle northam ...\n",
       "6  role movie n't author book funny author author...\n",
       "7  plane ceo search rescue mission call ceo harla...\n",
       "8  gritty movie movie keep sci-fi good keep suspe...\n",
       "9                                          girl girl"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['combined_corpus'] = df['cleaned_tokens'].apply(combine)\n",
    "\n",
    "df[['combined_corpus']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8eb1dbf-95bc-4448-bff3-174c9dc43006",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd988c7-e44f-44e9-92d3-5c060108471d",
   "metadata": {},
   "source": [
    "# 05. 정수 인코딩"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5cd2206-26a0-461a-9956-f1ce9f5d8754",
   "metadata": {},
   "source": [
    "`-` 토큰화된 각 단어에 특정 정수를 맵핑하여 고유 번호로 사용하는 방법\n",
    "\n",
    "* 가장 일반적인 방법 : 단어의 등장 빈도를 기준으로 정렬한 다음 인덱스를 부여하는 방식"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9204d32-9d4d-4c90-b6e3-b7683be2a073",
   "metadata": {},
   "source": [
    "`1` 하나의 로우 정수 인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "9e9e579c-8d62-4a8d-9b0a-886b961c9438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('movie', 10), ('jim', 7), ('stand-up', 3), ('day', 3), ('really', 3), ('terrible', 3), ('site', 2), ('bad', 2), ('even', 2), ('make', 2), ('special', 2), ('describe', 2), ('like', 2), ('actor', 2), ('love', 2), ('stand', 2), ('comedian', 2)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "tokens = df['cleaned_tokens'][4]\n",
    "\n",
    "vocab = Counter(tokens)\n",
    "vocab = vocab.most_common()\n",
    "\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e022851-e8a9-441f-83c4-fc9def81ef1f",
   "metadata": {},
   "source": [
    "`2` 단어와 코퍼스 안에서 해당 단어의 등장 빈도가 튜플 형태로 매칭되어 리스트 형태로 저장되어있음\n",
    "\n",
    "* 해당 결과를 가지고 각 단어에 인덱스를 부여"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "dda6dd60-4915-40c4-af71-6a6a310a3bc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'movie': 1, 'jim': 2, 'stand-up': 3, 'day': 4, 'really': 5, 'terrible': 6, 'site': 7, 'bad': 8, 'even': 9, 'make': 10, 'special': 11, 'describe': 12, 'like': 13, 'actor': 14, 'love': 15, 'stand': 16, 'comedian': 17}\n"
     ]
    }
   ],
   "source": [
    "word_to_idx = {}\n",
    "i = 0\n",
    "\n",
    "for (word, frequency) in vocab:\n",
    "    i = i + 1\n",
    "    word_to_idx[word] = i\n",
    "\n",
    "print(word_to_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8077510f-7a1a-4847-9d7b-14254b654957",
   "metadata": {},
   "source": [
    "`3` 토큰들을 부여된 인덱스로 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "592dd77f-a142-43aa-a3e5-16807a1fc54d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7, 1, 8, 9, 1, 1, 10, 1, 11, 12, 1, 1, 12, 1, 2, 10, 3, 4, 3, 2, 13, 2, 14, 15, 16, 4, 17, 11, 2, 4, 9, 7, 15, 2, 3, 2, 14, 1, 16, 17, 2, 13, 5, 6, 5, 6, 1, 6, 5, 8, 1]\n"
     ]
    }
   ],
   "source": [
    "encoded_idx = []\n",
    "\n",
    "for token in tokens:\n",
    "    idx = word_to_idx[token]\n",
    "    encoded_idx.append(idx)\n",
    "\n",
    "print(encoded_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758d44d6-eb91-439a-a6bc-cdb9c70a5307",
   "metadata": {},
   "source": [
    "`4` 전체 데이터프레임에 적용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc3bab3-2641-4abc-a93d-38d69c52e425",
   "metadata": {},
   "source": [
    "* `sum()`함수를 사용 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "73e98997-876d-4bf5-9dcd-09848af61181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['make', 'one', 'film', 'say', 'make']\n"
     ]
    }
   ],
   "source": [
    "tokens = sum(df[\"cleaned_tokens\"], [])\n",
    "print(tokens[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957e7d55-d455-4683-8e58-d0a2742592e9",
   "metadata": {},
   "source": [
    "* 합쳐진 토큰 리스트로 빈도를 계산하고 많이 등장한 순으로 정렬하여 인덱스를 부여"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "7a4f0428-7c15-451f-a0ea-ca23358308b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_idx = {}\n",
    "i = 0\n",
    "tokens = sum(df['cleaned_tokens'], [])\n",
    "\n",
    "vocab = Counter(tokens)\n",
    "vocab = vocab.most_common()\n",
    "\n",
    "for (word, frequency) in vocab:\n",
    "    i = i + 1\n",
    "    word_to_idx[word] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "cdb7d6b1-3ea6-4220-9909-dca75030c3d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'movie': 1, 'film': 2, \"n't\": 3, 'scene': 4, 'bad': 5, 'time': 6, 'reason': 7, 'make': 8, 'jim': 9, 'good': 10, 'one': 11, 'like': 12, 'could': 13, \"'re\": 14, 'quastel': 15, 'really': 16, 'even': 17, 'monster': 18, 'joan': 19, 'love': 20, 'author': 21, 'try': 22, 'dialogue': 23, 'idea': 24, 'italy': 25, 'colleague': 26, 'maggot': 27, 'end': 28, 'watch': 29, 'jump': 30, 'radio': 31, 'stand-up': 32, 'day': 33, 'terrible': 34, 'ehle': 35, 'northam': 36, 'search': 37, 'rescue': 38, 'call': 39, 'knowles': 40, 'henriksen': 41, 'easily': 42, 'see': 43, 'appear': 44, 'get': 45, 'character': 46, 'think': 47, 'use': 48, 'whether': 49, 'need': 50, 'though': 51, 'sci-fi': 52, 'look': 53, 'say': 54, 'new': 55, 'york': 56, 'barnard': 57, 'elvire': 58, 'audrey': 59, 'john': 60, 'saxon': 61, 'etruscan': 62, 'tomb': 63, 'drug': 64, 'story': 65, 'romantic': 66, 'waste': 67, 'etrusco': 68, 'send': 69, 'reporter': 70, 'fear': 71, 'site': 72, 'special': 73, 'describe': 74, 'actor': 75, 'stand': 76, 'comedian': 77, 'wonderful': 78, 'lust': 79, 'role': 80, 'book': 81, 'funny': 82, 'queen': 83, 'corn': 84, 'plane': 85, 'ceo': 86, 'mission': 87, 'harlan': 88, 'lance': 89, 'put': 90, 'wood': 91, 'two': 92, 'decent': 93, 'sasquatch': 94, 'edit': 95, 'want': 96, 'potential': 97, 'material': 98, 'relate': 99, 'crib': 100, 'exposition': 101, 'far': 102, 'costume': 103, 'would': 104, 'stereotype': 105, 'well': 106, 'effective': 107, 'occur': 108, 'line': 109, 'back': 110, 'irrelevant': 111, 'comment': 112, 'cut': 113, 'random': 114, 'show': 115, 'important': 116, 'either': 117, 'never': 118, 'leave': 119, 'gritty': 120, 'keep': 121, 'suspense': 122, 'girl': 123}\n"
     ]
    }
   ],
   "source": [
    "print(word_to_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c91fad-e725-4901-bb88-28b6fc004474",
   "metadata": {},
   "source": [
    "* 함수를 작성하여 토큰들에 정수 인코딩 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "263c9db3-d535-4daf-8321-58d610afc93f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>integer_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[8, 11, 2, 54, 8, 16, 5, 1, 12, 54, 8, 16, 5, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[2, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[55, 56, 19, 57, 58, 59, 57, 60, 61, 25, 62, 6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[2, 2, 30, 69, 3, 30, 31, 3, 69, 70, 71, 30, 7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[72, 1, 5, 17, 1, 1, 8, 1, 73, 74, 1, 1, 74, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[35, 36, 78, 78, 35, 36, 79, 79, 35, 36]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[80, 1, 3, 21, 81, 82, 21, 21, 80, 3, 82, 83, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[85, 86, 37, 38, 87, 39, 86, 88, 40, 89, 41, 9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[120, 1, 1, 121, 52, 10, 121, 122, 53, 1, 52, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[123, 123]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     integer_encoded\n",
       "0  [8, 11, 2, 54, 8, 16, 5, 1, 12, 54, 8, 16, 5, ...\n",
       "1                                             [2, 2]\n",
       "2  [55, 56, 19, 57, 58, 59, 57, 60, 61, 25, 62, 6...\n",
       "3  [2, 2, 30, 69, 3, 30, 31, 3, 69, 70, 71, 30, 7...\n",
       "4  [72, 1, 5, 17, 1, 1, 8, 1, 73, 74, 1, 1, 74, 1...\n",
       "5           [35, 36, 78, 78, 35, 36, 79, 79, 35, 36]\n",
       "6  [80, 1, 3, 21, 81, 82, 21, 21, 80, 3, 82, 83, ...\n",
       "7  [85, 86, 37, 38, 87, 39, 86, 88, 40, 89, 41, 9...\n",
       "8  [120, 1, 1, 121, 52, 10, 121, 122, 53, 1, 52, ...\n",
       "9                                         [123, 123]"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def idx_encoder(tokens, word_to_idx):\n",
    "    encoded_idx = []\n",
    "    \n",
    "    for token in tokens:\n",
    "        idx = word_to_idx[token]\n",
    "        encoded_idx.append(idx)\n",
    "        \n",
    "    return encoded_idx\n",
    "\n",
    "df['integer_encoded'] = df['cleaned_tokens'].apply(lambda x: idx_encoder(x, word_to_idx))\n",
    "df[['integer_encoded']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547f7e66-9714-40fb-bda4-7b1707aa240c",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8225694e-4ae0-4f3c-b2ce-e2a376745cf5",
   "metadata": {},
   "source": [
    "# 06. 패딩"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8473ba-96fb-4497-a153-06abf773c76c",
   "metadata": {},
   "source": [
    "`-` 정수 인코딩 수행 후, 다수의 문장들을 서로 길이를 맞춰서 행렬 형태로 만드는 작업\n",
    "\n",
    "* 이점 : 좀 더 복잡한 연산을 쉽게 처리하는 것이 가능해짐"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8ddcd9-b9d1-4210-a628-5323c2491bf0",
   "metadata": {},
   "source": [
    "## 제로 패딩"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a783d8c8-5d1e-4d8d-9d6d-b6b68c3f7557",
   "metadata": {},
   "source": [
    "`-` 가장 긴 문장의 길이를 구하여 해당 값을 기준으로 문장 길이를 맞추는 방법\n",
    "\n",
    "* 비어있는 값은 0으로 채운다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24bb1b61-f5cf-4144-8e78-27838df4aba8",
   "metadata": {},
   "source": [
    "`1` 기존 정수 인코딩한 데이터에서 최대 토큰이 몇 개인지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "1cc6b65c-395b-4936-8cbb-e65bbb37f6f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "토큰의 최대 개수: 200\n"
     ]
    }
   ],
   "source": [
    "max_len = max(len(item) for item in df['integer_encoded'])\n",
    "\n",
    "print('토큰의 최대 개수:', max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d3d79d-9dda-4723-957c-bf46456f0751",
   "metadata": {},
   "source": [
    "`2` 해당 값을 기준으로 다른 문장(코퍼스)들의 길이가 `200`이 되도록 `0`을 채워 넣음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "d0825a00-3640-484a-9ee9-f8a2a44aa31c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>integer_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[8, 11, 2, 54, 8, 16, 5, 1, 12, 54, 8, 16, 5, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[55, 56, 19, 57, 58, 59, 57, 60, 61, 25, 62, 6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[2, 2, 30, 69, 3, 30, 31, 3, 69, 70, 71, 30, 7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[72, 1, 5, 17, 1, 1, 8, 1, 73, 74, 1, 1, 74, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[35, 36, 78, 78, 35, 36, 79, 79, 35, 36, 0, 0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[80, 1, 3, 21, 81, 82, 21, 21, 80, 3, 82, 83, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[85, 86, 37, 38, 87, 39, 86, 88, 40, 89, 41, 9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[120, 1, 1, 121, 52, 10, 121, 122, 53, 1, 52, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[123, 123, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     integer_encoded\n",
       "0  [8, 11, 2, 54, 8, 16, 5, 1, 12, 54, 8, 16, 5, ...\n",
       "1  [2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "2  [55, 56, 19, 57, 58, 59, 57, 60, 61, 25, 62, 6...\n",
       "3  [2, 2, 30, 69, 3, 30, 31, 3, 69, 70, 71, 30, 7...\n",
       "4  [72, 1, 5, 17, 1, 1, 8, 1, 73, 74, 1, 1, 74, 1...\n",
       "5  [35, 36, 78, 78, 35, 36, 79, 79, 35, 36, 0, 0,...\n",
       "6  [80, 1, 3, 21, 81, 82, 21, 21, 80, 3, 82, 83, ...\n",
       "7  [85, 86, 37, 38, 87, 39, 86, 88, 40, 89, 41, 9...\n",
       "8  [120, 1, 1, 121, 52, 10, 121, 122, 53, 1, 52, ...\n",
       "9  [123, 123, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,..."
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for tokens in df['integer_encoded']:\n",
    "    while len(tokens) < max_len:\n",
    "        tokens.append(0)\n",
    "\n",
    "df[['integer_encoded']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3cb181-afea-4302-b1f8-aeb0c780a397",
   "metadata": {},
   "source": [
    "`-` 결과 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "6a6a55aa-2f21-410e-a108-c6659cf3e5d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[200, 200, 200, 200, 200, 200, 200, 200, 200, 200]"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(i) for i in df['integer_encoded']]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
