{
 "cells": [
  {
   "cell_type": "raw",
   "id": "fecec3d2-7fcf-4dfc-9cc1-9d644e1973bd",
   "metadata": {},
   "source": [
    "---\n",
    "title : \"01. 자연어 전처리 (1)\"\n",
    "author : \"GC\"\n",
    "date : \"05/14/24\"\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c792f8f7-fffc-4197-bb20-762ddcce1f3f",
   "metadata": {},
   "source": [
    "# 01. 자연어 전처리란?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b758625-4ebe-4d3e-8220-72139c61a34f",
   "metadata": {},
   "source": [
    "## ex1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba83549e-acd3-4e25-9f93-fea6683a9d68",
   "metadata": {},
   "source": [
    "`1` 맞춤법과 띄어쓰기 교정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de609be8-f339-4cc0-802f-f5e2d4fffc96",
   "metadata": {},
   "source": [
    "> ex 1.  Oh, Hi helo. Nice to meetyou.\n",
    "> \n",
    "> * step 1. 띄어쓰기 수정 : Oh, Hi hello. Nice to `meet you`.\n",
    ">\n",
    "> * step 2. 문장의 의미를 표현하는데 크게 기여하지 않는 단어 삭제\n",
    ">   * ~~Oh~~, Hi hello. Nice to `meet you`.\n",
    ">  \n",
    "> * step 3. 중복된 의미 단어 제거\n",
    ">   * ~~Oh~~, Hi ~~hello~~. Nice to `meet you`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea3971c-8c4a-41d1-957c-6f386f06df53",
   "metadata": {},
   "source": [
    "`2` 컴퓨터가 자연어를 잘 이해할 수 있도록 각 단어에 숫자 인덱스를 부여\n",
    "\n",
    "* {'Hi':0, 'Nice':1, 'to':2, 'meet':3, 'you':4}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700610c4-d8da-4f69-9156-bb89ea978208",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f38e6b-4671-4042-abca-971ad2c34102",
   "metadata": {},
   "source": [
    "# 자연어 전처리 과정들"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab49b893-bc76-488b-89a6-b2c7398b78af",
   "metadata": {},
   "source": [
    "아래의 순서가 일반적이나 정해진 표준은 아님. 분석의 목적과 활용할 자연어 데이터의 특성에 따라 적용해야 하는 전처리 단계가 다 다르고, 각 단계를 적용하는 순서에도 차이가 발생할 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce748f50-c145-4b36-ac84-4fc8fbb03158",
   "metadata": {},
   "source": [
    "`1` 토큰화: 자연어 데이터를 분석을 위한 작은 단위(토큰)로 분리\n",
    "\n",
    "`2` 정제: 분석에 큰 의미가 없는 데이터들을 제거\n",
    "\n",
    "`3` 정규화: 표현 방법이 다르지만 의미가 같은 단어들을 통합\n",
    "\n",
    "`4` 정수 인코딩: 컴퓨터가 이해하기 쉽도록 자연어 데이터에 정수 인덱스를 부여"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2389d2f5-f82b-4aac-a73b-b24f774199da",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba951cf-59e3-4d40-947a-053e66acb740",
   "metadata": {},
   "source": [
    "# 02. 단어 토큰화(Word Tokenization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb4f1ca-a2bd-49d3-b5b0-09e5a6fb2171",
   "metadata": {},
   "source": [
    "`-` 코퍼스(Corpus) : 분석에 활용하기 위한 자연어 데이터(말뭉치)\n",
    "\n",
    "`-` 이 코퍼스를 분석에 활용하려면 먼저 의미있는 작은 단위로 나눠야함\n",
    "\n",
    "* `의미있는 작은 단위` $\\to$ **토큰(Token)**\n",
    "\n",
    "* 위 같은 과정을 토큰화(Tokenization)이라고 한다.\n",
    "\n",
    "`-` 토큰화에는 단어 토큰화와 문장 토큰화가 있으며, 분석 목적에 맞게 둘 중 필요한 토큰화 방식을 적절하게 사용해야함.\n",
    "\n",
    "* 이번 챕터에서는 단어 토큰화에 대해 학습할 것임"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d169d5a-f63a-4e79-8249-33c538543fdd",
   "metadata": {},
   "source": [
    "## NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241f293c-f504-41d6-b99d-e041ee5d4413",
   "metadata": {},
   "source": [
    "`1` 터미널에서 아래와 같은 커맨드르 실행"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed95fadf-be7f-4cef-92dc-46b1fdbe222a",
   "metadata": {},
   "source": [
    "```python\n",
    "conda install nltk\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1ccd2d-0692-4b1e-adfc-dcdb3c3a001f",
   "metadata": {},
   "source": [
    "`2` 패키지에서 함수 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c054a4a4-01ba-4c8a-bd8c-a2d90864d202",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35954f6-0527-491e-ab69-230d3162a74a",
   "metadata": {},
   "source": [
    "`3` `nltk`에서 제공하는 토큰화 모듈인 `punkt`를 다운로드\n",
    "\n",
    "* `punkt` : 마침표나 약어(Mr., Dr.)와 같은 특별한 언어적 특성을 고려하여 토큰화를 할 수 있게 해주는 모듈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50f75bb0-80a3-41a5-a310-bbe08a408160",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\rkdcj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8218f18e-7d79-4b57-b0ea-6bedd55864ec",
   "metadata": {},
   "source": [
    "## 단어 토큰화 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0acb9ec7-0de3-4aeb-a79e-fd52cfbadb1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Although', 'it', \"'s\", 'not', 'a', 'happily-ever-after', 'ending', ',', 'it', 'is', 'very', 'realistic', '.']\n"
     ]
    }
   ],
   "source": [
    "text = \"Although it's not a happily-ever-after ending, it is very realistic.\"\n",
    "\n",
    "# 단어 토큰화\n",
    "tokenized_words = word_tokenize(text)\n",
    "\n",
    "print(tokenized_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ece622-de84-43f3-9587-62c93bf396e3",
   "metadata": {},
   "source": [
    "`-` 기본적으로 띄어쓰기, 어퍼스트로피('), 콤마(,)를 기준으로 토큰화를 수행하고 있으며 `하이픈(-)`은 토큰화의 기준으로 사용하지 않음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fef1345-0015-4968-af66-8abf2da6f93e",
   "metadata": {},
   "source": [
    "* 어떠한 기준을 가지고 단어 토큰화를 하는게 더 좋다고는할 수 없다. 분석에 활용하려는 코퍼스의 특성에 따라 적절한 토큰화 기준을 사용하면 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b3ec70-55d8-408e-acd2-ac9beef92a26",
   "metadata": {},
   "source": [
    "* [nltk 공식문서](https://www.nltk.org/api/nltk.tokenize.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82025f1a-157b-450b-bc4b-65391fe2858d",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36caac0c-49c5-4d1c-bba8-69a71dd5e4b3",
   "metadata": {},
   "source": [
    "# 03. 정제(Cleaning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416e28ad-73e0-43b1-970c-acddbc02532f",
   "metadata": {},
   "source": [
    "`-` 분석 목적에 적합하지 않은 단어들을 코퍼스에서 제거하는 과정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9389f29f-7a58-4db1-81c4-76425a17c239",
   "metadata": {},
   "source": [
    "## 등장빈도가 적은 단어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec1d5dca-87e8-4189-8a18-393ad645752b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from text import TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7accb232-9922-4cb7-ba89-dc3955465352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After reading the comments for this movie, I am not sure whether I should be angry, sad or sickened. Seeing comments typical of people who a)know absolutely nothing about the military or b)who base everything they think they know on movies like this or on CNN reports about Abu-Gharib makes me wonder about the state of intellectual stimulation in the world. At the time I type this the number of people in the US military: 1.4 million on Active Duty with another almost 900,000 in the Guard and Reserves for a total of roughly 2.3 million. The number of people indicted for abuses at at Abu-Gharib: Currently less than 20 That makes the total of people indicted .00083% of the total military. Even if you indict every single military member that ever stepped in to Abu-Gharib, you would not come close to making that a whole number.  The flaws in this movie would take YEARS to cover. I understand that it's supposed to be sarcastic, but in reality, the writer and director are trying to make commentary about the state of the military without an enemy to fight. In reality, the US military has been at its busiest when there are not conflicts going on. The military is the first called for disaster relief and humanitarian aid missions. When the tsunami hit Indonesia, devestating the region, the US military was the first on the scene. When the chaos of the situation overwhelmed the local governments, it was military leadership who looked at their people, the same people this movie mocks, and said make it happen. Within hours, food aid was reaching isolated villages. Within days, airfields were built, cargo aircraft started landing and a food distribution system was up and running. Hours and days, not weeks and months. Yes there are unscrupulous people in the US military. But then, there are in every walk of life, every occupation. But to see people on this website decide that 2.3 million men and women are all criminal, with nothing on their minds but thoughts of destruction or mayhem is an absolute disservice to the things that they do every day. One person on this website even went so far as to say that military members are in it for personal gain. Wow! Entry level personnel make just under $8.00 an hour assuming a 40 hour work week. Of course, many work much more than 40 hours a week and those in harm's way typically put in 16-18 hour days for months on end. That makes the pay well under minimum wage. So much for personal gain. I beg you, please make yourself familiar with the world around you. Go to a nearby base, get a visitor pass and meet some of the men and women you are so quick to disparage. You would be surprised. The military no longer accepts people in lieu of prison time. They require a minimum of a GED and prefer a high school diploma. The middle ranks are expected to get a minimum of undergraduate degrees and the upper ranks are encouraged to get advanced degrees.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "corpus = TEXT\n",
    "\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0350d0fe-e132-4eeb-a43c-906a1ec6d21f",
   "metadata": {},
   "source": [
    "### 빈도가 2이하인 단어들만 찾기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6b56029-5058-4f48-b32f-79cd3229bfc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88611db-e658-4fb8-bbf6-f3d7f479fa1e",
   "metadata": {},
   "source": [
    "`1` 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69bd24b3-c3d7-4a07-b6a5-9af1e7080a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_words = word_tokenize(corpus)\n",
    "#t_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73962e6-ae5c-4176-a3c0-b8b88e1b78d7",
   "metadata": {},
   "source": [
    "`2` 단어 빈도 수 카운트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f7c3bb6-6d03-42aa-9081-f633b0a34609",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Counter(t_words)\n",
    "#vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56dfd2e-856e-4f48-949c-a0bcc1345180",
   "metadata": {},
   "source": [
    "`3` 단어 빈도수가 2이하인 단어 리스트 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9ab4b12-7536-4346-9c91-ee3ae5b42352",
   "metadata": {},
   "outputs": [],
   "source": [
    "uncommon_words = [key for key, value in vocab.items() if value <=2]\n",
    "#uncommon_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0326c00-a992-4d7d-a065-82196b0d3f40",
   "metadata": {},
   "source": [
    "`4` 빈도수가 2이하인 단어들만 제거한 결과를 따로 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1976cf05-482c-4c6c-89dd-70801b017c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_words = [word for word in t_words if word not in uncommon_words]\n",
    "#c_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e0e72c-176e-4552-8a26-9794ed8cc3e5",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de29d4cc-4d06-490f-84c8-63d4c8830cc2",
   "metadata": {},
   "source": [
    "## 길이가 짧은 단어"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ccd97e-2855-4a0c-97f6-a78a12086369",
   "metadata": {},
   "source": [
    "영어 단어의 경우, 알파벳 하나 또는 두개로 구성된 단어는 코퍼스의 의미를 나타내는 데 중요하지 않을 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa31017-3949-473a-87f2-f25356abb067",
   "metadata": {},
   "source": [
    "그래서 이러한 단어들은 제거하는 것이 좋음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5232d42e-045e-4eb4-82f2-0989f045bf47",
   "metadata": {},
   "source": [
    "`1` 길이가 2이하인 단어 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b35ee599-1404-4559-b164-a41bdc18f15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_f_len = []\n",
    "\n",
    "for word in c_words :\n",
    "    if len(word) > 2:\n",
    "        c_f_len.append(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4eae13d-0cc7-4b57-9ca1-ee57afaf29af",
   "metadata": {},
   "source": [
    "`2` 정제 전과 후의 결과 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e1d80f56-6357-4ace-992a-47c91cfa5ba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정제 전: ['the', 'for', 'this', 'movie', ',', 'I', 'not', 'I', 'be', ',']\n",
      "정제 후: ['the', 'for', 'this', 'movie', 'not', 'people', 'who', 'about', 'the', 'military']\n"
     ]
    }
   ],
   "source": [
    "print('정제 전:', c_words[:10])\n",
    "print('정제 후:', c_f_len[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2130005-99de-46f3-bbb4-0b94a2646b89",
   "metadata": {},
   "source": [
    "## 함수 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b1d68b-8f7c-404a-9571-33837b4b5d44",
   "metadata": {},
   "source": [
    "`-` 위에서 만든 정제 기준을 언제든 활용할 수 있도록 함수로 작성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "896629de-154e-4512-8245-5ccdfd94eceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# 등장 빈도 기준 정제 함수\n",
    "def clean_by_freq(tokenized_words, cut_off_count):\n",
    "    # 파이썬의 Counter 모듈을 통해 단어의 빈도수 카운트하여 단어 집합 생성\n",
    "    vocab = Counter(tokenized_words)\n",
    "    \n",
    "    # 빈도수가 cut_off_count 이하인 단어 set 추출\n",
    "    uncommon_words = {key for key, value in vocab.items() if value <= cut_off_count}\n",
    "    \n",
    "    # uncommon_words에 포함되지 않는 단어 리스트 생성\n",
    "    cleaned_words = [word for word in tokenized_words if word not in uncommon_words]\n",
    "\n",
    "    return cleaned_words\n",
    "\n",
    "# 단어 길이 기준 정제 함수\n",
    "def clean_by_len(tokenized_words, cut_off_length):\n",
    "    # 길이가 cut_off_length 이하인 단어 제거\n",
    "    cleaned_by_freq_len = []\n",
    "    \n",
    "    for word in tokenized_words:\n",
    "        if len(word) > cut_off_length:\n",
    "            cleaned_by_freq_len.append(word)\n",
    "\n",
    "    return cleaned_by_freq_len"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91736f89-07bc-4d34-a0d9-5bf693773e00",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f132f4fd-9155-429f-99c6-b686011e612e",
   "metadata": {},
   "source": [
    "# 04. 불용어(stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28d05f4-12f2-4622-a3fc-3feb4834e8b4",
   "metadata": {},
   "source": [
    "정의 : 코퍼스에서 큰 의미가 없거나, 분석 목적에서 벗어나는 단어"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32882f8c-0c33-49a4-8e91-330f687f5d1b",
   "metadata": {},
   "source": [
    "## step1. 불용어 세트 준비"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a73acb-c8d4-432a-ad9c-45d9f0dc4128",
   "metadata": {},
   "source": [
    "`-` `nltk`에서는 기본 불용어 목록 179개를 제공한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033c9975-615b-4c18-ac3c-9511dbb27876",
   "metadata": {},
   "source": [
    "* 아래와 같은 방법으로 불용어 목록에 접근할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8d25fb-1643-4b88-91d1-968dd2a233b0",
   "metadata": {},
   "source": [
    "`1` 불용어 목록 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2d2c5a47-65a3-4262-b227-9f5c1db8dd38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rkdcj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24743a7b-af0a-4173-8a2a-212949d5addd",
   "metadata": {},
   "source": [
    "`2` 불용어들을 세트 자료형으로 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "959789a5-ece0-4003-bc3e-eb72006db115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "불용어 개수: 179\n"
     ]
    }
   ],
   "source": [
    "stopwords_set = set(stopwords.words(\"english\"))\n",
    "\n",
    "print(f\"불용어 개수: {len(stopwords_set)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9a7511-9a4d-4e20-b090-24f486b50a25",
   "metadata": {},
   "source": [
    "`3` 불용어 추가 및 삭제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3d284d4e-66e3-4782-84a8-98bafdb3bf05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "불용어 개수: 178\n"
     ]
    }
   ],
   "source": [
    "stopwords_set.add(\"hello\")\n",
    "stopwords_set.remove(\"the\")\n",
    "stopwords_set.remove(\"me\")\n",
    "\n",
    "print(f\"불용어 개수: {len(stopwords_set)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701e994d-7afb-42b0-bb3d-f6da2513e197",
   "metadata": {},
   "source": [
    "## step2. 불용어 제거하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3fdcd8ba-9cda-45b1-a0a9-72271a9ea066",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words_set = set(stopwords.words('english'))\n",
    "\n",
    "# 불용어 제거\n",
    "cleaned_words = []\n",
    "\n",
    "for word in c_f_len:\n",
    "    if word not in stop_words_set:\n",
    "        cleaned_words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "807f23dc-2a4c-49be-88f4-28899b7d1101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "불용어 제거 전: 169\n",
      "불용어 제거 후: 67\n"
     ]
    }
   ],
   "source": [
    "# 불용어 제거 결과 확인\n",
    "print('불용어 제거 전:', len(c_f_len))\n",
    "print('불용어 제거 후:', len(cleaned_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e601a4b-9608-4538-89f0-79f34283a065",
   "metadata": {},
   "source": [
    "## step3. 불용어 처리 함수 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "009a8e3d-0f8a-4946-b691-0ba824aceb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불용어 제거 함수\n",
    "def clean_by_stopwords(tokenized_words, stop_words_set):\n",
    "    cleaned_words = []\n",
    "    \n",
    "    for word in tokenized_words:\n",
    "        if word not in stop_words_set:\n",
    "            cleaned_words.append(word)\n",
    "            \n",
    "    return cleaned_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fdbc006-eb88-474d-91e1-05d641d52d2c",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f9d25b-29e2-4dc3-9542-e5da8a2ad5b2",
   "metadata": {},
   "source": [
    "# 05. 정규화(Normalization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee067c0c-5fcc-469f-835d-d1521fb2cce2",
   "metadata": {},
   "source": [
    "의미가 같은 단어들을 하나로 통일하는 과정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f648e37-5c86-4e4b-a3e4-a93f5b3c282e",
   "metadata": {},
   "source": [
    "## 방법 1. 대소문자 통합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4fb44694-5097-43cd-9860-9688526535f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what can i do for you? do your homework now.\n"
     ]
    }
   ],
   "source": [
    "text = \"What can I do for you? Do your homework now.\"\n",
    "\n",
    "# 소문자로 변환\n",
    "print(text.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4bc72e-52ae-4687-b122-13cfea2bb9d8",
   "metadata": {},
   "source": [
    "## 방법 2. 규칙 기반 정규화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1abdb6-6034-476d-ae0c-180f1db357a4",
   "metadata": {},
   "source": [
    "`1` 동의어 사전 작성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "de6a0883-c3c5-4398-acea-b785b716f412",
   "metadata": {},
   "outputs": [],
   "source": [
    "synonym_dict = {'US':'USA', 'U.S':'USA', 'Ummm':'Umm', 'Ummmm':'Umm' }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2228d17f-8c93-4810-b936-06fb2ee94949",
   "metadata": {},
   "source": [
    "`2` 단어 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1b69eb8c-e94d-4eb2-a467-49957069e5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"She became a US citizen. Ummmm, I think, maybe and or.\"\n",
    "normalized_words = []\n",
    "\n",
    "tokenized_words = nltk.word_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61878a3-3533-486a-92a8-1d13954e83e0",
   "metadata": {},
   "source": [
    "`3` 동의어 사전에 있는 단어라면, `value`에 해당하는 값으로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5575630c-80fb-4d6a-913c-3b6efc7e1457",
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in tokenized_words : \n",
    "    if word in synonym_dict.keys() :\n",
    "        word = synonym_dict[word]\n",
    "    \n",
    "    normalized_words.append(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a96980-6979-49d6-8db3-b4687144352a",
   "metadata": {},
   "source": [
    "`4` 결과 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "24540afc-2815-44c1-94bd-881f6ab74cd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['She', 'became', 'a', 'USA', 'citizen', '.', 'Umm', ',', 'I', 'think', ',', 'maybe', 'and', 'or', '.']\n"
     ]
    }
   ],
   "source": [
    "print(normalized_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42dd90e-c2d4-441c-b8dd-6215fadbd7fa",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be1bb51-62fc-47ae-bc0c-ac3bf7a02f24",
   "metadata": {},
   "source": [
    "# 06. 어간 추출(Stemming)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea50e983-b7ca-4a7d-be08-74c11e2262a5",
   "metadata": {},
   "source": [
    "어간 : 특정한 단어의 핵심이 되는 부분\n",
    "\n",
    "어간 추출 : 단어에서 어간을 찾아내 추출하는 것"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185de37d-a84c-42f5-a23a-d2c50166ce68",
   "metadata": {},
   "source": [
    "`-` 포터 스테머 알고리즘(Porter Stemmer Algorithm) : 대표적인 어간 추출 알고리즘\n",
    "\n",
    "* 단순히 어미만 잘라내는 방식"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5635148-164f-4758-9cbc-a5ac2a8e2f8e",
   "metadata": {},
   "source": [
    "> ex\n",
    "> * Formalize $\\to$ Formal\n",
    "> * Relational $\\to$ Relate\n",
    "> * **Activate $\\to$ Activ(?)**\n",
    "> * Encouragement $\\to$ Encourage\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e1d58a-dfc3-4dc5-8c48-27049264a795",
   "metadata": {},
   "source": [
    "* 단순히 어미를 잘라내기 때문에 `Activate` $\\to$ `Activ(?)`처럼 사전에 없는 단어가 추출되는 경우도 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32423fed-c7a3-4142-88ed-2861952ecb58",
   "metadata": {},
   "source": [
    "* 따라서 코퍼스이 특성이나 분석 여건에 따라 어간 추출을 하는게 적합한지 잘 판단해야 한다. 그렇지 않으면 분석에 활용돼야 하는 중요한 단어가 손실될 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68fea22c-74a2-4103-b8b1-5ff5d634d892",
   "metadata": {},
   "source": [
    "## NLTK 어간추출"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a704e5b-b299-40ef-9633-df8ad81dda85",
   "metadata": {},
   "source": [
    "`1` 함수 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "03589f3d-ef3c-4a0a-850f-c94a03f2d924",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "porter_stemmer = PorterStemmer()\n",
    "porter_stemmed_words = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34eb9fe2-873c-4060-9b6c-4ac05a1b81f9",
   "metadata": {},
   "source": [
    "`2` 단어 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f25dae3a-9de9-4458-ab8d-be9ddcf0ef9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"You are so lovely. I am loving you now.\"\n",
    "tokenized_words = nltk.word_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b433d4ef-b7fd-47fc-a337-e07dcf06821f",
   "metadata": {},
   "source": [
    "`3` 어간 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "32a631a6-f2d4-46ef-a5c4-03123ffba305",
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in tokenized_words:\n",
    "    stem = porter_stemmer.stem(word)\n",
    "    porter_stemmed_words.append(stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e785f22d-050b-4c57-b7cb-fd935fc2d7fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['you', 'are', 'so', 'love', '.', 'i', 'am', 'love', 'you', 'now', '.']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "porter_stemmed_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79334f61-3331-4597-972f-91be1e4a644e",
   "metadata": {},
   "source": [
    "## extra. 랭커스터 스테머 알고리즘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0f9cd3a6-21e7-4ae5-a3f9-c28e6775dcbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "\n",
    "lancaster_stemmer = LancasterStemmer()\n",
    "text = \"You are so lovely. I am loving you now.\"\n",
    "lancaster_stemmed_words = []\n",
    "\n",
    "# 랭커스터 스테머의 어간 추출\n",
    "for word in tokenized_words:\n",
    "    stem = lancaster_stemmer.stem(word)\n",
    "    lancaster_stemmed_words.append(stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a45e977b-dde7-4915-892d-b3183d464dc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['you', 'ar', 'so', 'lov', '.', 'i', 'am', 'lov', 'you', 'now', '.']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lancaster_stemmed_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be090d88-c32d-4d24-80d8-9f308b7f4da3",
   "metadata": {},
   "source": [
    "`-` 차이점 : 랭커스터 스테머 알고리즘은 뒤에 `e`와 같은 묵음 처리 부분을 어간에 포함시키지 않는다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca7f268-1539-4dec-abd3-26ba4aa4036a",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20eb84e9-3e95-4436-aec3-fd3951801e10",
   "metadata": {},
   "source": [
    "# 07. 실습. IMDB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac8450f-a576-4f34-a5f5-8ec020aee72d",
   "metadata": {},
   "source": [
    "`-` 데이터셋 : IMDb는 The Internet Movie Database의 약자로, 약 200만개 이상의 영화 관련 정보들이 저장되어 있는 데이터 베이스이다.\n",
    "\n",
    "* 이중 10개의 데이터만 가져와서 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de786cdc-e9c1-4acd-8d2b-58b805c2804c",
   "metadata": {},
   "source": [
    "## step1. 데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "546379a8-f225-4ad6-9b01-8eeb77db9978",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(action = \"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5357c651-ff7d-4395-be8d-1aa6923b196b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"Watching Time Chasers, it obvious that it was...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I saw this film about 20 years ago and remembe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Minor Spoilers In New York, Joan Barnard (Elvi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I went to see this film with a great deal of e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"Yes, I agree with everyone on this site this ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>\"Jennifer Ehle was sparkling in \\\"\"Pride and P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Amy Poehler is a terrific comedian on Saturday...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>\"A plane carrying employees of a large biotech...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>A well made, gritty science fiction movie, it ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>\"Incredibly dumb and utterly predictable story...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review\n",
       "0  \"Watching Time Chasers, it obvious that it was...\n",
       "1  I saw this film about 20 years ago and remembe...\n",
       "2  Minor Spoilers In New York, Joan Barnard (Elvi...\n",
       "3  I went to see this film with a great deal of e...\n",
       "4  \"Yes, I agree with everyone on this site this ...\n",
       "5  \"Jennifer Ehle was sparkling in \\\"\"Pride and P...\n",
       "6  Amy Poehler is a terrific comedian on Saturday...\n",
       "7  \"A plane carrying employees of a large biotech...\n",
       "8  A well made, gritty science fiction movie, it ...\n",
       "9  \"Incredibly dumb and utterly predictable story..."
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('imdb.tsv', delimiter='\\\\t')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b0423e-9dfc-431c-ad62-0802bfb297a0",
   "metadata": {},
   "source": [
    "## step2. 대소문자 통합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b39dabc7-299f-4861-8098-170fa8799aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review'] = df['review'].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eceb214e-096e-4a01-a790-8ae4178ca877",
   "metadata": {},
   "source": [
    "## step3. 단어 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ed4192d2-ce59-45d7-92e4-33214583d8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['word_tokens'] = df['review'].apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82083a86-f520-48c9-afe0-08473f6c26e0",
   "metadata": {},
   "source": [
    "## step4. 데이터 정제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2482faaa-dd63-49f5-a961-7e3d8b43e963",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from preprocess import clean_by_freq\n",
    "from preprocess import clean_by_len\n",
    "from preprocess import clean_by_stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8668e3f2-1deb-4b9e-915e-0dd0fe3d7741",
   "metadata": {},
   "source": [
    "`-` 이건 뭔데?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbeddd7f-f70d-46de-ad99-342d408a370d",
   "metadata": {},
   "source": [
    "```python\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15ff117-3713-42bb-b913-393f667b45b8",
   "metadata": {},
   "source": [
    "* `ipynb` 파일에서 직접 만든 파이썬 모듈(`.py`)을 불러와 사용할 때, 파이썬 모듈 파일이 중간에 수정되면 해당 내용이 자동으로 반영되지 않는 문제가 발생\n",
    "\n",
    "* 그래서, `preprocess.py` 파일을 수정할 때마다 커널을 `Restart`해야함\n",
    "\n",
    "* 이러한 번거러움을 줄이기 위해 위의 코드를 실행한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "aabf19f6-9169-445d-8240-7025100aa5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_set = set(stopwords.words('english'))\n",
    "\n",
    "df['cleaned_tokens'] = df['word_tokens'].apply(lambda x: clean_by_freq(x, 1))\n",
    "df['cleaned_tokens'] = df['cleaned_tokens'].apply(lambda x: clean_by_len(x, 2))\n",
    "df['cleaned_tokens'] = df['cleaned_tokens'].apply(lambda x: clean_by_stopwords(x, stopwords_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d43348-0755-49c9-a769-5e0432236563",
   "metadata": {},
   "source": [
    "## step5. 어간 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8af13d67-d889-4f79-b09d-a73492ce4b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocess import stemming_by_porter\n",
    "\n",
    "df['stemmed_tokens'] = df['cleaned_tokens'].apply(stemming_by_porter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb06ca4e-3f0d-49f7-9213-f9e587b3e366",
   "metadata": {},
   "source": [
    "## step6. 결과 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f883bff6-3dde-4af2-8fb4-5c43c8c407d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>word_tokens</th>\n",
       "      <th>cleaned_tokens</th>\n",
       "      <th>stemmed_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"watching time chasers, it obvious that it was...</td>\n",
       "      <td>[``, watching, time, chasers, ,, it, obvious, ...</td>\n",
       "      <td>[one, film, said, really, bad, movie, like, sa...</td>\n",
       "      <td>[one, film, said, realli, bad, movi, like, sai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i saw this film about 20 years ago and remembe...</td>\n",
       "      <td>[i, saw, this, film, about, 20, years, ago, an...</td>\n",
       "      <td>[film, film]</td>\n",
       "      <td>[film, film]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>minor spoilers in new york, joan barnard (elvi...</td>\n",
       "      <td>[minor, spoilers, in, new, york, ,, joan, barn...</td>\n",
       "      <td>[new, york, joan, barnard, elvire, audrey, bar...</td>\n",
       "      <td>[new, york, joan, barnard, elvir, audrey, barn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i went to see this film with a great deal of e...</td>\n",
       "      <td>[i, went, to, see, this, film, with, a, great,...</td>\n",
       "      <td>[went, film, film, went, jump, send, n't, jump...</td>\n",
       "      <td>[went, film, film, went, jump, send, n't, jump...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"yes, i agree with everyone on this site this ...</td>\n",
       "      <td>[``, yes, ,, i, agree, with, everyone, on, thi...</td>\n",
       "      <td>[site, movie, bad, even, movie, made, movie, s...</td>\n",
       "      <td>[site, movi, bad, even, movi, made, movi, spec...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  \\\n",
       "0  \"watching time chasers, it obvious that it was...   \n",
       "1  i saw this film about 20 years ago and remembe...   \n",
       "2  minor spoilers in new york, joan barnard (elvi...   \n",
       "3  i went to see this film with a great deal of e...   \n",
       "4  \"yes, i agree with everyone on this site this ...   \n",
       "\n",
       "                                         word_tokens  \\\n",
       "0  [``, watching, time, chasers, ,, it, obvious, ...   \n",
       "1  [i, saw, this, film, about, 20, years, ago, an...   \n",
       "2  [minor, spoilers, in, new, york, ,, joan, barn...   \n",
       "3  [i, went, to, see, this, film, with, a, great,...   \n",
       "4  [``, yes, ,, i, agree, with, everyone, on, thi...   \n",
       "\n",
       "                                      cleaned_tokens  \\\n",
       "0  [one, film, said, really, bad, movie, like, sa...   \n",
       "1                                       [film, film]   \n",
       "2  [new, york, joan, barnard, elvire, audrey, bar...   \n",
       "3  [went, film, film, went, jump, send, n't, jump...   \n",
       "4  [site, movie, bad, even, movie, made, movie, s...   \n",
       "\n",
       "                                      stemmed_tokens  \n",
       "0  [one, film, said, realli, bad, movi, like, sai...  \n",
       "1                                       [film, film]  \n",
       "2  [new, york, joan, barnard, elvir, audrey, barn...  \n",
       "3  [went, film, film, went, jump, send, n't, jump...  \n",
       "4  [site, movi, bad, even, movi, made, movi, spec...  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
